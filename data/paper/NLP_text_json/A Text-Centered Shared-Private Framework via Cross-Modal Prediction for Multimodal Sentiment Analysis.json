{
    "Abstract": "Multimodal fusion is a core problem for multi-modal sentiment analysis. Previous works usu-ally treat all three modal features equally andimplicitly explore the interactions between dif-ferent modalities. In this paper, we break thiskind of methods in two ways. Firstly, we ob-serve that textual modality plays the most im-portant role in multimodal sentiment analysis,and this can be seen from the previous works.Secondly, we observe that comparing to thetextual modality, the other two kinds of non-textual modalities (visual and acoustic) canprovide two kinds of semantics, shared and pri-vate semantics. The shared semantics from theother two modalities can obviously enhancethe textual semantics and make the sentimentanalysis model more robust, and the privatesemantics can be complementary to the tex-tual semantics and meanwhile provide differ-ent views to improve the performance of senti-ment analysis together with the shared seman-tics. Motivated by these two observations, wepropose a text-centered shared-private frame-work (TCSP) for multimodal fusion, whichconsists of the cross-modal prediction and sen-timent regression parts. Experiments on theMOSEI and MOSI datasets demonstrate theeffectiveness of our shared-private framework,which outperforms all baselines. Furthermore,our approach provides a new way to utilize theunlabeled data for multimodal sentiment anal-ysis.",
    "Keywords": null,
    "Body": [
        "IntroductionMultimodal sentiment analysis is an emerging re-search field, which aims to understand people's sen-timent using not only textual but also non-textual(visual, acoustic) data. This task has attracted in-creasing attention from the community recently, aspeople have realized that non-textual clues are help-ful for detecting sentiment and the huge demands\u0003Corresponding AuthorTextualVisualAcousticCross-Modal Prediction[Private] Higher Prediction Losses[Shared] Higher Attention Weights[Private] Higher Prediction Losses[Shared] Higher Attention WeightsFigure 1: Distinguishing the shared and private featuresvia cross-modal prediction.",
        "for the identification of opinions and sentiment inthe video.",
        "Comparing to the traditional textual sentimentanalysis(Liu, 2012), previous work demonstratesthat the other non-textual data can improve the fi-nal performance (Chen et al., 2017; Zadeh et al.,2018b; Sun et al., 2020). There are two reasons.",
        "The first reason is that the three modalities can con-vey some common semantics. In this case, thesenon-textual common semantics do not provide ad-ditional information beyond textual data, but therepetitive information from them can strengthen thefinal performance. We call them shared semantics.",
        "The other reason is that the three modalities havetheir own special semantics, which are different toother modalities. These non-textual private seman-tics is modality-specific and hard to be predictedonly by textual data. Thus this kind of semanticsfrom the non-textual modalities can help to detectthe final sentiment more accurately. We call themprivate semantics.",
        "Previous works usually don't distinguish theshared semantics and the private semantics but treateach modal semantics as a whole, lacking the abil-ity to explicitly explore the interaction between",
        "4731different modalities. In this paper, we propose atext-centered shared-private framework for multi-modal sentiment analysis. In this framework, thetextual modality is considered as the core modal-ity, and we first design a cross-modal predictiontask to explicitly distinguish the shared and the pri-vate semantics between the textual modality andthe non-textual(visual, acoustic) modality and thenpropose the sentiment regression model includingthe shared and private modules to fuse the textualfeatures with two types of non-textual features.",
        "In order to explore the shared and private se-mantics from non-textual modalities, we designthe cross-modal prediction task, which is like amachine translation framework. The source is asequence of textual modal features and the targetis a sequence of another modal (visual or acoustic)",
        "features. We can explore the shared and privatesemantics via training two cross-modal predictionmodels, textual-to-visual and textual-to-acousticmodels. In specific, as shown in Figure 1, we ap-ply the pre-trained textual-to-visual and textual-to-acoustic models to predict the targets. The featuresof the target modality with higher prediction lossesare distinguished as private. For each word, thoseputting higher attention weights on this word aredistinguished as shared.",
        "After obtaining shared and private features, wepropose the sentiment regression model to fuse thetextual features with two types of features. The sen-timent regression model mainly consists of threeparts, shared module, private module, and regres-sion layer. In the shared module, each textual fea-ture interacts with the corresponding shared fea-tures to get the enhanced features, which are thenfed into a fusion block to obtain the final sharedrepresentation. Meanwhile, in the private module,the private features of each non-textual modalityare passed through the attention layer to obtain thefinal private representation. Finally, we feed theobtained representations into the regression layerto predict the sentiment score.",
        "We conduct experiments on two multimodalsentiment analysis benchmarks: CMU-MOSI andCMU-MOSEI. The experimental results show thatour model outperforms all baselines. This candemonstrate that the shared-private framework formultimodal sentiment analysis can explicitly usethe shared semantics between different modalitiesto enhance the final performance of sentiment anal-ysis, and meanwhile can explicitly use the privatesemantics of each modality as the supplementalclues for sentiment analysis. In addition, we canobserve that our designed cross-modal predictiontask can accurately distinguish the shared and pri-vate non-textual semantics.",
        "Our contributions can be concluded as follows.",
        "1.We propose a challenging text-centeredshared-private framework for the multimodalsentiment analysis. This framework can ef-fectively fuse textual and non-textual featuresbenefitting from the unlabeled data.",
        "2.We design a cross-modal prediction task toexplore the shared and private semantics foreach non-textual modality.",
        "3.We achieve significant improvements fromlearning the shared and private semantics fromdifferent modalities compared to the algo-rithms without distinguishing the shared andprivate semantics.",
        "2 Related WorkThere are two lines of works conducted on multi-modal sentiment analysis.",
        "One is focusing on utterance-level multimodalfeature fusion. These methods use the features ofthe overall utterance. For example, they first extractthe frame-level visual or acoustic features and thenaverage them to obtain the final features, which arecalled utterance-level features. The utterance-leveltextual features can be obtained by applying RNNsfor words. The obtained utterance-level featuresare fed into the fusion model to get the multimodalrepresentation. Some models have been proposedfor effective multimodal feature fusion. Zadeh et al.",
        "(2017) proposed Tensor Fusion to explicitly captureunimodal, bimodal, and trimodal interactions. Butthis method uses the three-fold Cartesian product tofuse the multimodal features, which makes the timecost very high. To address it, Liu et al. (2018) pre-sented the Efficient Low-rank Multimodal Fusion,which applies multimodal fusion using low-ranktensors to accelerate the fusion process. Mai et al.",
        "(2020) proposed a graph fusion network to modelunimodal, bimodal, and trimodal interactions suc-cessively.",
        "The utterance-level features mainly containglobal information, which may fail to capture localinformation. Therefore, recent works are mostlyfocusing on word-level multimodal feature fusion.",
        "4732And our work in this paper is also based on word-level features. To extract word-level features, thefirst step is applying force alignment to obtain thetimestamps of each word including the start timeand end time. And then following the timestamps,the utterance is split into some video clips. Fi-nally, word-level visual or acoustic features areobtained by averaging the frame-level features ofthe video clips. Based on word-level features, lotsof methods are proposed for performing word-levelmultimodal feature fusion. Zadeh et al. (2018a)",
        "proposed the Memory Fusion Network(MFN) tocapture the interactions across both different modal-ities and timesteps. Inspired by the observation thatthe meaning of words often varies dynamically indifferent non-verbal contexts, Wang et al. (2019)",
        "proposed the Recurrent Attended Variation Embed-ding Network (RA VEN). This model applies theAttention Gating module to fuse the word-level fea-tures, which can dynamically use the non-verbalfeatures to shift the word embeddings. Tsai et al.",
        "(2019) presented multimodal transformer (Mult),which uses the cross-modal attention to capture thebimodal interactions, motivated by the great suc-cess of transformer in NLP(Vaswani et al., 2017).",
        "Besides, there is a related work (Pham et al.,2019) need to be noticed, which proposed thattranslation from a source to a target modality pro-vides a way to learn joint representations and pro-posed the Multimodal Cyclic Translation Networkmodel (MCTN) to learn joint multimodal represen-tations. Comparing to this work, our model hasa significant difference. That is we use the cross-modal prediction task to distinguish the shared andprivate non-textual features instead of training themodel as an auxiliary task. In this way, we canobtain more useful information by deeply probingthe cross-modal prediction model.",
        "3 ApproachIn this section, we will introduce our shared-privateframework for multimodal sentiment analysis in de-tail. In this framework, we treat the textual modal-ity as the core, then how to explore the shared andthe private semantics of the non-textual modalitycompared to the textual modality, and how to fuseall three modal features are two important steps.",
        "For the first step, we design a cross-modal pre-diction task and obtain the shared and private fea-tures of the non-textual modalities via training twocross-modal prediction models, textual-to-visualand textual-to-acoustic models. And for the sec-ond step, we design a sentiment regression modelto fuse the textual features and the two types offeatures.",
        "3.1 Cross-Modal PredictionTask Definition: The cross-modal predictiontask is formalized as follows. Given a sequence oftextual features denoted as xl=fxtl: 1\u0014t\u0014L; xtl2Rdlg,Lis the length of the given sequence,tis the timestep, and the goal is to predict the cor-responding sequence of visual or acoustic featuresdenoted as xi=fxti: 1\u0014t\u0014L; xti2Rdig,i2fv; ag. Cross-modal prediction task is inspiredby the machine translation task. The inputs are thetextual features, and the generated outputs are thenon-textual (visual or acoustic) features. Duringthe translation from the textual modality to othermodalities, we can exploit the shared and the pri-vate semantics of the non-textual modalities.",
        "Prediction Model: We use the Seq2Seq modelwith attention (Bahdanau et al., 2015) as our modelframework. The encoder takes the textual fea-tures xlas inputs and outputs the hidden stateshenc=fhtenc: 1\u0014t\u0014L; htenc2Rdhg. Thedecoder takes the previous hidden state ht\u00001decandhidden states of the encoder as inputs and predictsthe non-textual feature xti,i2 fv; ag, at the ttimestep. We choose the MSE as our loss func-tion. The prediction loss values are denoted asel!i=fetl!i: 1\u0014t\u0014Lg. The attention map ofthe prediction model is denoted as mi!l. In prac-tice, we apply LSTMs (Hochreiter and Schmid-huber, 1997) to implement our encoders and de-coders. After training the cross-modal predictionmodels using textual-visual and textual-acousticpaired data, we can obtain two models, textual-to-visual and textual-to-acoustic models. We then usethe obtained models to distinguish the shared andprivate features and record the results as shared andprivate masks, which will be passed to the senti-ment regression model.",
        "Shared Mask: We propose the shared mask tofind out the shared semantics of the two kinds ofnon-textual modalities. The basic assumption isthat during cross-modal prediction, if the predic-tion model wants to generate a non-textual featureas precisely as possible, it should pay more atten-tion to the input textual features, that contain moreshared semantics. Based on this assumption, we de-sign the method to obtain the shared mask. Given",
        "4733Visual/AcousticTextual0.10.20.40.20.1Visual/AcousticTextual(1)",
        "(3)Visual/AcousticTextualFigure 2: Obtaining the shared mask from the cross-modal prediction model.",
        "mi!l,i2fv; ag, for each row t, we first sort theattention weights mt;\u0003i!land then get the indexesStof the largest Ksvalues. Finally, we can get theshared mask smask ,smask2RL\u0003L.smaskt1;t2is 1 if the t12St2and 0 otherwise.",
        "To describe this method intuitively, we show thisprocess in Figure 2. There are three steps: (1) Webuild an attention graph and the values of edgesmean the attention weights. We illustrate a partof it for simplicity. (2) We only keep the edgeswith larger values for each non-textual node anddelete others. (3) We map the graph to the sharedmask smask ,smaskt1;t2is 1 if there is a edgebetween textual node t1and non-textual node t2and 0 otherwise. The shared mask will be passed tothe share module of the sentiment regression modelto make the model focus on the shared features.",
        "Private Mask: In order to find out the private se-mantics of the two kinds of non-textual modalities,we propose the private mask. The basic assump-tion is that the features containing modality-privateinformation are difficult to be predicted by textualmodality. The private mask of a given utteranceis obtained as follows. Given an utterance whichincludes three modalities, textual, visual and acous-tic, denoted as xi=fxti: 1\u0014t\u0014L; xti2Rdig,i2fl; v; ag, we first use the trained predictionmodels to get the loss values, el!vandel!a. Thenwe sort the loss values to obtain the indexes Pofthe largest Kpvalues. Finally, We can get the pri-vate mask pmask ,pmask2RL.pmasktis 1if the t2Pand 0 otherwise. The private maskwill be used by the private module of the sentimentregression model to force the model to focus onprivate features.",
        "3.2 RegressionIn this section, we study how to fuse the shared andprivate information obtained from Section 3.1. Anillustration of our framework is given in Figure 3 .",
        "3.2.1 Input LayerGiven an utterance which includes three modalities,textual, visual and acoustic, the extracted multi-modal features are denoted as xi=fxti: 1\u0014t\u0014L; xti2Rdig,i2fl; v; ag. We use three LSTMnetworks to encode the input multimodal featuresxi, producing hi=fhti: 1\u0014t\u0014L; hti2Rdhg.",
        "hl= LSTM l(xl)",
        "hv= LSTM v(xv)",
        "ha= LSTM a(xa)(1)",
        "3.2.2 Shared ModuleThe core idea of the shared module is leveragingthe shared information from non-textual modal fea-tures to enhance the representations of words. Toachieve it, we propose the masked cross-modal at-tention network, which can utilize the shared masksobtained from cross-modal prediction models to fo-cus on the non-textual shared features.",
        "In the masked cross-modal attention network,we first calculate the attention scores across thenon-textual representations hi,i2fv; ag, for eachword. We denote the scores as sl!i.",
        "where W1; W32Rdh\u00022dh,W2; W42R1\u0002dh,b1; b32Rdhare the parameters of the score func-tions.",
        "To focus on the shared features, we first calculatethe attention weights wl!vandwl!ausing thesoftmax function and mask the other features outusing the shared mask.",
        "wt1;t2l!v=est1;t2l!vPLt3=1est1;t3l!vwt1;t2l!a=est1;t2l!aPLt3=1est1;t3l!a(3)",
        "wl!v=wl!v\u000esmask l!vwl!a=wl!a\u000esmask l!a(4)",
        "4734TextualVisualAcousticLSTMLSTMLSTMCross-ModalAttentionCross-ModalAttentionConcatLSTMSelf-AttentionShared RepresentationPrivate RepresentationPrivate RepresentationTextualTextualEncoderEncoderDecoderLSTMAttentionVisualDecoderLSTMAttentionAcousticAttentionAttentionRegression LayerPrivate MaskShared MaskShared MaskPrivate MaskCross Modal Prediction ModelShared ModulePrivate ModuleFigure 3: Illustration of our shared-private framework.",
        "We obtain the non-textual shared context vectorscvandca.cv;ca2RL\u0002dh.",
        "cv=wl!vhvca=wl!aha(5)",
        "To fuse textual and non-textual shared features,we concatenate cv,ca, and hland feed it into thefusion LSTM network, producing rs2RL\u00023dh.",
        "We further use a self-attention layer, which is de-noted as SelfAttentionLayer , to learn the final rep-resentation. The self-attention layer is similar to thecross-modal attention network. We use the last steprepresentation of rnas the shared representation,which is denoted as rs.",
        "rm= LSTM fusion ([cv;ca;hl])",
        "rn= SelfAttentionLayer( rm)(6)",
        "3.2.3 Private ModuleTo enable the model to capture the unique informa-tion contained in non-textual modalities, we designthe private module. Specifically, we use the at-tention network to learn informative and modality-private representations.",
        "stv=W5htv+b5sta=W6hta+b6(7)",
        "where W5; W62R1\u0002dh,b5; b62Rare theparameters of the score functions.We use private masks to ignore other featuresand apply the softmax function to get the attentionweights.",
        "Finally, we compute the weighted sum and repre-sent them as pvandpa, which are called the privaterepresentations.",
        "wtv=estvPLt1=1est1vwta=estaPLt1=1est1a(9)",
        "pv=wvhvpa=waha(10)",
        "3.2.4 Regression LayerWe design the regression layer, which is imple-mented by a two-layer network with ReLU activa-tion function, to fuse the shared and private repre-sentations.",
        "^y=Wo(ReLU (Wf([rs;pv;pa])+bf))+bo(11)",
        "where Wf2Rdh\u00025dh,Wo2R1\u0002dh,bf2Rdh,bo2R.",
        "4735Table 1: Hyperparameters of our model.",
        "Models Parameters MOSI MOSEICross-ModalPredictionBatch Size 24 24Max Length 50 128Hidden Size 100 100Epochs 40 40Learning Rate 0.0001 0.0001Dropout 0.5 0.5Patience 5 10RegressionBatch Size 24 24Max Length 50 128Hidden Size 100 100Epochs 30 30Learning Rate 0.001 0.001Dropout 0.5 0.5Selection Number 5 5Patience 5 54 Experiments4.1 DatasetsWe conduct experiments on two public datasets,CMU-MOSI (Zadeh, 2015) and CMU-MOSEI(Zadeh et al., 2018b) to evaluate our proposedmodel. CMU multimodal opinion-level sentimentintensity (CMU-MOSI) consists of 93 videos col-lected from the YouTube website. The length ofthe videos varies from 2-5 mins. These videosare split into 2,199 short video clips and labeledwith sentiment scores from -3 (strongly negative)",
        "to 3 (strongly positive). CMU multimodal opinionsentiment and emotion intensity (CMU-MOSEI)",
        "consists of 23,453 annotated video utterances from1,000 distinct speakers and 250 topics. Each utter-ance is annotated with sentiment scores from -3(strongly negative) to 3 (strongly positive).",
        "The multimodal features used in our experimentsare described as follows. We use glove word em-beddings (Pennington et al., 2014) to represent thewords. The dimension of each word embeddingis 300. We extract the visual features using Facet,which can extract 35 facial action units (Ekmanet al., 1980; Ekman, 1992) from each frame result-ing in a 35-dimensional vector. The acoustic fea-tures are obtained by applying COV AREP (Degot-tex et al., 2014), which includes 12 Mel-frequencycepstral coefficients (MFCCs) and other low-levelfeatures. The dimension of the acoustic feature is74.",
        "4.2 Evaluation MetricsFollowing previous works, we take 2-class ac-curacy(Acc), f1 score(F1), mean absolute error(MAE), and correlation(Corr) as our evaluationmetrics. As the prediction results are real values,we first use mean absolute error and Corr betweenprediction scores and ground truths to evaluate themodels. In addition, we then map the sentimentscores into sentiment labels and use classificationmetrics, such as accuracy and f1 score, to assessthe model performance.",
        "4.3 Training DetailsThe hyperparameters of our model are listed inTable 1. In practice, we apply dropout before thelast linear layer for regularization. We use Adamas the optimizer. The learning rate is decayed oncethe validation loss stops decreasing. The SelectionNumber is the number of selected shared/privatefeatures, KsandKp. We take the same value forKsandKpfor simplicity.",
        "4.4 BaselinesWe compare our proposed model with the fol-lowing baselines. EF-LSTM fuses the multi-modal features by concatenating and applies anLSTM network to get the final representation. LF-LSTM first uses three LSTM networks to encodethree modal features and concatenates three ob-tained representations to get the final representation.",
        "MFN (Zadeh et al., 2018a) captures the interac-tions across both the different modalities and time.",
        "RA VEN (Wang et al., 2019) first combines the non-verbal information with word representations andthen feeds the modified word representations intoan LSTM network to obtain the utterance repre-sentation. MCTN (Pham et al., 2019) learns jointmultimodal representations by translating betweenmodalities. MulT (Tsai et al., 2019) uses cross-modal transformers to fuse multimodal features.",
        "Multimodal Routing (Tsai et al., 2020) proposesa routing mechanism to capture the interactions be-tween input modalities and outputs. TCSP(Base)",
        "is our base model. The model architecture is assame as our full model, but it doesn't use sharedand private masks. Comparing TCSP(Base) andTCSP(Full), we can judge whether distinguish-ing the shared and private features of non-textualmodalities is useful.",
        "4.5 Experimental ResultsWe compare our model with several baselines andthe experimental results are shown in Table 2. Com-paring our base model with other baselines, ourbase model fails to obtain the best result and un-derperforms RA VEN and MulT for the Acc, F1",
        "4736Table 2: Experimental results on the test sets of the MOSEI and MOSI dataset. The best results are in bold . Asthe Multimodal Routing model is designed for classification, we don't report the regression metrics of it for faircomparison.",
        "Models MOSI MOSEIAcc\" F1\" MAE# Corr\" Acc\" F1\" MAE# Corr\"",
        "Models MOSI MOSEIAcc\"F1\"MAE#Corr\"Acc\"F1\"MAE#Corr\"",
        "achieves the best performance and outperforms allbaselines on both datasets. This can demonstratethat the shared-private framework proposed in thispaper is effective for multimodal sentiment analy-sis. Furthermore, it can be observed that the sharedand private features for each non-textual modal-ity obtained from the cross-modal prediction taskcan provide useful clues for the interactions be-tween different modalities. Thus, these non-textualshared-private features can be jointly fused withthe textual features to improve the multimodal sen-timent analysis.",
        "We also observe that there is a larger marginbetween our full model and our base model on theMOSI dataset. We attribute it to the small data sizeof the MOSI dataset. It is insufficient for trainingthe base model, which makes it benefit more fromthe shared and private information.",
        "It should be noted that, in Table 2, we providetwo results for each method on each dataset. Theleft result is obtained by rerunning the public codesin the same experimental setting, which refers tothe same dataset split and the same extracted fea-tures of three modalities. The right result is copiedfrom previous papers and the experimental settingsare different. To guarantee the justice, we compareour TCSP model with the left results.5 Analysis5.1 Ablation StudyWe conduct the ablation experiments to distinguishthe contribution of each part. As shown in Table 3,ablating either shared mask or private mask hurtsthe model performance, which indicates that bothmasks are useful for the sentiment prediction. Theshared mask can enable the sentiment regressionmodel to get the modality-shared features result-ing in a more robust regression model. The pri-vate mask makes the regression model focus onmodality-private features, which provides extra in-formation for sentiment prediction. With the helpof shared and private masks, the regression modelin the shared-private framework can fuse the tex-tual features with two types of non-textual featuresindividually, which is the more effective methodfor multimodal feature fusion.",
        "5.2 Effect of Selection NumberSelection Number is the number of selectedshared/private features, KsandKp. We take thesame value for KsandKpfor simplicity. We evalu-ate our model with different selected numbers from1 to 8 on the MOSEI dataset to quantify the effect.",
        "The experimental results are shown in Figure 4.",
        "We can observe that our model achieves the bestperformance on the Acc and F1 metrics when theSelection Number is 5. The possible reason is that",
        "473780.080.581.081.582.082.583.012345678Selection NumberAccF1Figure 4: Experimental results with different selectionnumbers on the MOSEI dataset.",
        "0.0000.5001.0001.5002.0002.5003.0003.5004.0004.5005.00079.580.080.581.081.582.082.583.020%40%60%80%100%Proportions of data Prediction Loss(L->V)Prediction Loss(L->A)F1AccFigure 5: Experimental results on the MOSEI datasetwith different proportions of data used for the cross-modal prediction model.",
        "too small Selection Number makes the model onlyfocus on few features. This could result in missinguseful information. In contrast, too large one makesthe model attend too many features, which weak-ens the effect of masks. For this reason, selecting amiddle number could be better.",
        "5.3 Effect of Cross-Modal Prediction ModelThe cross-modal prediction task is the core of ourshared-private framework, and it has been demon-strated that this task is effective from Table 3 andSection 5.1. In this section, we want to further ex-plore the effect of cross model prediction for thefinal regression model.",
        "In Figure 5, we design different cross-modal pre-diction models trained with different proportions(from 20% to 100%) of MOSEI data and then fusethe obtained shared and private information intothe regression models. It should be noticed thatall regression models are trained with all data ofthe MOSEI dataset. The results show that whenwe use more data, the final performance is better.",
        "And meanwhile, it can be observed that the twokinds of prediction losses (from textual to visualmodality and from textual to acoustic modality) aredecreased when the proportion of the used data isincreased.",
        "This can reveal that the cross-modal predictionmodel trained with more data can provide more in-formative supervision signals, which are the sharedand private masks specifically. If the performanceof cross-modal prediction model is low, it is im-possible to teach the regression model to play theprecise role in the shared-private framework.",
        "6 ConclusionIn this paper, we propose a text-centered shared-private framework for multimodal sentiment analy-sis. In this framework, we treat the textual modalityas the core and aim to use the other non-textualmodalities to help enrich the semantics of the tex-tual modality. For each non-textual modality, weconsider two types of semantics, shared and pri-vate, which have different functions. Shared se-mantics can enhance the textual semantics to makethe model more robust and the private semanticscan provide extra information for more precise pre-diction.",
        "To distinguish these two semantics, we design across-modal prediction task and record the resultsas share and private masks. We further propose aregression model utilizing the shared and privatemodules to fuse the textual features with two non-textual features. The experimental results demon-strate that distinguishing the shared and privatenon-textual semantics and explicitly modeling theinteractions between textual and two non-textualsemantics is a better way for the multimodal sen-timent analysis than just treating each non-textualfeatures as a whole. The analyses show that theregression model can benefit more from the bet-ter cross-modal prediction model, which also indi-cates that the cross-modal prediction process canproduce useful supervision signals only using unla-beled data.",
        "In future work, we plan to collect more unla-beled data to enhance our model. In addition, wewould also like to explore other approaches usingthe unlabeled data to help multimodal feature fu-sion.",
        "AcknowledgmentsThis work was supported in part by the follow-ing Grants: National Natural Science Founda-tion of China (No. 61632011, No. 61772153),National Key R&D Program of China (No."
    ]
}
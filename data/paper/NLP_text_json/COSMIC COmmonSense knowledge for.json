{
    "Abstract": "In this paper, we address the task of utter-ance level emotion recognition in conversa-tions using commonsense knowledge. Wepropose C OSMIC , a new framework thatincorporates different elements of common-sense such as mental states, events, andcausal relations, and build upon them to learninteractions between interlocutors participat-ing in a conversation. Current state-of-the-art methods often encounter difficulties incontext propagation, emotion shift detection,and differentiating between related emotionclasses. By learning distinct commonsenserepresentations, C OSMIC addresses these chal-lenges and achieves new state-of-the-art re-sults for emotion recognition on four differ-ent benchmark conversational datasets. Ourcode is available at https://github.com/declare-lab/conv-emotion .",
    "Keywords": null,
    "Body": [
        "IntroductionEmotion recognition is a long-standing researchproblem in Artificial Intelligence (AI). With thegrowing popularity of conversational AI research,the topic of emotion recognition in conversationshas received significant attention from the researchcommunity (Li et al., 2020; Ghosal et al., 2019;",
        "Zhang et al., 2019). Identifying emotions in con-versations is a core step toward fine-grained con-versation understanding, which in turn is essen-tial for downstream tasks such as emotion-awarechat agents (Lin et al., 2019; Rashkin et al., 2019),visual question answering (Tapaswi et al., 2016;",
        "Azab, 2019), health conversations (Althoff et al.,2016; P ¬¥erez-Rosas et al., 2017) and others.",
        "Natural conversations are complex as they aregoverned by several distinct variables that affectthe flow of a conversation and the emotional dy-namics of the participants. These variables includeReaction of A: Gets tired Reaction of B: Irritated Effect on B: Gets yelled atAngryAngryPerson APerson BLook, it's a beautiful day outside,why are we arguing?",
        "Well, what do you want me to do  about it?  What do you want?Reaction of A: Angry, annoyed Intent of B: Help out Effect on B: Thinks what to do Commonsense InferenceCommonsense InferenceI want you to pretend likehe's coming back.AngryInfluenced by the other personFigure 1: Commonsense knowledge can lead to ex-plainable dialogue understanding. It will help mod-els to understand, reason, and explain events and sit-uations. In this particular example, commonsense in-ference is applied to a sequence of utterances in a two-party conversation. Person A's first utterance indicatesthat he/she is tired of arguing with person B. The toneof the utterance also implies that person B is gettingyelled at by person A, which invokes a reaction of ir-ritation in person B. Person B then asks what he/shecan do to help and says this while being angry. Thisagain makes person A annoyed and influences him/herto respond with anger. This kind of inferred common-sense knowledge about the reaction, effect, and intentof the speaker and the listener helps in predicting theemotional dynamics of the participants.",
        "topic, viewpoint, speaker personality, argumenta-tion logic, intent, and so on (Poria et al., 2019b).",
        "Additionally, individual utterances are also gov-erned by the mental state, intent, and emotionalstate of the participants at the time when they areuttered. In this conversation model, only the utter-ances can be observed as the conversation unfolds,",
        "2471while other variables such as speaker state and in-tent remain latent as they are not directly observedby the other participants. Similarly, the emotionalstate of the speakers cannot be directly observed,but it can be inferred from the utterances that areobservable.1The commonsense knowledge of the participantsin a conversation plays a central role in inferring thelatent variables of a conversation. It is used to guidethe participants through their reasoning about thecontent of the conversation, dialog planning, deci-sion making, and many other reasoning tasks. It isalso used to recognize other finer-grained elementsof a conversation, such as avoiding repetition, ask-ing questions, refraining from giving unrelated re-sponses, and so on ‚Äî all of which control aspectsof the conversation such as fluency, interestingness,inquisitiveness, or empathy. Commonsense knowl-edge is thus necessary to model the nature and flowof the dialogue and the emotional dynamics of theparticipants. In Figure 1, we illustrate one such sce-nario where commonsense knowledge is utilized toinfer emotions of the utterances in a dialogue.",
        "Natural language is often indicative of one'semotion. Hence, emotion recognition has beenenjoying popularity in the field of NLP (Kratzwaldet al., 2018; Colneri ÀÜc and Demsar, 2018), due toits widespread applications in opinion mining, rec-ommender systems, healthcare, and so on. Onlyin the past few years has emotion recognition inconversation (ERC) gained attention from the NLPcommunity (Yeh et al., 2019; Chen et al., 2018;",
        "Majumder et al., 2019; Zhou et al., 2018) due tothe growing availability of public conversationaldata. ERC can be used to analyze conversationsthat take place on social media. It can also aidin analyzing conversations in real time, whichcan be instrumental in legal trials, interviews, e-health services, and more. Unlike vanilla emotionrecognition of sentences/utterances, ERC ideallyrequires context modeling of the individual utter-ances. This context can be attributed to the pre-ceding utterances, and relies on the temporal se-quence of utterances. Compared to the recentlypublished works on ERC (Chen et al., 2018; Ma-jumder et al., 2019; Zhou et al., 2018; Qin et al.,2020; Zhong et al., 2019; Zhang et al., 2019), bothlexicon-based (Wu et al., 2006; Mohammad andTurney, 2010; Shaheen et al., 2014) and modern1In multimodal conversations, there are other variables thatcan be observed, such as facial expressions, gestures, pitch,and acoustic indicators.deep learning-based (Kratzwald et al., 2018; Col-neriÀÜc and Demsar, 2018) vanilla emotion recogni-tion approaches fail to work well on ERC datasetsas this work ignores the conversation specific fac-tors such as the presence of contextual cues, thetemporality in speakers' turns, or speaker-specificinformation.",
        "In this paper, we introduce COSMIC , acommonsense-guided framework for emotion iden-tification in conversations. By building upon avery large commonsense knowledge base, our pro-posed framework captures some of the complexinteractions between personality, events, mentalstates, intents, and emotions leading towards a bet-ter understanding of the emotional dynamics andother aspects of conversation. Through extensiveevaluations on four different conversation datasetsand comparisons with several baselines and state-of-the-art models, we show the effectiveness of amodel that explicitly accounts for commonsense.",
        "Moreover, feature ablation experiments highlightthe role that such knowledge plays in identifyingemotion in conversations.",
        "2 Related WorkEmotion recognition has been an active area of re-search for many years and has been explored acrossinter-disciplinary fields such as machine learning,signal processing, social and cognitive psychol-ogy, etc (Picard, 2010). The seminal work fromEkman (1993) presented findings on facial expres-sions, methods to measure facial expression andtheir relation with human emotion. Acoustic infor-mation and visual cues were later used for emotionrecognition by Datcu and Rothkrantz (2014).",
        "However, emotion recognition in conversationshas gained popularity only recently due to the emer-gence of publicly available conversational datasetscollected from social media platforms and scriptedsituations such as movies and tv-shows (Poria et al.,2019a; Zahiri and Choi, 2018). The main approachtowards conversational emotion recognition is toperform contextual modeling in either textual ormultimodal setting with deep-learning based algo-rithms. Poria et al. (2017) used recurrent neuralnetworks for multimodal emotion recognition fol-lowed by (Majumder et al., 2019), where party andglobal states were used for modeling the emotionaldynamics. An external knowledge base was usedin (Zhong et al., 2019) with transformer networksto perform emotion recognition. Some of the other",
        "2472important works include (Hazarika et al., 2018a,b;",
        "Zadeh et al., 2018b; Chen et al., 2017; Zadeh et al.,2018a).",
        "3 Methodology3.1 Task definitionGiven the transcript of a conversation along withspeaker information for each constituent utter-ance, the ERC task aims to identify the emo-tion of each utterance from a set of pre-definedemotions. Figure 1 illustrates one such conver-sation between two people, where each utter-ance is labeled by the underlying emotion. For-mally, given an input sequence of Nutterances[(u1;p1);(u2;p2);:::; (uN;pN)], where each ut-teranceui= [ui;1;ui;2;:::;u i;T]consists of Twordsui;jspoken by party pi, the task is to pre-dict the emotion label eiof each utterance ui. Inconversational emotion recognition, the task is toclassify each of the constituting utterances into itsappropriate emotion category. In literature, themain approach towards this problem has been tofirst produce context independent representationsand then perform contextual modeling. We identifythese two distinct modeling phases and aim to im-prove both of them through the proposed COSMICframework. Our framework consists of three mainstages:",
        "1.Context independent feature extraction frompretrained transformer language models.",
        "2.Commonsense feature extraction from a com-monsense knowledge graph.",
        "3.Incorporating commonsense knowledge to de-sign better contextual representations and us-ing it for the final emotion classification.",
        "The overall architecture of the COSMIC frame-work is illustrated in Figure 2.",
        "3.2 Context Independent Feature ExtractionWe employ the RoBERTa model (Liu et al., 2019)",
        "to extract context independent utterance level fea-ture vectors. We first fine-tune the RoBERTa Largemodel for emotion label prediction from the tran-script of the utterances. RoBERTa Large followsthe original BERT Large (Devlin et al., 2018) archi-tecture having 24 layers, 16 self-attention heads ineach block and a hidden dimension of 1024, result-ing in a total of 355M parameters. Let an utterancexconsists of a sequence of BPE tokenized tokensx1;x2;:::;x N, with emotion label Ex. In this set-ting, the fine-tuning of the pretrained RoBERTamodel is realized through a sentence classifica-tion task. A special token [CLS]is appended atthe beginning of the utterance to create the inputsequence for the model: [CLS];x1;x2;:::;x N.",
        "This sequence is passed through the model, and theactivation from the last layer corresponding to the[CLS]token is then used in a small feedforwardnetwork to classify it into its emotion class Ex.",
        "Once the model has been fine-tuned for emotionlabel classification, we pass the [CLS]appendedBPE tokenized utterances to it and extract out acti-vations from the final four layers corresponding tothe[CLS]token. These four vectors are then av-eraged to obtain the context independent utterancefeature vector with a dimension of 1024.",
        "3.3 Commonsense Feature ExtractionCommonsenseFeatureNotation Nature CausalRelationIntent of speaker IScs(:)Mental state CauseEffect on speaker EScs(:)Mental state EffectReaction of speaker RS cs(:) Event EffectEffect of listeners ELcs(:)Mental state EffectReaction of listeners RL cs(:) Event EffectTable 1: Functional notations of commonsense knowl-edge used in C OMET . The functions take as input theutteranceuand returns the feature indicated in the left-most column. Intent and effect on speaker and listen-ers can be categorized into mental states , whereas theirreactions are events . Intent is also a causal variablewhereas the rest are effects .",
        "In this work, we use the commonsense trans-former model COMET (Bosselut et al., 2019) to ex-tract the commonsense features. COMET is trainedon several commonsense knowledge graphs to per-form automatic knowledge base construction. Themodel is given a triplet fs;r;ogfrom the graphand is trained to generate the object phrase ofromconcatenated subject phrase sand relation phraser. COMET is an encoder-decoder model that usesthe pretrained autoregressive language model GPT(Radford et al., 2018) as the base generative model.",
        "To perform the task of generative commonsenseknowledge construction, COMET is trained onATOMIC (The Atlas of Machine Commonsense)",
        "(Sap et al., 2019), a collection of everyday infer-ential if-then commonsense knowledge organizedthrough textual descriptions. ATOMIC consists ofnine different if-then relation types to distinguish",
        "2473agents vs themes, causes vs effects, voluntary vsnon-voluntary events, and actions vs mental states.",
        "Given an event in which X participates, the nine re-lation types ( r) are inferred as follows: i) intent ofX, ii)need of X , iii)attribute of X , iv)effect on X , v)",
        "wanted by X , vi)reaction of X , vii) effect on others ,viii) wanted by others , and ix) reaction of others .",
        "As an example, given an event or subject phrase ( s):",
        "‚ÄúPerson X gives Person Y a compliment‚Äù, the infer-ence from COMET for relation phrase ( r):intentof X andreaction of others would be ‚ÄúX wanted tobe nice‚Äù and ‚ÄúY will feel flattered‚Äù respectively.",
        "COMET is a generative model and as illustratedin the above example it produces a discrete se-quence of commonsense knowledge conditionedon the subject and relation phrase. In our modelhowever, we make use of continuous vectors ofcommonsense representations. For that, we takethe pretrained COMET model on ATOMIC knowl-edge graph and discard the phrase generating de-coder module. We treat utterance Uas the subjectphrase and concatenate it with the relation phrase r.",
        "Next, we pass the concatenated fU\brgthroughthe encoder of COMET and extract out the activa-tions from the final time-step. In particular we usethe relations presented in Table 1: intent of X ,effecton X ,reaction of X ,effect on others andreactionof others (whereXis the speaker and others arelisteners). Performing this feature extraction opera-tion results in five different vectors (respective tothe five different relations) for each utterance in theconversation. These vectors are 768 dimensional.",
        "The nature of the various relation types inATOMIC allows us to extend it naturally to con-versational frameworks. The relations enable themodeling of phenomenons such as content (event,persona, mental states) and causal relations (cause,effect, stative) which are essential elements for un-derstanding conversational context. These differentrelations are of key importance because generallythere is a major interplay between virtually all ofthem throughout the course of a conversation. Forinstance, the relations i) - vi) are all intrinsically re-lated to the speaker and vii) - ix) are all akin to thelistener. On a more fine-grained level, the intent,effect andreact components of the speaker and lis-tener are all elemental for understanding the natureof the conversation. We surmise that adopting theserelational variables in a unified framework wouldbe highly useful to create enhanced representationsof the conversation.State Influenced ByContext StateUtterance,Internal state, External stateInternal StateContext state,Effect on speaker, listenerExternal StateContext state, Utterance,Reaction of speaker, listenerIntent State Internal state, Intent of speakerEmotion StateUtterance, Intent stateInternal state, External stateTable 2: Different states and the respective variablesthey are influenced by. Italic variables are forms ofcommonsense knowledge from Table 1.",
        "3.4 Commonsense Conversational ModelWe first introduce our notations and presenta high level view of the main architecture ofour COSMIC model. A conversation consistsofNutterancesu1;u2;:::;u N, in which Mdistinct speakers/participants p1;p2;:::;p Mtakepart. Utterance utis spoken by participantps(ut). For every t2 f1;2;:::;Ng, we de-note context independent RoBERTa vectors byxt. Commonsense vectors corresponding to in-tent of X ,effect on X, reaction of X, effecton others and reaction of others are denotedbyIScs(ut);EScs(ut);RScs(ut);ELcs(ut);andRLcs(ut)respectively. Xis assumed to be thespeaker and others are assumed to be the listeners.",
        "Since conversations are highly sequential in na-ture and contextual information flows along a se-quence, a context state ctandattention vector atare formulated that model the sequential depen-dency between utterances. The context state andattention vector are always shared between all theparticipants of the conversation.",
        "Aninternal state ,external state andintent stateare used to model different mental states, actionsand events for the participants. These are rep-resented by qk;t;rk;tandik;tfor the participantsk2[1;2;:::;M ]. The internal state and the ex-ternal state can be collectively considered as thespeaker state. This states are necessary to capturethe complex mental and emotional dynamics ofthe participants. The emotion state etis then mod-elled from a combination of the three states and theimmediate preceding emotion state. Finally the ap-propriate emotion class for the utterance is inferredfrom the emotion state.",
        "In our framework, context and commonsensemodeling is performed using GRU cells (Chung",
        "2474GRUQGRUCqA,t‚àí1AttentionatctGRUEetÃÇytqA,t‚àí1Note:Concatenation Internal state External state Intent state Speaker-independent state Emotion-repct‚àí1c1GRURGRUICSKrA,t‚àí1iA,t‚àí1qA,trA,tiA,txtUtterancerA,t‚àí1qA,tet‚àí1xt‚Ñ∞ùíÆcs(.)‚ÑõùíÆcs(.)IùíÆcs(.)qA,t‚àí1rA,t‚àí1iA,t‚àí1Speaker (A)",
        "Listener (B)qA,trA,tiA,tctct‚àí1iA,t‚àí1qB,t‚àí1rB,t‚àí1iB,t‚àí1GRUQqB,t‚àí1GRURrB,t‚àí1iB,t‚àí1qB,trB,tiB,tAttentionatct‚àí1c1qB,trB,tiB,tCSKxt‚Ñ∞‚Ñícs(.)‚Ñõ‚Ñícs(.)UtterancextListener (A)",
        "Speaker (B)time ttime t+1Attentionat+1ctc1CSKxt+1Utterance‚Ñõ‚Ñícs(.)‚Ñ∞‚Ñícs(.)Figure 2: Illustration of C OSMIC framework. CSK : Commonsense knowledge from COMET. In practice we useBidirectional GRU cells. However, for clarity unidirectional cells are shown in the sketch.",
        "et al., 2014). GRU cells take as input ytand updateits hidden state from ht\u00001tohtusing the transfor-mation:ht=GRU (ht\u00001;yt). New hidden statehtalso serves as the output of the current step.",
        "The cell is parameterized by weights Wand biasesbof appropriate sizes depending upon the inputytand outputht. We use five Bidirectional GRUcellsGRU C;GRU Q;GRU R;GRU I, andGRU Efor modeling context state, internal state, externalstate, intent state, and emotion state respectively.",
        "For ease of representation we formulate the differ-ent states with unidirectional GRU cells here.",
        "Context State: The context state stores and prop-agates the overall utterance-level information alongthe sequence of the conversation flow. This state isupdated using context GRU cell GRU Cafter eachtime-steptwhen the utterance is uttered by someparticipantps(ut). RoBERTa feature vector xt, in-ternal stateqs(ut);t\u00001, and external state rs(ut);t\u00001of the speaker from the immediate previous time-step (just before uttering the utterance) are con-catenated and serve as the input vector for GRU C.",
        "ct=GRU C(ct\u00001;(xt\bqs(ut);t\u00001\brs(ut);t\u00001))",
        "We also pool attention vector atfrom the history ofcontext [c1;c2;:::;c t\u00001]using soft-attention. Thisattention vector is later used to perform updates oninternal and external states.",
        "ui=tanh(Wsci+bs); i2[1;t\u00001]",
        "i=exp(uTixi)Pt\u00001i=1exp(uTixi)",
        "at=t\u00001Xi=1ici (2)",
        "Internal State: The internal state of the partici-pants is conditioned on how the individual is feel-ing and what is the effect perceived from otherparticipants. This state may remain concealed, asparticipants may not always express explicitly theirfeeling or outlook through external stance or re-actions. Apart from feelings, this state can alsobe considered to include aspects that the partici-pant actively tries not to express or features that areconsidered common knowledge and don't requireexplicit communication. The effect on oneself isthus elemental to represent the internal state of theparticipants. We model the internal state of the par-ticipants using GRU Q. For time-step t, the internalstate of the speaker ps(ut)is updated by taking intoaccount the attention vector atand commonsensevector effect on speakerEScs(ut)",
        "qs(ut);t=GRU Q(qs(ut);t\u00001;(at\bES cs(ut)))",
        "2475For all the other participants apart from the speaker,this update is performed using effect on listenersELcs(ut).",
        "qj;t=GRU Q(qj;t\u00001;(at\bEL cs(ut)));8j6=s(ut)",
        "External State: Unlike the internal state, the ex-ternal state of the participants is all about the ex-pressions, reactions, and responses. Naturally, thisstate can be easily seen, felt, or understood by theother participants. For instance, the actual utter-ance, the manner of articulation, the speech, andother acoustic features, the visual expression, ges-tures, and stance can all be loosely considered tofall under the regime of external state. GRU Rup-dates the external state of the speaker ps(ut)bytaking as input the concatenation of attention vec-torat, utterance vector xtand commonsense vectorreaction of speaker RScs(ut)",
        "rs(ut);t=GRU R(rs(ut);t\u00001;(at\bxt\bRS cs(ut)))",
        "For listeners, this update is performed using reac-tion of listenersRLcs(ut).",
        "rj;t=GRU R(rj;t\u00001;(at\bxt\b RL cs(ut)));",
        "Intent State: Intent is a mental state that repre-sents the commitment to carry out a particular setof actions. The intent of the speaker always plays acrucial role in determining the emotional dynamicsof a conversation. The intent of the speaker changesfromis(ut);t\u00001tois(ut);tat time-stept. This changeis invoked by the commonsense intent of speakervectorIScs(ut)and internal speaker state qs(ut);tat that respective time-step t. The intent states arecaptured by GRU cell GRU I:",
        "is(ut);t=GRU I(is(ut);t\u00001;(IScs(ut)\bqs(ut);t))",
        "The intent of the listener(s), however, is kept un-changed. This is because the intent of a participantwho is silent should not change. The change shouldoccur only when the particular participant speaksagain.",
        "Emotion State: The emotional state determinesthe emotional mood of the speaker and the emotionclass of the utterance. We posit that the emotionalstate depends upon the utterance and compositestate of the speaker that takes into account the inter-nal, external, and intent state. Naturally the currentemotion state also depends on the previous emotionstate of the speaker. GRU Ecaptures the emotionstate by combining all of the factors as following,et=GRU E(et\u00001;(xt\bqs(ut);t\brs(ut);t\bis(ut);t))",
        "Emotion Classification: Finally all the utter-ances in the conversation are classified with a fullyconnected network from etPt=softmax (Wsmaxet+bsmax);8t2[1;N]",
        "^yt=argmaxk(Pt[k]) (10)",
        "4 Experimental Setup4.1 DatasetsDataset#dialogues #utterancestrain val test train val testIEMOCAP 120 12 31 5810 1623DailyDialog 11,118 1,000 1,000 87,832 7,912 7,863MELD 1,039 114 280 9,989 1,109 2,610EmoryNLP 659 89 79 7,551 954 984Dataset #classes MetricIEMOCAP 6 Weighted Avg. F1DailyDialog 7* Macro F1 and Micro F1MELD 3 and 7 Weighted Avg. F1 over 3 and 7 classesEmoryNLP 3 and 7 Weighted Avg. F1 over 3 and 7 classesTable 3: Statistics of splits and evaluation metrics usedin different datasets. In MELD and EmoryNLP evalu-ation is performed for 3 class (broad) and 7 class (fine-grained) classification. Neutral * classes constitutes to83% of the DailyDialog dataset. These are excludedwhen calculating the Micro F1 score.",
        "We benchmark COSMIC on four different con-versational emotion recognition datasets: i) IEMO-CAP (Busso et al., 2008) ii) MELD (Poria et al.,2019a) iii) DailyDialog (Li et al., 2017), and iv)",
        "EmoryNLP (Zahiri and Choi, 2018). IEMOCAPand DailyDialog are two-party datasets, whereasMELD and EmoryNLP are multi-party datasets.",
        "We report experimental results for conversationalemotion recognition from the textual informationfor all four datasets. Information about the datasetsis shown in Table 3.",
        "IEMOCAP (Busso et al., 2008) is a datasetof two person conversations among ten differentunique speakers. The train set dialogues come fromthe first eight speakers, whereas the test set dia-logues are from the last two. Each utterance is",
        "2476annotated with one of the following six emotions:",
        "happy, sad, neutral, angry, excited, andfrustrated .",
        "DailyDialog (Li et al., 2017) covers various top-ics about our daily life and follows the natural hu-man communication approach. All utterances arelabeled with both emotion categories and dialogueacts. The emotion can belong to one of the follow-ing seven labels: anger, disgust, fear, joy, neutral,sadness , and surprise . The dataset has over 83%neutral labels and these are excluded during Micro-F1 evaluation.",
        "MELD (Poria et al., 2019a) is a multimodaldataset extended from the EmotionLines dataset(Chen et al., 2018). MELD is collected from theTV show Friends and has more than 1400 dia-logues and 13000 utterances. Utterances are la-beled with emotion and sentiment classes. Theemotion classes belong to anger, disgust, sadness,joy, surprise, fear , orneutral , and the sentimentclasses belong to positive, negative orneutral .",
        "EmoryNLP (Zahiri and Choi, 2018) is anotherdataset also based on the show Friends . Utterancesin this dataset are annotated on seven and threeemotion classes. The seven emotion classes areneutral, joyful, peaceful, powerful, scared, mad andsad. To create three emotion classes: joyful, peace-ful, and powerful are grouped together to form thepositive class; scared, mad andsadare groupedtogether to form the negative class; and the neutralclass is kept unchanged.",
        "4.2 Training SetupFor context independent feature extraction, theRoBERTa model is fine-tuned on the set of all ut-terances and their emotion labels in the trainingdata. We fine-tune the RoBERTa model for a batchsize of 32 utterances with Adam optimizer withlearning rate of 1e-5. In the case of MELD andEmoryNLP datasets, we use a residual connectionbetween the first and the penultimate layer whichbrings more stability in the training in the emotionrecognition model. The emotion recognition modelis trained with Adam optimizer having a learningrate of 1e-4.",
        "5 Results and Analysis5.1 Baseline and State-of-the art MethodsFor a comprehensive evaluation of COSMIC ,we compare it against the following methods:",
        "CNN (Kim, 2014) is a convolutional neural net-work model trained on top of pretrained GloVeembeddings. Standard configurations of filter sizesare used. The model is trained at the utterancelevel to predict the emotion classes. ICON (Haz-arika et al., 2018b) uses two GRU networks to learnthe utterance representations for dialogues betweentwo-participants. The output of the two speakerGRUs is then connected using another GRU thathelps in performing explicit inter-speaker model-ing. ICON is limited to conversations with onlytwo participants only. KET (Zhong et al., 2019)",
        "or Knowledge enriched transformers dynamicallyleverages external commonsense knowledge usinghierarchical self-attention and context aware graphattention. ConGCN (Zhang et al., 2019) consid-ers utterances and participants of a conversationas nodes of graph network and models both con-text and speaker sensitive dependence for emotiondetection. BERT DCR-Net (Qin et al., 2020) isa deep co-interactive relation network that usesBERT based features for joint dialogue act recog-nition and emotion (sentiment) classification. Arelation layer learns to explicitly model the rela-tion and interaction between these two tasks in amulti-task setting. BERT+MTL (Li et al., 2020)",
        "is a multi-task learning framework where featuresextracted from BERT are used in a recurrent neuralnetwork for emotion recognition and speaker iden-tification. DialogueRNN (Majumder et al., 2019)",
        "models the emotion of utterances in a conversa-tion with speaker, context and emotion informationfrom neighbour utterances. These factors are mod-eled using three separate GRU networks to keeptrack of the individual speaker states.",
        "We report and compare the performance ofCOSMIC on test data in Table 4. State-of-the-artmodels use GloVe embeddings to extract context-independent features. As features extracted fromtransformer based networks such as BERT andRoBERTa generally outperform traditional wordembeddings such as word2vec and GloVe, we alsoreport results of the models when used with BERTor RoBERTa features.",
        "5.2 Comparison with the State-of-the-ArtMethodsIEMOCAP and DailyDialog: IEMOCAP andDailyDialog contain dyadic conversations withmostly natural and coherent utterances. We observethat RoBERTa features improve the DialogueRNNmodels, and other BERT based models performsimilarly. COSMIC improves over all the models,",
        "2477MethodsIEMOCAP DailyDialog MELD EmoryNLPW-Avg F1 Macro F1 Micro F1W-AvgF1 (3-cls)W-AvgF1 (7-cls)W-AvgF1 (3-cls)W-AvgF1 (7-cls)GloVe-basedCNN 52.04 36.87 50.32 64.25 55.02 38.05 32.59ICON 58.54 - - - - - -KET 59.56 - 53.37 - 58.18 - 34.39ConGCN - - - - 57.40 - -DialogueRNN 62.57 41.80 55.95 66.10 57.03 48.93 31.70(Ro)BERT(a)-basedBERT DCR-Net - 48.90 - - - - -BERT+MTL - - - - 61.90 - 35.92RoBERTa 54.55 48.20 55.16 72.12 62.02 55.28 37.29RoBERTa DialogueRNN 64.76 49.65 57.32 72.14 63.61 55.36 37.44COSMIC 65.28 51.05 58.48 73.20 65.21 56.51 38.11w/o Speaker CSK 63.27 50.18 57.45 72.94 64.41 55.46 37.35w/o Listener CSK 65.05 48.67 58.28 72.90 64.76 56.57 38.15w/o Speaker, Listener CSK 63.05 48.68 56.16 72.62 64.28 55.34 37.10Table 4: Comparison of results against various methods. Scores are average of five runs. Test scores are computedat best validation scores. C OSMIC achieves new state-of-the-art results across all the datasets. CSK refers tocommonsense knowledge components from COMET. We report the average score of the 10 runs for RoBERTaDialogueRNN and COSMIC. The CNN and DialogueRNN scores using Glove embeddings are obtained from(Ghosal et al., 2020).",
        "however the improvement on IEMOCAP is not aslarge as it is on DailyDialog. COSMIC achievesnew state-of-the-art scores of 65.28 on IEMOCAP;",
        "51.05 and 58.48 in DailyDialog for the two differ-ent evaluation metrics.",
        "MELD and EmoryNLP: These two datasetshave been annotated from the TV show Friends,and utterances are often very short. Although dia-logues occasionally contain emotion specific words,this does not happen very often at the utterancelevel. Naturally, emotion dynamics are highly con-textual in nature and almost always depend on sur-rounding utterances. It has been observed in previ-ous work that emotion modeling in MELD is diffi-cult because often there are a lot of speakers in eachconversation but they utter only a small numberof utterances. Sophisticated models such as Dia-logueRNN do not bring as much improvement overCNN as they do on IEMOCAP. We observe that,COSMIC brings a large improvement over othermodels on the fine-grained (7 class) classificationsetup for both datasets. It achieves new state-of-the-art weighted F1 scores of 73.20 and 56.51 onthree class classification; 65.21 and 38.11 on sevenclass classification on MELD and EmoryNLP.",
        "5.3 The Role of CommonsenseIn Table 4, we also report results of ablation studiesby removing listener-specific and speaker-specificcommonsense components. For speaker ablation,we discardIScs(ut);EScs(ut);RScs(ut), and ob-serve a sharp drop in performance in most cases.",
        "For listener ablation, we discard ELcs(ut);andRLcs(ut)and find that the performance also dropsbut not as much as the speaker ablation. In fact,listener ablation leads to slight improvement in per-formance in EmoryNLP. The results suggest thatspeaker-specific commonsense has a greater impactin the overall performance of COSMIC , which isexpected because we are predicting the emotionclass of the speaker at each utterance. Finally, abla-tion with respect to both components at the sametime naturally leads to higher drop in overall per-formance.",
        "5.4 Case StudyWe illustrate a case study on a test conversationinstance from the IEMOCAP dataset in Figure 3.",
        "The conversation begins with a couple of neutralutterances, but then the situation quickly escalates,and finally, it ends with a lot of angry andfrustratedutterances from both the speakers. State-of-the-artmodels like DialogueRNN often find this kind ofscenarios difficult, when there is a couple of suddenemotions shifts in between ( neutral tofrustratedand then neutral again). These models also tend tomisclassify utterances that have subtle differencesin emotion classes such as frustrated andangry . InCOSMIC , the propagation of commonsense knowl-edge makes it easier for the model to handle the",
        "2478Why does that bother you?",
        "AnnoyedUpsetFrowns‚ÑõùíÆcs:‚Ñõ‚Ñícs:‚Ñ∞‚Ñícs:",
        "Look, it's a beautiful day outside, why are we arguing?",
        "Gets tiredGets yelled atIrritated‚ÑõùíÆcs:‚Ñ∞‚Ñícs:‚Ñõ‚Ñícs:What have I got to hide, Kate?  What the hell is the matter with you?  To be in controlConfused, UpsetBecomes annoyed, frustrated                                 ‚ÑêùíÆcs:‚Ñ∞ùíÆcs:‚Ñõ‚Ñícs:Well, what do you want me to do about it?  What do you want?",
        "To help outThinks about what to doBecomes angry, annoyed ‚ÑêùíÆcs:‚Ñ∞ùíÆcs:‚Ñõ‚Ñícs:NeutralAngryAngryFrustratedNeutralFrustratedAngryFigure 3: Case study from the IEMOCAP dataset. Discrete commonsense sequences are shown for more inter-pretability. Commonsense knowledge helps in predicting emotion shifts and understanding difference betweenclosely related emotion classes such as angry andfrustrated .",
        "sudden transitions and to understand the subtle dif-ference between closely related emotion classes. InFigure 3, for the first utterance, the commonsensemodel predicts that the reaction of speaker isan-noyed and propagation of this information helps inpredicting that the speaker's next utterance actuallybelongs to the frustrated class. Similarly for therest of the illustrated utterances, the commonsenseknowledge from effect on speaker andreaction oflistener helps the model in distinguishing and pre-dicting the anger andfrustrated classes correctly.",
        "5.5 Strategies to Incorporate CommonsenseApart from the five commonsense features that weuse in COSMIC (Table 1), there are four other fea-tures that can be extracted from COMET: attributeof speaker ,need of speaker ,wanted by speaker ,andwanted by listeners . We incorporate them us-ing different strategies that add extra complexity inour framework but ultimately do not improve theperformance by a significant margin. We experi-mented along the following directions:",
        "\u000fAttribute of speaker is loosely considered as apersonality trait. This latent variable influenced theinternal, external andintent states . We find that thediscrete attribute features from COMET are mostlya single word like ‚Äòstubborn', ‚Äòpatient', ‚Äòargumen-tative', ‚Äòcalm', etc and they change quite abruptlyfor the same participant in continuing utterances.",
        "Hence, we find that their vectorized representationsdo not help much.\u000fNeed of speaker ,wanted by speaker , andwanted by listeners are considered as output vari-ables that are to be predicted from the input ut-terance and the five basic commonsense features(Table 1). We add auxiliary output functions andjointly optimize the emotion classification loss withmean-squared loss between predictions and refer-ence commonsense vectors. This strategy also doesnot help much in improving the emotion classifica-tion performance.",
        "Although the performance improvement is ob-served using commonsense knowledge across thedatasets, this improvement is not very substantial.",
        "In the future, we plan to identify better common-sense knowledge sources and develop models thatcan infuse this knowledge into deep learning mod-els more efficiently.",
        "6 ConclusionIn this work, we presented COSMIC , a frame-work that models various aspects of commonsenseknowledge by considering mental states, events, ac-tions, and cause-effect relations for emotion recog-nition in conversations. Using commonsense rep-resentations, our model alleviates issues such asdifficulty in detecting emotion shifts and misclas-sification between related emotion classes that areoften present in current RNN and GCN based meth-ods. COSMIC achieves new state-of-the-art resultsfor emotion recognition across several benchmarkdatasets."
    ]
}
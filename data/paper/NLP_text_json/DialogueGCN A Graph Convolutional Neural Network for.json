{
    "Abstract": "Emotion recognition in conversation (ERC)has received much attention, lately, from re-searchers due to its potential widespread ap-plications in diverse areas, such as health-care,education, and human resources. In this pa-per, we present Dialogue Graph ConvolutionalNetwork (DialogueGCN), a graph neural net-work based approach to ERC. We leverage selfand inter-speaker dependency of the interlocu-tors to model conversational context for emo-tion recognition. Through the graph network,DialogueGCN addresses context propagationissues present in the current RNN-based meth-ods. We empirically show that this method al-leviates such issues, while outperforming thecurrent state of the art on a number of bench-mark emotion classification datasets.",
    "Keywords": null,
    "Body": [
        "IntroductionEmotion recognition has remained an active re-search topic for decades (K. D'Mello et al., 2006;",
        "Busso et al., 2008; Strapparava and Mihalcea,2010). However, the recent proliferation of openconversational data on social media platforms,such as Facebook, Twitter, Youtube, and Red-dit, has warranted serious attention (Poria et al.,2019b; Majumder et al., 2019; Huang et al., 2019)",
        "from researchers towards emotion recognition inconversation (ERC). ERC is also undeniably im-portant in affective dialogue systems (as shown inFig. 1) where bots understand users' emotions andsentiment to generate emotionally coherent andempathetic responses.",
        "Recent works on ERC process the constituentutterances of a dialogue in sequence, with a re-current neural network (RNN). Such a scheme isillustrated in Fig. 2 (Poria et al., 2019b), that re-lies on propagating contextual and sequential in-formation to the utterances. Hence, we feed the∗Corresponding authorMy head is aching FrustratedI'm sorry to hear that! SadDid you suffer a head injury earlier? NeutralYes NeutralPlease, immediately see a nearby doctor Excited No NeutralPleased to hear that Happy Did you consume alcohol recently? Neutral UserHealth AssistantYou might be concussed! Sad Figure 1: Illustration of an affective conversation wherethe emotion depends on the context. Health assistantunderstands affective state of the user in order to gen-erate affective and empathetic responses.",
        "conversation to a bidirectional gated recurrent unit(GRU) (Chung et al., 2014). However, like mostof the current models, we also ignore intent mod-elling, topic, and personality due to lack of la-belling on those aspects in the benchmark datasets.",
        "In theory, RNNs like long short-term memory(LSTM) (Hochreiter and Schmidhuber, 1997) andGRU should propagate long-term contextual infor-mation. However, in practice it is not always thecase (Bradbury et al., 2017). This affects the ef-ficacy of RNN-based models in various tasks, in-cluding ERC.",
        "To mitigate this issue, some variants ofthe state-of-the-art method, DialogueRNN (Ma-jumder et al., 2019), employ attention mechanismthat pools information from entirety or part of theconversation per target utterance. However, thispooling mechanism does not consider speaker in-formation of the utterances and the relative po-sition of other utterances from the target utter-ance. Speaker information is necessary for mod-arXiv:1908.11540v1  [cs.CL]  30 Aug 2019",
        "St+1BStAUt+1BUtAPBPATopicIt+1BItAUt−1BU<tA,BU<t−1A,BPerson APerson Btt+1EtAEt+1BIt−2AIt−1BFigure 2: Interaction among different controlling vari-ables during a dyadic conversation between persons Aand B. Grey and white circles represent hidden and ob-served variables, respectively. Prepresents person-ality,Urepresents utterance, Srepresents interlocu-tor state,Irepresents interlocutor intent, Erepresentsemotion and Topic represents topic of the conversation.",
        "This can easily be extended to multi-party conversa-tions.",
        "elling inter-speaker dependency, which enablesthe model to understand how a speaker coercesemotional change in other speakers. Similarly, byextension, intra-speaker or self-dependency aidsthe model with the understanding of emotional in-ertia of individual speakers, where the speakers re-sist the change of their own emotion against ex-ternal influence. On the other hand, considera-tion of relative position of target and context ut-terances decides how past utterances influence fu-ture utterances and vice versa. While past utter-ances influencing future utterances is natural, theconverse may help the model fill in some relevantmissing information, that is part of the speaker'sbackground knowledge but explicitly appears inthe conversation in the future. We leverage thesetwo factors by modelling conversation using a di-rected graph. The nodes in the graph representindividual utterances. The edges between a pairof nodes/utterances represent the dependency be-tween the speakers of those utterances, along withtheir relative positions in the conversation. Byfeeding this graph to a graph convolution network(GCN) (Defferrard et al., 2016), consisting of twoconsecutive convolution operations, we propagatecontextual information among distant utterances.",
        "We surmise that these representations hold richercontext relevant to emotion than DialogueRNN.",
        "This is empirically shown in Section 5.",
        "The remainder of the paper is organized as fol-lows — Section 2 briefly discusses the relevantand related works on ERC; Section 3 elaboratesthe method; Section 4 lays out the experiments;",
        "Section 5 shows and interprets the experimentalresults; and finally, Section 6 concludes the paper.",
        "2 Related WorkEmotion recognition in conversation is a pop-ular research area in natural language process-ing (Kratzwald et al., 2018; Colneri ˆc and Dem-sar, 2018) because of its potential applications ina wide area of systems, including opinion mining,health-care, recommender systems, education, etc.",
        "However, emotion recognition in conversationhas attracted attention from researchers only inthe past few years due to the increase in availabil-ity of open-sourced conversational datasets (Chenet al., 2018; Zhou et al., 2018; Poria et al., 2019a).",
        "A number of models has also been proposedfor emotion recognition in multimodal data i.e.",
        "datasets with textual, acoustic and visual informa-tion. Some of the important works include (Po-ria et al., 2017; Chen et al., 2017; Zadeh et al.,2018a,b; Hazarika et al., 2018a,b), where mainlydeep learning-based techniques have been em-ployed for emotion (and sentiment) recognitionin conversation, in only textual and multimodalsettings. The current state-of-the-art model inemotion recognition in conversation is (Majumderet al., 2019), where authors introduced a partystate and global state based recurrent model formodelling the emotional dynamics.",
        "Graph neural networks have also been verypopular recently and have been applied to semi-supervised learning, entity classification, link pre-diction, large scale knowledge base modelling,and a number of other problems (Kipf andWelling, 2016; Schlichtkrull et al., 2018; Brunaet al., 2013). Early work on graph neural networksinclude (Scarselli et al., 2008). Our graph modelis closely related to the graph relational modellingwork introduced in (Schlichtkrull et al., 2018).",
        "3 MethodologyOne of the most prominent strategies for emotionrecognition in conversations is contextual mod-",
        "elling. We identify two major types of context inERC – sequential context and speaker-level con-text. Following Poria et al. (2017), we model thesetwo types of context through the neighbouring ut-terances, per target utterance.",
        "Computational modeling of context should alsoconsider emotional dynamics of the interlocutorsin a conversation. Emotional dynamics is typi-cally subjected to two major factors in both dyadicand multiparty conversational systems — inter-speaker dependency and self-dependency. Inter-speaker dependency refers to the emotional in-fluence that counterparts produce in a speaker.",
        "This dependency is closely related to the fact thatspeakers tend to mirror their counterparts to buildrapport during the course of a dialogue (Navar-retta et al., 2016). However, it must be taken intoaccount, that not all participants are going to af-fect the speaker in identical way. Each participantgenerally affects each other participants in uniqueways. In contrast, self-dependency, or emotionalinertia, deals with the aspect of emotional influ-ence that speakers have on themselves during con-versations. Participants in a conversation are likelyto stick to their own emotional state due to theiremotional inertia, unless the counterparts invoke achange. Thus, there is always a major interplaybetween the inter-speaker dependency and self-dependency with respect to the emotional dynam-ics in the conversation.",
        "We surmise that combining these two dis-tinct yet related contextual information schemes(sequential encoding and speaker level encod-ing) would create enhanced context representationleading to better understanding of emotional dy-namics in conversational systems.",
        "3.1 Problem DefinitionLet there be Mspeakers/parties p1;p2;:::;pMina conversation. The task is to predict the emo-tion labels ( happy ,sad,neutral ,angry ,excited ,frustrated ,disgust , and fear) of the constituent ut-terancesu1;u2;:::;uN, where utterance uiis ut-tered by speaker ps(ui), whilesbeing the mappingbetween utterance and index of its correspondingspeaker. We also represent ui∈RDmas the featurerepresentation of the utterance, obtained using thefeature extraction process described below.3.2 Context Independent Utterance-LevelFeature ExtractionA convolutional neural network (Kim, 2014) isused to extract textual features from the transcriptof the utterances. We use a single convolutionallayer followed by max-pooling and a fully con-nected layer to obtain the feature representationsfor the utterances. The input to this network isthe 300 dimensional pretrained 840B GloVe vec-tors (Pennington et al., 2014). We use filters ofsize 3, 4 and 5 with 50 feature maps in each. Theconvoluted features are then max-pooled with awindow size of 2 followed by the ReLU activa-tion (Nair and Hinton, 2010). These are then con-catenated and fed to a 100dimensional fully con-nected layer, whose activations form the represen-tation of the utterance. This network is trained atutterance level with the emotion labels.",
        "3.3 ModelWe now present our Dialogue Graph Convolu-tional Network (DialogueGCN1) framework foremotion recognition in conversational setups. Di-alogueGCN consists of three integral components— Sequential Context Encoder, Speaker-LevelContext Encoder, and Emotion Classifier. Anoverall architecture of the proposed framework isillustrated in Fig. 3.",
        "3.3.1 Sequential Context EncoderSince, conversations are sequential by nature, con-textual information flows along that sequence. Wefeed the conversation to a bidirectional gated re-current unit (GRU) to capture this contextual in-formation: gi=←/leftrightline/leftrightline→GRU S(gi(+;−)1;ui), for i = 1,2,. . . , N, where uiandgiare context-independentand sequential context-aware utterance represen-tations, respectively.",
        "Since, the utterances are encoded irrespective ofits speaker, this initial encoding scheme is speakeragnostic, as opposed to the state of the art, Dia-logueRNN (Majumder et al., 2019).",
        "3.3.2 Speaker-Level Context EncoderWe propose the Speaker-Level Context Encodermodule in the form of a graphical network to cap-ture speaker dependent contextual information in aconversation. Effectively modelling speaker levelcontext requires capturing the inter-dependency1Implementation available at https://github.",
        "com/SenticNet/conv-emotion",
        "h1h2h3h4h5g1g2g3g4g5GCN1. Sequential Context Encoding2. Speaker-Level Context Encoding3. ClassificationConcatenationu1u2GRUSGRUSg1g2u3GRUSg3u4GRUSg4u5GRUSg5FeaturesgihiClassifyLabelsSpeaker 2(p2)Speaker 1 (p1)Edge Types:p1→p1p2→p2p1→p2p2→p1Towards pastTowards futureNode Types:×{}}{Figure 3: Overview of DialogueGCN, congruent to the illustration in Table 1.",
        "and self-dependency among participants. We de-sign a directed graph from the sequentially en-coded utterances to capture this interaction be-tween the participants. Furthermore, we proposea local neighbourhood based convolutional fea-ture transformation process to create the enrichedspeaker-level contextually encoded features. Theframework is detailed here.",
        "First, we introduce the following notation: aconversation having Nutterances is representedas a directed graph G=(V;E;R;W), with ver-tices/nodesvi∈V, labeled edges (relations) rij∈Ewherer∈Ris the relation type of the edge be-tweenviandvjandijis the weight of the la-beled edgerij, with 0/uni2A7Dij/uni2A7D1, whereij∈Wandi;j∈[1;2;:::;N].",
        "Graph Construction: The graph is constructedfrom the utterances in the following way,Vertices: Each utterance in the conversation isrepresented as a vertex vi∈VinG. Each vertex viis initialized with the corresponding sequentiallyencoded feature vector gi, for alli∈[1;2;:::;N].",
        "We denote this vector as the vertex feature. Vertexfeatures are subject to change downstream, whenthe neighbourhood based transformation processis applied to encode speaker-level context.",
        "Edges: Construction of the edges Edepends onthe context to be modeled. For instance, if wehypothesize that each utterance (vertex) is con-textually dependent on all the other utterances ina conversation (when encoding speaker level in-formation), then a fully connected graph wouldbe constructed. That is each vertex is connectedto all the other vertices (including itself) with anedge. However, this results in O(N2)number ofedges, which is computationally very expensivefor graphs with large number of vertices. A morepractical solution is to construct the edges by keep-ing a past context window size of pand a futurecontext window size of f. In this scenario, eachutterance vertex vihas an edge with the immedi-ateputterances of the past: vi−1;vi−2;::vi−p,fut-terances of the future: vi+1;vi+2;::vi+fand itself:",
        "vi. For all our experiments in this paper, we con-sider a past context window size of 10 and futurecontext window size of 10.",
        "As the graph is directed, two vertices can haveedges in both directions with different relations.",
        "Edge Weights: The edge weights are set usinga similarity based attention module. The attentionfunction is computed in a way such that, for eachvertex, the incoming set of edges has a sum totalweight of 1. Considering a past context windowsize ofpand a future context window size of f,the weights are calculated as follows,ij=softmax (gTiWe[gi−p;:::;gi+f]);",
        "This ensures that, vertex viwhich has incom-ing edges with vertices vi−p;:::;vi+f(as speaker-level context) receives a total weight contributionof 1.",
        "Relations: The relation rof an edgerijis setdepending upon two aspects:",
        "Speaker dependency — The relation dependson both the speakers of the constituting vertices:",
        "ps(ui)(speaker ofvi) andps(uj)(speaker ofvj).",
        "Temporal dependency — The relation also de-",
        "pends upon the relative position of occurrence ofuiandujin the conversation: whether uiis utteredbeforeujor after. If there are Mdistinct speakersin a conversation, there can be a maximum of M(speaker ofui)∗M(speaker ofuj)∗2(uioccursbeforeujor after)=2M2distinct relation types rin the graph G.",
        "Each speaker in a conversation is uniquely af-fected by each other speaker, hence we hypoth-esize that explicit declaration of such relationaledges in the graph would help in capturing theinter-dependency and self-dependency among thespeakers in play, which in succession would facil-itate speaker-level context encoding.",
        "As an illustration, let two parties p1;p2partici-pate in a dyadic conversation having 5 utterances,whereu1;u3;u5are uttered by p1andu2;u4areuttered byp2. If we consider a fully connectedgraph, the edges and relations will be constructedas shown in Table 1.",
        "Feature Transformation: We now describethe methodology to transform the sequentially en-coded features using the graph network. The ver-tex feature vectors ( gi) are initially speaker inde-pendent and thereafter transformed into a speakerdependent feature vector using a two-step graphconvolution process. Both of these transforma-tions can be understood as special cases of a ba-sic differentiable message passing method (Gilmeret al., 2017).",
        "In the first step, a new feature vector h(1)",
        "iiscomputed for vertex viby aggregating local neigh-bourhood information (in this case neighbour ut-terances specified by the past and future contextwindow size) using the relation specific transfor-mation inspired from (Schlichtkrull et al., 2018):",
        "i=\u001b(/summation.dispr∈R/summation.dispj∈Nriijci;rW(1)",
        "rgj+iiW(1)",
        "where,ijandiiare the edge weights, Nride-notes the neighbouring indices of vertex iunderrelationr∈R.ci;ris a problem specific nor-malization constant which either can be set in ad-vance, such that, ci;r=/divides.alt0Nri/divides.alt0, or can be automat-ically learned in a gradient based learning setup.",
        "\u001bis an activation function such as ReLU, W(1)",
        "randW(1)",
        "0are learnable parameters of the transfor-mation. In the second step, another local neigh-bourhood based transformation is applied over theoutput of the first step,h(2)",
        "i=\u001b(/summation.dispj∈NriW(2)h(1)",
        "where,W(2)andW(2)",
        "0are parameters of thesetransformation and \u001bis the activation function.",
        "This stack of transformations, Eqs. (2) and (3),effectively accumulates normalized sum of the lo-cal neighbourhood (features of the neighbours) i.e.",
        "the neighbourhood speaker information for eachutterance in the graph. The self connection en-sures self dependent feature transformation.",
        "Emotion Classifier: The contextually encodedfeature vectors gi(from sequential encoder) andh(2)",
        "i(from speaker-level encoder) are concate-nated and a similarity-based attention mechanismis applied to obtain the final utterance representa-tion:",
        "i=softmax (hTiW[h1;h2:::;hN]); (5)",
        "Finally, the utterance is classified using a fully-connected network:",
        "li=ReLU(Wl~hi+bl); (7)",
        "Pi=softmax (Wsmaxli+bsmax); (8)",
        "^yi=argmaxk(Pi[k]): (9)",
        "Relation ps(ui),ps(uj)i<j (i;j)",
        "Table 1:ps(ui)andps(uj)denotes the speaker of ut-terancesuianduj. 2 distinct speakers in the conver-sation implies 2∗M2=2∗22=8distinct relationtypes. The rightmost column denotes the indices of thevertices of the constituting edge which has the relationtype indicated by the leftmost column.",
        "Training Setup: We use categorical cross-entropy along with L2-regularization as the mea-sure of loss ( L) during training:",
        "L=−1∑Ns=1c(s)N/summation.dispi=1c(i)",
        "/summation.dispj=1logPi;j[yi;j]+\u0015/parallel.alt1\u0012/parallel.alt12;",
        "whereNis the number of samples/dialogues, c(i)",
        "is the number of utterances in sample i,Pi;jis theprobability distribution of emotion labels for utter-ancejof dialoguei,yi;jis the expected class labelof utterance jof dialoguei,\u0015is the L2-regularizerweight, and \u0012is the set of all trainable parameters.",
        "We used stochastic gradient descent basedAdam (Kingma and Ba, 2014) optimizer to trainour network. Hyperparameters were optimized us-ing grid search.",
        "4 Experimental Setting4.1 Datasets UsedWe evaluate our DialogueGCN model on threebenchmark datasets — IEMOCAP (Busso et al.,2008), A VEC (Schuller et al., 2012), andMELD (Poria et al., 2019a). All these threedatasets are multimodal datasets containing tex-tual, visual and acoustic information for every ut-terance of each conversation. However, in thiswork we focus on conversational emotion recog-nition only from the textual information. Multi-modal emotion recognition is outside the scope ofthis paper, and is left as future work.",
        "IEMOCAP (Busso et al., 2008) dataset con-tains videos of two-way conversations of tenunique speakers, where only the first eight speak-ers from session one to four belong to the train-set. Each video contains a single dyadic dialogue,segmented into utterances. The utterances are an-notated with one of six emotion labels, which arehappy, sad, neutral, angry, excited, and frustrated.",
        "A VEC (Schuller et al., 2012) dataset is amodification of SEMAINE database (McKeownet al., 2012) containing interactions between hu-mans and artificially intelligent agents. Eachutterance of a dialogue is annotated with fourreal valued affective attributes: valence ( [−1;1]),arousal ( [−1;1]), expectancy ( [−1;1]), and power([0;∞)). The annotations are available every0.2 seconds in the original database. However,in order to adapt the annotations to our need ofutterance-level annotation, we averaged the at-tributes over the span of an utterance.",
        "MELD (Poria et al., 2019a) is a multimodalemotion/sentiment classification dataset which hasbeen created by the extending the EmotionLinesdataset (Chen et al., 2018). Contrary to IEMO-CAP and A VEC, MELD is a multiparty dialogdataset. MELD contains textual, acoustic and vi-sual information for more than 1400 dialoguesand 13000 utterances from the Friends TV series.",
        "Each utterance in every dialog is annotated as oneof the seven emotion classes: anger, disgust, sad-ness, joy, surprise, fear or neutral.",
        "Dataset#dialogues #utterancestrain val test train val testIEMOCAP 120 31 5810 1623A VEC 63 32 4368 1430MELD 1039 114 280 9989 1109 2610Table 2: Training, validation and test data distributionin the datasets. No predefined train/val split is providedin IEMOCAP and A VEC, hence we use 10% of thetraining dialogues as validation split.",
        "4.2 Baselines and State of the ArtFor a comprehensive evaluation of DialogueGCN,we compare our model with the following baselinemethods:",
        "CNN (Kim, 2014) This is the baseline convolu-tional neural network based model which is identi-cal to our utterance level feature extractor network(Section 3.2). This model is context independentas it doesn't use information from contextual ut-terances.",
        "Memnet (Sukhbaatar et al., 2015) This is anend-to-end memory network baseline (Hazarikaet al., 2018b). Every utterance is fed to the net-work and the memories, which correspond to theprevious utterances, is continuously updated in amulti-hop fashion. Finally the output from thememory network is used for emotion classifica-tion.",
        "c-LSTM (Poria et al., 2017) Context-aware ut-terance representations are generated by capturingthe contextual content from the surrounding ut-terances using a Bi-directional LSTM (Hochreiterand Schmidhuber, 1997) network. The context-aware utterance representations are then used foremotion classification. The contextual-LSTM",
        "model is speaker independent as it doesn't modelany speaker level dependency.",
        "c-LSTM+Att (Poria et al., 2017) In this variantof c-LSTM, an attention module is applied to theoutput of c-LSTM at each timestamp by followingEqs. (5) and (6). Generally this provides bettercontext to create a more informative final utterancerepresentation.",
        "CMN (Hazarika et al., 2018b) CMN modelsutterance context from dialogue history using twodistinct GRUs for two speakers. Finally, utterancerepresentation is obtained by feeding the currentutterance as query to two distinct memory net-works for both speakers. However, this model canonly model conversations with two speakers.",
        "ICON (Hazarika et al., 2018b) ICON which isan extension of CMN, connects outputs of indi-vidual speaker GRUs in CMN using another GRUfor explicit inter-speaker modeling. This GRU isconsidered as a memory to track the overall con-versational flow. Similar to CMN, ICON can notbe extended to apply on multiparty datasets e.g.,MELD.",
        "DialogueRNN (Majumder et al., 2019) This isthe state-of-the-art method for ERC. It is a recur-rent network that uses two GRUs to track individ-ual speaker states and global context during theconversation. Further, another GRU is employedto track emotional state through the conversation.",
        "DialogueRNN claims to model inter-speaker rela-tion and it can be applied on multiparty datasets.",
        "5 Results and Discussions5.1 Comparison with State of the Art andBaselineWe compare the performance of our proposed Di-alogueGCN framework with the state-of-the-artDialogueRNN and baseline methods in Tables 3and 4. We report all results with average of 5 runs.",
        "Our DialogueGCN model outperforms the SOTAand all the baseline models, on all the datasets,while also being statistically significant under thepaired t-test (p <0.05).",
        "IEMOCAP and A VEC: On the IEMOCAPdataset, DialogueGCN achieves new state-of-the-art average F1-score of 64.18% and accuracy of65.25%, which is around 2% better than Dia-logueRNN, and at least 5% better than all the otherbaseline models. Similarly, on A VEC dataset, Di-alogueGCN outperforms the state-of-the-art on allthe four emotion dimensions: valence, arousal, ex-pectancy, and power.",
        "To explain this gap in performance, it is im-portant to understand the nature of these mod-els. DialogueGCN and DialogueRNN both tryto model speaker-level context (albeit differently),whereas, none of the other models encode speaker-level context (they only encode sequential con-text). This is a key limitation in the baseline mod-els, as speaker-level context is indeed very impor-tant in conversational emotion recognition.",
        "As for the difference of performance betweenDialogueRNN and DialogueGCN, we believe thatthis is due to the different nature of speaker-levelcontext encoding. DialogueRNN employs a gatedrecurrent unit (GRU) network to model individ-ual speaker states. Both IEMOCAP and A VECdataset has many conversations with over 70 utter-ances (the average conversation length is 50 utter-ances in IEMOCAP and 72 in A VEC). As recur-rent encoders have long-term information propa-gation issues, speaker-level encoding can be prob-lematic for long sequences like those found inthese two datasets. In contrast, DialogueGCN triesto overcome this issue by using neighbourhoodbased convolution to model speaker-level context.",
        "MELD: The MELD dataset consists of multi-party conversations and we found that emotionrecognition in MELD is considerably harder tomodel than IEMOCAP and A VEC - which onlyconsists of dyadic conversations. Utterances inMELD are much shorter and rarely contain emo-tion specific expressions, which means emotionmodelling is highly context dependent. Moreover,the average conversation length is 10 utterances,with many conversations having more than 5 par-ticipants, which means majority of the participantsonly utter a small number of utterances per con-versation. This makes inter-dependency and self-dependency modeling difficult. Because of thesereasons, we found that the difference in results be-tween the baseline models and DialogueGCN isnot as contrasting as it is in the case of IEMOCAPand A VEC. Memnet, CMN, and ICON are notsuitable for this dataset as they exclusively work indyadic conversations. Our DialogueGCN modelachieves new state-of-the-art F1 score of 58.10%outperforming DialogueRNN by more than 1%.",
        "MethodsIEMOCAPHappy Sad Neutral Angry Excited Frustrated Average(w)",
        "Speaker 1 There's nothing I can do for you, ma'am. angrySpeaker 2I don't-frustratedSpeaker 2I don't have time for this today, and-frustrated(a)",
        "Okay frustrated I mean we've gone through all of this.  I've been to five people already who—frustratedYes, lots of really like -- sentimental value only, but—frustrated (b)",
        "Figure 4: Visualization of edge-weights in Eq. (1) — (a) Target utterance attends to other speaker's utterance forcorrect context; (b) Short utterance attends to appropriate contextual utterances to be classified correctly.",
        "We surmise that this improvement is a result of thespeaker dependent relation modelling of the edgesin our graph network which inherently improvesthe context understanding over DialogueRNN.",
        "5.2 Effect of Context WindowWe report results for DialogueGCN model in Ta-bles 3 and 4 with a past and future context windowsize of (10, 10) to construct the edges. We also car-ried out experiments with decreasing context win-dow sizes of (8, 8), (4, 4), (0, 0) and found thatperformance steadily decreased with F1 scores of62.48%, 59.41% and 55.80% on IEMOCAP. Di-alogueGCN with context window size of (0, 0) isequivalent to a model with only sequential encoder(as it only has self edges), and performance is ex-pectedly much worse. We couldn't perform exten-sive experiments with larger windows because ofcomputational constraints, but we expect perfor-mance to improve with larger context sizes.",
        "5.3 Ablation StudyWe perform ablation study for different level ofcontext encoders, namely sequential encoder andspeaker-level encoder, in Table 5. We removethem one at a time and found that the speaker-level encoder is slightly more important in over-all performance. This is due to speaker-level en-coder mitigating long distance dependency issueof sequential encoder and DialogueRNN. Remov-ing both of them results in a very poor F1 scoreof 36.7 %, which demonstrates the importanceof contextual modelling in conversational emotionrecognition.",
        "Further, we study the effect of edge relationmodelling. As mentioned in Section 3.3.2, thereare total 2M2distinct edge relations for a con-versation with Mdistinct speakers. First we re-moved only the temporal dependency (resultinginM2distinct edge relations), and then only thespeaker dependency (resulting in 2distinct edgerelations) and then both (resulting in a single edgerelation all throughout the graph). The resultsof these tests in Table 6 show that having thesedifferent relational edges is indeed very impor-tant for modelling emotional dynamics. These re-sults support our hypothesis that each speaker ina conversation is uniquely affected by the others,and hence, modelling interlocutors-dependency isrudimentary. Fig. 4a illustrates one such instancewhere target utterance attends to other speaker'sutterance for context. This phenomenon is com-",
        "MethodsA VECMELDValence Arousal Expectancy PowerCNN 0.545 0.542 0.605 8.71 55.02Memnet 0.202 0.211 0.216 8.97 -bc-LSTM 0.194 0.212 0.201 8.90 56.44bc-LSTM+Att 0.189 0.213 0.190 8.67 56.70CMN 0.192 0.213 0.195 8.74 -ICON 0.180 0.190 0.180 8.45 -DialogueRNN 0.168 0.165 0.175 7.90 57.03DialogueGCN 0.157 0.161 0.168 7.68 58.10Table 4: Comparison with the baseline methods on A VEC and MELD dataset; MAE and F1 metrics are user forA VEC and MELD, respectively.",
        "SequentialEncoderSpeaker-LevelEncoderF13 3 64.183 7 55.307 3 56.717 7 36.75Table 5: Ablation results w.r.t the contextual encodermodules on IEMOCAP dataset.",
        "SpeakerDependencyEdgesTemporalDependencyEdgesF13 3 64.183 7 62.527 3 61.037 7 60.11Table 6: Ablation results w.r.t the edge relations inspeaker-level encoder module on IEMOCAP dataset.",
        "monly observable for DialogueGCN, as comparedto DialogueRNN.",
        "5.4 Performance on Short UtterancesEmotion of short utterances, like “okay”, “yeah”,depends on the context it appears in. For exam-ple, without context “okay” is assumed ‘neutral'.",
        "However, in Fig. 4b, DialogueGCN correctly clas-sifies “okay” as ‘frustration', which is apparentfrom the context. We observed that, overall, Di-alogueGCN correctly classifies short utterances,where DialogueRNN fails.5.5 Error AnalysisWe analyzed our predicted emotion labels andfound that misclassifications are often among sim-ilar emotion classes. In the confusion matrix, weobserved that our model misclassifies several sam-ples of ‘frustrated' as ‘angry' and ‘neutral'. Thisis due to subtle difference between frustration andanger. Further, we also observed similar misclassi-fication of ‘excited' samples as ‘happy' and ‘neu-tral'. All the datasets that we use in our experi-ment are multimodal. A few utterances e.g., ‘ok.",
        "yes' carrying non-neutral emotions were misclas-sified as we do not utilize audio and visual modal-ity in our experiments. In such utterances, wefound audio and visual (in this particular exam-ple, high pitched audio and frowning expression)",
        "modality providing key information to detect un-derlying emotions ( frustrated in the above utter-ance) which DialogueGCN failed to understand byjust looking at the textual context."
    ]
}
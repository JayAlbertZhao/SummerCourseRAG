{
    "Abstract": "Emotion detection in conversations is a necessary step for anumber of applications, including opinion mining over chathistory, social media threads, debates, argumentation mining,understanding consumer feedback in live conversations, andso on. Currently systems do not treat the parties in the conver-sation individually by adapting to the speaker of each utter-ance. In this paper, we describe a new method based on recur-rent neural networks that keeps track of the individual partystates throughout the conversation and uses this informationfor emotion classification. Our model outperforms the state-of-the-art by a significant margin on two different datasets.",
    "Keywords": null,
    "Body": [
        "IntroductionEmotion detection in conversations has been gaining in-creasing attention from the research community due to itsapplications in many important tasks such as opinion min-ing over chat history and social media threads in YouTube,Facebook, Twitter, and so on. In this paper, we present amethod based on recurrent neural networks (RNN) that cancater to these needs by processing the huge amount of avail-able conversational data.",
        "Current systems, including the state of the art (Hazarika etal. 2018), do not distinguish different parties in a conversa-tion in a meaningful way. They are not aware of the speakerof a given utterance. In contrast, we model individual partieswith party states, as the conversation flows, by relying on theutterance, the context, and current party state. Our model isbased on the assumption that there are three major aspectsrelevant to the emotion in a conversation: the speaker, thecontext from the preceding utterances, and the emotion ofthe preceding utterances. These three aspects are not neces-sarily independent, but their separate modeling significantlyoutperforms the state-of-the-art (Table 2). In dyadic conver-sations, the parties have distinct roles. Hence, to extract thecontext, it is crucial to consider the preceding turns of boththe speaker and the listener at a given moment (Fig. 1).",
        "Our proposed DialogueRNN system employs three gatedrecurrent units (GRU) (Chung et al. 2014) to model theseaspects. The incoming utterance is fed into two GRUs calledglobal GRU and party GRU to update the context and partyCopyright © 2019, Association for the Advancement of ArtificialIntelligence (www.aaai.org). All rights reserved.state, respectively. The global GRU encodes correspondingparty information while encoding an utterance.",
        "Attending over this GRU gives contextual representationthat has information of all preceding utterances by differentparties in the conversation. The speaker state depends on thiscontext through attention and the speaker's previous state.",
        "This ensures that at time t, the speaker state directly gets in-formation from the speaker's previous state and global GRUwhich has information on the preceding parties. Finally, theupdated speaker state is fed into the emotion GRU to decodethe emotion representation of the given utterance, which isused for emotion classification. At time t, the emotion GRUcell gets the emotion representation of t−1and the speakerstate oft.",
        "The emotion GRU, along with the global GRU, plays apivotal role in inter-party relation modeling. On the otherhand, party GRU models relation between two sequentialstates of the same party. In DialogueRNN, all these threedifferent types of GRUs are connected in a recurrent manner.",
        "We believe that DialogueRNN outperforms state-of-the-artcontextual emotion classifiers such as (Hazarika et al. 2018;",
        "Poria et al. 2017) because of better context representation.",
        "The rest of the paper is organized as follows: Section 2discusses related work; Section 3 provides detailed descrip-tion of our model; Sections 4 and 5 present the experimentalresults; finally, Section 6 concludes the paper.",
        "2 Related WorkEmotion recognition has attracted attention in various fieldssuch as natural language processing, psychology, cogni-tive science, and so on (Picard 2010). Ekman (1993) foundcorrelation between emotion and facial cues. Datcu andRothkrantz (2008) fused acoustic information with visualcues for emotion recognition. Alm, Roth, and Sproat (2005)",
        "introduced text-based emotion recognition, developed inthe work of Strapparava and Mihalcea (2010). W ¨ollmer etal. (2010) used contextual information for emotion recog-nition in multimodal setting. Recently, Poria et al. (2017)",
        "successfully used RNN-based deep networks for multi-modal emotion recognition, which was followed by otherworks (Chen et al. 2017; Zadeh et al. 2018a; 2018b).",
        "Reproducing human interaction requires deep under-standing of conversation. Ruusuvuori (2013) states thatemotion plays a pivotal role in conversations. It has been ar-arXiv:1811.00405v4  [cs.CL]  25 May 2019",
        "She's been in New York three and a half years. Why all of the sudden? [ neutral ]Why does that bother you? [ neutral ]",
        "What's going on here Joe? [frustrated]PAMaybe he just wanted to see her again? [ neutral ]He lived next door to the girl all his life, why  wouldn't he want to see her again? [ neutral ]How do you know he is even thinking about it? [frustrated]PBSLLLSSLSSSSLspeakerlistenerLFigure 1: In this dialogue, PA's emotion changes are influ-enced by the behavior of PB.",
        "gued that emotional dynamics in a conversation is an inter-personal phenomenon (Richards, Butler, and Gross 2003).",
        "Hence, our model incorporates inter-personal interactions inan effective way. Further, since conversations have a natu-ral temporal nature, we adopt the temporal nature throughrecurrent network (Poria et al. 2017).",
        "Memory networks (Sukhbaatar et al. 2015) has beensuccessful in several NLP areas, including question an-swering (Sukhbaatar et al. 2015; Kumar et al. 2016), ma-chine translation (Bahdanau, Cho, and Bengio 2014), speechrecognition (Graves, Wayne, and Danihelka 2014), and soon. Thus, Hazarika et al. (2018) used memory networksfor emotion recognition in dyadic conversations, where twodistinct memory networks enabled inter-speaker interaction,yielding state-of-the-art performance.",
        "3 Methodology3.1 Problem DefinitionLet there be Mparties/participants p1;p2;:::;pM(M=2for the datasets we used) in a conversation. The task is to pre-dict the emotion labels ( happy ,sad,neutral ,angry ,excited ,andfrustrated ) of the constituent utterances u1;u2;:::;uN,where utterance utis uttered by party ps(ut), whilesbeingthe mapping between utterance and index of its correspond-ing party. Also, ut∈RDmis the utterance representation,obtained using feature extractors described below.",
        "3.2 Unimodal Feature ExtractionFor a fair comparison with the state-of-the-art method, con-versational memory networks (CMN) (Hazarika et al. 2018),we follow identical feature extraction procedures.",
        "Textual Feature Extraction We employ convolutionalneural networks (CNN) for textual feature extraction. Fol-lowing Kim (2014), we obtain n-gram features from eachutterance using three distinct convolution filters of sizes 3,4, and 5 respectively, each having 50 feature-maps. Outputsare then subjected to max-pooling followed by rectified lin-ear unit (ReLU) activation. These activations are concate-nated and fed to a 100dimensional dense layer, which is re-garded as the textual utterance representation. This networkis trained at utterance level with the emotion labels.Audio and Visual Feature Extraction Identical to Haz-arika et al. (2018), we use 3D-CNN and openSMILE (Ey-ben, W ¨ollmer, and Schuller 2010) for visual and acousticfeature extraction, respectively.",
        "3.3 Our ModelWe assume that the emotion of an utterance in a conversationdepends on three major factors:",
        "1. the speaker.",
        "2. the context given by the preceding utterances.",
        "3. the emotion behind the preceding utterances.",
        "Our model DialogueRNN,1shown in Fig. 2a, models thesethree factors as follows: each party is modeled using a partystate which changes as and when that party utters an utter-ance. This enables the model to track the parties' emotiondynamics through the conversations, which is related to theemotion behind the utterances. Furthermore, the context ofan utterance is modeled using a global state (called global,because of being shared among the parties), where the pre-ceding utterances and the party states are jointly encodedfor context representation, necessary for accurate party staterepresentation. Finally, the model infers emotion represen-tation from the party state of the speaker along with the pre-ceding speakers' states as context. This emotion representa-tion is used for the final emotion classification.",
        "We use GRU cells (Chung et al. 2014) to update the statesand representations. Each GRU cell computes a hidden statedefined asht=GRU∗(ht−1;xt), wherextis the currentinput andht−1is the previous GRU state. htalso serves asthe current GRU output. We provide the GRU computationdetails in the supplementary. GRUs are efficient networkswith trainable parameters: W{r;z;c }",
        "∗;{h;x}andb{r;z;c }",
        "We model the emotion representation of the current ut-terance as a function of the emotion representation of theprevious utterance and the state of the current speaker. Fi-nally, this emotion representation is sent to a softmax layerfor emotion classification.",
        "Global State (Global GRU) Global state aims to capturethe context of a given utterance by jointly encoding utteranceand speaker state. Each state also serves as speaker-specificutterance representation. Attending on these states facilitatesthe inter-speaker and inter-utterance dependencies to pro-duce improved context representation. The current utteranceutchanges the speaker's state from qs(ut);t−1toqs(ut);t. Wecapture this change with GRU cell GRU Gwith output sizeDG, usingutandqs(ut);t−1:",
        "whereDGis the size of global state vector, DPis thesize of party state vector, W{r;z;c }",
        "G;h∈RDG×DG,W{r;z;c }",
        "G;x∈RDG×(Dm+DP),b{r;z;c }",
        "G∈RDG,qs(ut);t−1∈RDP,gt;gt−1∈RDG,DPis party state size, and ⊕represents concatenation.",
        "1Implementation available at https://github.com/senticnet/conv-emotion",
        "GRUEGRUEqA,tqB,t+1et−1Emotion representationSpeaker-state modelinget+1ClassifyPerson AGlobal Statêyt+1̂ytGRUPGRUPGRUGGRUGqA,t−1qA,tqB,tqB,t+1TAVutAttentionctct+1TAVut+1InputAttentionInput gtgt+1time ttime t+1Person BGRUEetGRUEet+1̂yt̂yt+1qA,t−1qB,tNote:Concatenation Dot Product Matmul Speaker-state Global-state Emotion-repgt−1g1g1gtAttention block for time tctutContext-vector:GRUGGRUGg0g1gt−2gt−1(a)",
        "GRUPqi,t−1GRUGutgt−1ctGRULqj,t−1gtqi,tqj,tSpeaker iListener jGlobal stateGRUEet−1etEmotion Representation̂ytInput Context Output∀j∈[1,M],j≠iVisualInput vj,t(b)",
        "Figure 2: (a) DialogueRNN architecture. (b) Update schemes for global, speaker, listener, and emotion states for tthutterancein a dialogue. Here, Person iis the speaker and Persons j∈[1;M]andj≠iare the listeners.",
        "Party State (Party GRU) DialogueRNN keeps track ofthe state of individual speakers using fixed size vectorsq1;q2;:::;qMthrough out the conversation. These states arerepresentative of the speakers' state in the conversation, rel-evant to emotion classification. We update these states basedon the current (at time t) role of a participant in the conver-sation, which is either speaker or listener, and the incomingutteranceut. These state vectors are initialized with null vec-tors for all the participants. The main purpose of this moduleis to ensure that the model is aware of the speaker of eachutterance and handle it accordingly.",
        "Speaker Update (Speaker GRU): Speaker usuallyframes the response based on the context, which is the pre-ceding utterances in the conversation. Hence, we capturecontextctrelevant to the utterance utas follows:",
        "softmax (x)=[ex1/slash.left\u0006iexi;ex2/slash.left\u0006iexi;:::]; (3)",
        "whereg1;g2;:::;gt−1are preceding t−1global states ( gi∈RDG),W∈RDm×DG,T∈R(t−1), andct∈RDG. InEq. (2), we calculate attention scoresover the previousglobal states representative of the previous utterances. Thisassigns higher attention scores to the utterances emotionallyrelevant tout. Finally, in Eq. (4) the context vector ctis cal-culated by pooling the previous global states with.",
        "Now, we employ a GRU cell GRU Pto update the currentspeaker state qs(ut);t−1to the new state qs(ut);tbased on in-coming utterance utand context ctusing GRU cell GRU Pof output size DPqs(ut);t=GRU P(qs(ut);t−1;(ut⊕ct)); (5)whereW{r;z;c }",
        "P;h∈RDP×DP,W{r;z;c }",
        "P;x∈RDP×(Dm+DG),b{r;z;c }",
        "P∈RDP, andqs(ut);t;qs(ut);t−1∈RDP. This encodesthe information on the current utterance along with its con-text from the global GRU into the speaker's state qs(ut),which helps in emotion classification down the line.",
        "Listener Update: Listener state models the listeners'",
        "change of state due to the speaker's utterance. We tried twolistener state update mechanisms:",
        "• Simply keep the state of the listener unchanged, that is∀i≠s(ut);qi;t=qi;t−1: (6)",
        "• Employ another GRU cell GRU Lto update the listenerstate based on listener visual cues (facial expression) vi;tand its context ct, as∀i≠s(ut);qi;t=GRU L(qi;t−1;(vi;t⊕ct));(7)",
        "wherevi;t∈RDV,W{r;z;c }",
        "L;h∈RDP×DP,W{r;z;c }",
        "L;x∈RDP×(DV+DG), andb{r;z;c }",
        "L∈RDP. Listener visualfeatures of party iat timet vi;tare extracted usingthe model introduced by Arriaga, Valdenegro-Toro, andPl¨oger (2017), pretrained on FER2013 dataset, where fea-ture sizeDV=7.",
        "The simpler first approach turns out to be sufficient, sincethe second approach yields very similar result while increas-ing number of parameters. This is due to the fact that a lis-tener becomes relevant to the conversation only when he/shespeaks. In other words, a silent party has no influence in aconversation. Now, when a party speaks, we update his/herstateqiwith context ctwhich contains relevant informationon all the preceding utterances, rendering explicit listenerstate update unnecessary. This is shown in Table 2.",
        "Emotion Representation (Emotion GRU) We infer theemotionally relevant representation etof utterance utfromthe speaker's state qs(ut);tand the emotion representation ofthe previous utterance et−1. Since context is important to theemotion of the incoming utterance ut,et−1feeds fine-tunedemotionally relevant contextual information from other theparty states qs(u<t);<tinto the emotion representation et.",
        "This establishes a connection between the speaker state andthe other party states. Hence, we model etwith a GRU cell(GRU E) with output size DEaset=GRU E(et−1;qs(ut);t); (8)",
        "whereDEis the size of emotion representation vector,e{t;t−1}∈RDE,W{r;z;c }",
        "E;h∈RDE×DE,W{r;z;c }",
        "E;x∈RDE×DP,andb{r;z;c }",
        "E∈RDE.",
        "Since speaker state gets information from global states,which serve as speaker-specific utterance representation,one may claim that this way the model already has accessto the information on other parties. However, as shown inthe ablation study (Section 5.6) emotion GRU helps to im-prove the performance by directly linking states of precedingparties. Further, we believe that speaker and global GRUs(GRU P,GRU G) jointly act similar to an encoder, whereasemotion GRU serves as a decoder.",
        "Emotion Classification We use a two-layer perceptronwith a final softmax layer to calculate c=6emotion-classprobabilities from emotion representation etof utterance utand then we pick the most likely emotion class:",
        "lt=ReLU(Wlet+bl); (9)",
        "Pt=softmax (Wsmaxlt+bsmax); (10)",
        "^yt=argmaxi(Pt[i]); (11)",
        "whereWl∈RDl×DE,bl∈RDl,Wsmax∈Rc×Dl,bsmax∈Rc,Pt∈Rc, and ^ytis the predicted label for utterance ut.",
        "Training We use categorical cross-entropy along with L2-regularization as the measure of loss ( L) during training:",
        "L=−1∑Ns=1c(s)N/summation.dispi=1c(i)",
        "/summation.dispj=1logPi;j[yi;j]+\u0015/parallel.alt1\u0012/parallel.alt12; (12)",
        "whereNis the number of samples/dialogues, c(i)is thenumber of utterances in sample i,Pi;jis the probability dis-tribution of emotion labels for utterance jof dialoguei,yi;jis the expected class label of utterance jof dialoguei,\u0015isthe L2-regularizer weight, and \u0012is the set of trainable pa-rameters where\u0012={W;W{r;z;c }",
        "P;W{r;z;c }",
        "G;W{r;z;c }",
        "b{r;z;c }",
        "E;Wl;bl;Wsmax;bsmax}:",
        "We used stochastic gradient descent basedAdam (Kingma and Ba 2014) optimizer to train ournetwork. Hyperparameters are optimized using grid search(values are added to the supplementary material).",
        "3.4 DialogueRNN VariantsWe use DialogueRNN (Section 3.3) as the basis for the fol-lowing models:DialogueRNN + Listener State Update (DialogueRNN l):",
        "This variant updates the listener state based on the the re-sulting speaker state qs(ut);t, as described in Eq. (7).",
        "Bidirectional DialogueRNN (BiDialogueRNN): Bidi-rectional DialogueRNN is analogous to bidirectional RNNs,where two different RNNs are used for forward and back-ward passes of the input sequence. Outputs from the RNNsare concatenated in sequence level. Similarly, in BiDia-logueRNN, the final emotion representation contains infor-mation from both past and future utterances in the dialoguethrough forward and backward DialogueRNNs respectively,which provides better context for emotion classification.",
        "DialogueRNN + attention (DialogueRNN+Att): Foreach emotion representation et, attention is applied overall surrounding emotion representations in the dialogue bymatching them with et(Eqs. (13) and (14)). This providescontext from the relevant (based on attention score) futureand preceding utterances.",
        "Bidirectional DialogueRNN + Emotional attention (BiDi-alogueRNN+Att): For each emotion representation etofBiDialogueRNN, attention is applied over all the emotionrepresentations in the dialogue to capture context from theother utterances in dialogue:",
        "whereet∈R2DE,W∈R2DE×2DE,~et∈R2DE, andTt∈RN. Further, ~etare fed to a two-layer perceptron for emotionclassification, as in Eqs. (9) to (11).",
        "4 Experimental Setting4.1 Datasets UsedWe use two emotion detection datasets IEMOCAP (Busso etal. 2008) and A VEC (Schuller et al. 2012) to evaluate Dia-logueRNN. We partition both datasets into train and test setswith roughly 80/slash.left20ratio such that the partitions do not shareany speaker. Table 1 shows the distribution of train and testsamples for both dataset.",
        "Dataset PartitionUtterance DialogueCount CountIEMOCAPtrain + val 5810 120test 1623 31A VECtrain + val 4368 63test 1430 32Table 1: Dataset split ((train + val) / test ≈80%/slash.left20%).",
        "IEMOCAP: IEMOCAP (Busso et al. 2008) dataset con-tains videos of two-way conversations of ten unique speak-ers, where only the first eight speakers from session oneto four belong to the train-set. Each video contains a sin-gle dyadic dialogue, segmented into utterances. The utter-ances are annotated with one of six emotion labels, whichare happy, sad, neutral, angry, excited, and frustrated.",
        "A VEC: A VEC (Schuller et al. 2012) dataset is a modifi-cation of SEMAINE database (McKeown et al. 2012) con-taining interactions between humans and artificially intel-ligent agents. Each utterance of a dialogue is annotatedwith four real valued affective attributes: valence ( [−1;1]),arousal ( [−1;1]), expectancy ( [−1;1]), and power ( [0;∞)).",
        "The annotations are available every 0.2 seconds in the orig-inal database. However, in order to adapt the annotations toour need of utterance-level annotation, we averaged the at-tributes over the span of an utterance.",
        "4.2 Baselines and State of the ArtFor a comprehensive evaluation of DialogueRNN, we com-pare our model with the following baseline methods:",
        "c-LSTM (Poria et al. 2017): Biredectional LSTM(Hochreiter and Schmidhuber 1997) is used to capture thecontext from the surrounding utterances to generate context-aware utterance representation. However, this model doesnot differentiate among the speakers.",
        "c-LSTM+Att (Poria et al. 2017): In this variant attentionis applied applied to the c-LSTM output at each timestampby following Eqs. (13) and (14). This provides better contextto the final utterance representation.",
        "TFN (Zadeh et al. 2017): This is specific to multimodalscenario. Tensor outer product is used to capture inter-modality and intra-modality interactions. This model doesnot capture context from surrounding utterances.",
        "MFN (Zadeh et al. 2018a): Specific to multimodal sce-nario, this model utilizes multi-view learning by modelingview-specific and cross-view interactions. Similar to TFN,this model does not use contextual information.",
        "CNN (Kim 2014): This is identical to our textual featureextractor network (Section 3.2) and it does not use contex-tual information from the surrounding utterances.",
        "Memnet (Sukhbaatar et al. 2015): As described in Haz-arika et al. (2018), the current utterance is fed to a memorynetwork, where the memories correspond to preceding utter-ances. The output from the memory network is used as thefinal utterance representation for emotion classification.",
        "CMN (Hazarika et al. 2018): This state-of-the-artmethod models utterance context from dialogue history us-ing two distinct GRUs for two speakers. Finally, utterancerepresentation is obtained by feeding the current utteranceas query to two distinct memory networks for both speakers.",
        "4.3 ModalitiesWe evaluated our model primarily on textual modality. How-ever, to substantiate efficacy of our model in multimodal sce-nario, we also experimented with multimodal features.",
        "5 Results and DiscussionWe compare DialogueRNN and its variants with the base-lines for textual data in Table 2. As expected, on average Di-alogueRNN outperforms all the baseline methods, includingthe state-of-the-art CMN, on both of the datasets.5.1 Comparison with the State of the ArtWe compare the performance of DialogueRNN against theperformance of the state-of-the-art CMN on IEMOCAP andA VEC datasets for textual modality.",
        "IEMOCAP As evidenced by Table 2, for IEMOCAPdataset, our model surpasses the state-of-the-art methodCMN by 2:77% accuracy and 3:76% f1-score on average.",
        "We think that this enhancement is caused by the fundamen-tal differences between CMN and DialogueRNN, which are1. party state modeling with GRU Pin Eq. (5),2. speaker specific utterance treatment in Eqs. (1) and (5),3. and global state capturing with GRU Gin Eq. (1).",
        "Since we deal with six unbalanced emotion labels, we alsoexplored the model performance for individual labels. Dia-logueRNN outperforms the state-of-the-art method CMN infive out of six emotion classes by significant margin. Forfrustrated class, DialogueRNN lags behind CMN by 1:23%f1-score. We think that DialogueRNN may surpass CMN us-ing a standalone classifier for frustrated class. However, itcan be observed in Table 2 that some of the other variantsof DialogueRNN, like BiDialogueRNN has already outper-formed CMN for frustrated class.",
        "A VEC DialogueRNN outperforms CMN for valence,arousal, expectancy, and power attributes; see Table 2. Ityields significantly lower mean absolute error ( MAE ) andhigher Pearson correlation coefficient ( r) for all four at-tributes. We believe this to be due to the incorporation ofparty state and emotion GRU, which are missing from CMN.",
        "5.2 DialogueRNN vs. DialogueRNN VariantsWe discuss the performance of different DialogueRNN vari-ants on IEMOCAP and A VEC datasets for textual modality.",
        "DialogueRNN l:Following Table 2, using explicit listenerstate update yields slightly worse performance than reg-ular DialogueRNN. This is true for both IEMOCAP andA VEC datasets in general. However, the only exception tothis trend is for happy emotion label for IEMOCAP, whereDialogueRNN loutperforms DialogueRNN by 1:71% f1-score. We surmise that, this is due to the fact that a lis-tener becomes relevant to the conversation only when he/shespeaks. Now, in DialogueRNN, when a party speaks, we up-date his/her state qiwith context ctwhich contains relevantinformation on all the preceding utterances, rendering ex-plicit listener state update of DialogueRNN lunnecessary.",
        "BiDialogueRNN: Since BiDialogueRNN captures con-text from the future utterances, we expect improved per-formance from it over DialogueRNN. This is confirmed inTable 2, where BiDialogueRNN outperforms DialogueRNNon average on both datasets.",
        "DialogueRNN+Attn: DialogueRNN+Attn also uses in-formation from the future utterances. However, here we takeinformation from both past and future utterances by match-ing them with the current utterance and calculating attentionscore over them. This provides relevance to emotionally im-portant context utterances, yielding better performance than",
        "MethodsIEMOCAP A VECF1Valence(r)Arousal(r)Expectancy(r)Power(r)",
        "TFN 56.8 0.01 0.10 0.12 0.12MFN 53.5 0.14 25 0.26 0.15c-LSTM 58.3 0.14 0.23 0.25 -0.04CMN 58.5 0.23 0.30 0.26 -0.02BiDialogueRNN+att text 62.7 0.35 0.59 0.37 0.37BiDialogueRNN+att MM 62.9 0.37 0.60 0.37 0.41Table 3: Comparison with the baselines for trimodal(T+V+A) scenario. BiDialogueRNN+att MM = BiDia-logueRNN+att in multimodal setting.",
        "BiDialogueRNN. The improvement over BiDialogueRNNis1:23% f1-score for IEMOCAP and consistently lowerMAE and higherrin A VEC.",
        "BiDialogueRNN+Attn: Since this setting generates thefinal emotion representation by attending over the emo-tion representation from BiDialogueRNN, we expect bet-ter performance than both BiDialogueRNN and Dia-logueRNN+Attn. This is confirmed in Table 2, where thissetting performs the best in general than any other meth-ods discussed, on both datasets. This setting yields 6:62%higher f1-score on average than the state-of-the-art CMNand2:86% higher f1-score than vanilla DialogueRNN forIEMOCAP dataset. For A VEC dataset also, this setting givesthe best performance across all the four attributes.",
        "5.3 Multimodal SettingAs both IEMOCAP and A VEC dataset contain multi-modal information, we have evaluated DialogueRNN onmultimodal features as used and provided by Hazarika etal. (2018). We use concatenation of the unimodal features asa fusion method by following Hazarika et al. (2018), sincefusion mechanism is not a focus of this paper. Now, as wecan see in Table 3, DialogueRNN significantly outperformsthe strong baselines and state-of-the-art method CMN.",
        "5.4 Case StudiesDependency on preceding utterances (DialogueRNN)",
        "One of the crucial components of DialogueRNN is its atten-tion module over the outputs of global GRU ( GRU G). Fig-ure 3b shows theattention vector (Eq. (2)) over the historyof a given test utterance compared with the attention vectorfrom the CMN model. The attention of our model is more fo-cused compared with CMN: the latter gives diluted attentionscores leading to misclassifications. We observe this trendof focused-attention across cases and posit that it can be in-terpreted as a confidence indicator. Further in this example,the test utterance by PA(turn 44) comprises of a changein emotion from neutral tofrustrated . DialogueRNN antic-ipates this correctly by attending to turn 41 and 42 that arespoken byPAandPB, respectively. These two utterancesprovide self and inter-party influences that trigger the emo-tional shift. CMN, however, fails to capture such dependen-cies and wrongly predicts neutral emotion.",
        "Dependency on future utterances (BiDialogueRNN+Att)",
        "Fig. 3a visualizes the(Eq. (13)) attention over the emotionrepresentations etfor a segment of a conversation betweena couple. In the discussion, the woman ( PA) is initially ata neutral state, whereas the man ( PB) is angry throughout.",
        "The figure reveals that the emotional attention of the womanis localized to the duration of her neutral state (turns 1-16 ap-proximately). For example, in the dialogue, turns 5;7, and 8strongly attend to turn 8. Interestingly, turn 5attends to bothpast (turn 3) and future (turn 8) utterances. Similar trendacross other utterances establish inter-dependence betweenemotional states of future and past utterances. The beneficialconsideration of future utterances through GRU Eis also ap-parent through turns 6;9. These utterances focus on the dis-tant future (turn 49;50) where the man is at an enraged state,thus capturing emotional correlations across time. Although,turn6is misclassified by our model, it still manages to in-fer a related emotional state ( anger ) against the correct state(frustrated ). We analyze more of this trend in section 5.5.",
        "Dependency on distant context For all correct predic-tions in the IEMOCAP test set in Fig. 3d we summarizethe distribution over the relative distance between test ut-terance and ( 2nd) highest attended utterance – either in thehistory or future – in the conversation. This reveals a de-creasing trend with the highest dependence being within thelocal context. However, a significant portion of the test ut-terances (∼18%), attend to utterances that are 20to40turnsaway from themselves, which highlights the important roleof long-term emotional dependencies. Such cases primarilyoccur in conversations that maintain a specific affective toneand do not incur frequent emotional shifts. Fig. 3c demon-strates a case of long-term context dependency. The pre-sented conversation maintains a happy mood throughout thedialogue. Although the 34thturn comprising the sentenceHorrible thing. I hated it. seems to be a negative expres-",
        "- 0.30 - 0.15 - 0.00TurnUtterance PA                               PBEmotion LabelPredictionTurns :       1      2     3      4      5      6      7      8      9    ….     49    50    51   …5Oh, maybe a little. Nothing serious.neutralneutral6He let him kiss you.  You said you didfrustrationanger7Well, what of it.neutralneutral8It gave him a lot of pleasure  and it didn't hurt me.neutralneutral9That's a nice point  of view I must sayangerangerpastfuture(a)",
        "0.80.60.40.20.040414243TurnUtteranceEmotion41PA: Because I paid it on time.fruTurnUtteranceEmotionDialogueRNNCMN44PA:   But if I call back, I have to go through this whole rigmarole again.frufruneuTest Utterance:CMNDialogueRNNTurns393837363534333231SpeakerPAPBPAPBPAPBPBPBPBPBPAPAPATurnUtteranceEmotion42PB: Yeah, since it was a computer glitch … you should just call backneuEmotionneuneufruneuneuneuneuneuneuneuneuneuneu(b)",
        "- 0.20 - 0.10 - 0.00TurnUtteranceEmotionTrue Predicted34Horrible thing. I hated it.excitedexcitedTurnUtteranceEmotion11Uhh, It's amazing howonce you feel ..excited024681012141618202224262830323436384042Turns:",
        "TurnUtteranceEmotion14Being that sacred andwonderful thing, love.happyTest UtteranceΔt=23(c)",
        "80Distance between test  utterance andHighest attention2nd highest attention0204060-20-40-60Δt020406080100120140Frequency of correct predictions(d)",
        "Figure 3: (a) Illustration of theattention over emotion representations et; (b) Comparison of attention scores over utterancehistory of CMN and DialogueRNN (attention). (c) An example of long-term dependency among utterances. (d) Histogram of\u0001t=distance between the target utterance and its context utterance based onattention scores.",
        "sion, when seen with the global context, it reveals the ex-citement present in the speaker. To disambiguate such cases,our model attends to distant utterances in the past (turn 11,14) which serve as prototypes of the emotional tonality ofthe overall conversation.",
        "5.5 Error AnalysisA noticeable trend in the predictions is the high level ofcross-predictions amongst related emotions. Most of themisclassifications by the model for happy emotion are forexcited class. Also, anger andfrustrated share misclassifica-tions amongst each other. We suspect this is due to subtle dif-ference between those emotion pairs, resulting in harder dis-ambiguation. Another class with high rate of false-positivesis the neutral class. Primary reason for this could be its ma-jority in the class distribution over the considered emotions.",
        "At the dialogue level, we observe that a significant amountof errors occur at turns having a change of emotion fromthe previous turn of the same party. Across all the occur-rences of these emotional-shifts in the testing set, our modelcorrectly predicts 47:5%instances. This stands less as com-pared to the 69:2%success that it achieves at regions of noemotional-shift . Changes in emotions in a dialogue is a com-plex phenomenon governed by latent dynamics. Further im-provement of these cases remain as an open area of research.",
        "5.6 Ablation StudyThe main novelty of our method is the introduction of partystate and emotion GRU ( GRU E). To comprehensively studythe impact of these two components, we remove them one ata time and evaluate their impact on IEMOCAP.Party State Emotion GRU F1- + 55.56+ - 57.38+ + 59.89Table 4: Ablated DialogueRNN for IEMOCAP dataset.",
        "As expected, following Table 4, party state stands veryimportant, as without its presence the performance falls by4:33%. We suspect that party state helps in extracting usefulcontextual information relevant to parties' emotion.",
        "Emotion GRU is also impactful, but less than party state,as its absence causes performance to fall by only 2:51%. Webelieve the reason to be the lack of context flow from theother parties' states through the emotion representation ofthe preceding utterances."
    ]
}
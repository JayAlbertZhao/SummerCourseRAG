{
    "Abstract": " Emotion is intrinsic to humans and consequently, emotion understanding is a key part ofhuman-like articial intelligence (AI). Emotion recognition in conversation (ERC) is becoming increasinglypopular as a new research frontier in natural language processing (NLP) due to its ability to mine opinionsfrom the plethora of publicly available conversational data on platforms such as Facebook, Youtube, Reddit,Twitter, and others. Moreover, it has potential applications in health-care systems (as a tool for psychologicalanalysis), education (understanding student frustration), and more. In Addition, ERC is also extremelyimportant for generating emotion-aware dialogues that require an understanding of the user's emotions.Catering to these needs calls for effective and scalable conversational emotion-recognition algorithms.However, it is a difcult problem to solve because of several research challenges. In this paper, we discussthese challenges and shed light on recent research in thiseld. We also describe the drawbacks of theseapproaches and discuss the reasons why they fail to successfully overcome the research challenges in ERC.INDEX TERMS Emotion recognition, sentiment analysis, dialogue systems, natural language processing.I",
    "Keywords": " Emotion recognition, sentiment analysis, dialogue systems, natural language processing.I",
    "Body": [
        "INTRODUCTIONEmotion is often dened as an individual's mental stateassociated with thoughts, feelings, and behavior. Stoics likeCicero organized emotions into four categories - metus (fear),aegritudo (pain), libido (lust), and laetitia (pleasure). Later,evolutionary theories of emotion were initiated in the late19th century by Darwin and Prodger [1]. He hypothesizedthat emotions evolved through natural selection and, hence,have cross-culturally universal counterparts. In recent times,Plutchik [2] categorized emotion into eight primary types,visualized by the wheel of emotions. Further, Ekman [3]",
        "argued for the correlation between emotion and facialexpression.",
        "Natural language is often indicative of one's emotion.",
        "Hence, emotion recognition has been enjoying popularity intheeld of NLP [4], [5], due to its widespread applicationsin opinion mining, recommender systems, health-care, andThe associate editor coordinating the review of this manuscript andapproving it for publication was Tao Zhou.so on. Strapparava and Mihalcea [6] addressed the task ofemotion detection in news headlines. A number of emotionlexicons [7], [8] have been developed to tackle the textualemotion recognition problem.",
        "Only in the past few years has emotion recognition inconversation (ERC) gained attention from the NLP commu-nity [9]\u0015[12] due to the growing availability of public con-versational data. ERC can be used to analyze conversationsthat take place on social media. It can also aid in analyzingconversations in real times, which can be instrumental in legaltrials, interviews, e-health services, and more.",
        "Unlike vanilla emotion recognition of sentences/utterances, ERC ideally requires context modeling of theindividual utterances. This context can be attributed to thepreceding utterances, and relies on the temporal sequenceof utterances. Compared to the recently published works onERC [10]\u0015[12], both lexicon-based [8], [13], [14] and moderndeep learning-based [4], [5] vanilla emotion recognitionapproaches fail to work well on ERC datasets as these worksignore the conversation specic factors such as the presenceVOLUME 7, 2019This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/100943",
        "S. Poria et al.: ERC: Research Challenges, Datasets, and Recent Advancesof contextual cues, the temporality in speakers' turns,or speaker-specic information. Fig. 4a and Fig. 4b showan example where the same utterance changes its meaningdepending on its preceding utterance.",
        "A. TASK DEFINITIONGiven the transcript of a conversation along with speakerinformation of each constituent utterance, the ERC task aimsto identify the emotion of each utterance from a set of pre-dened emotions. Fig. 2 illustrates one such conversationbetween two people, where each utterance is labeled bythe underlying emotion. Formally, given the input sequenceofNnumber of utterances [(u 1;p1);(u2;p2); : : : ; (uN;pN)],where each utterance uiD[ui;1;ui;2; : : : ; ui;T] consists of Twords ui;jand spoken by party pi, the task is to predict theemotion label eiof each utterance ui.",
        "B. CONTROLLING VARIABLES IN CONVERSATIONSConversations are broadly categorized into two categories:",
        "task oriented and chit-chat (also referred to as non-task ori-ented). Both types of conversations are governed by differentfactors or pragmatics [15], such as topic, interlocutors' per-sonality, argumentation logic, viewpoint, intent [16], and soon. Fig.1 shows how these factors play out in a dyadic con-versation. Firstly, topic (Topic) and interlocutor personality(P\u0003) always inuence the conversation, irrespective of thetime. A speaker makes up his/her mind (St\u0003) about the reply(Ut\u0003) based on the contextual preceding utterances (U<t\u0003) fromboth speaker and listener, the previous utterance being themost important one since it usually makes the largest changein the joint task model (for task-oriented conversations) orthe speaker's emotional state (for chit-chat). Delving deeper,the pragmatic features, as explained by Hovy [15], like argu-mentation logic, interlocutor viewpoint, inter-personal rela-tionship and dependency, situational awareness are encodedin speaker state (St\u0003). Intent (It\u0003) of the speaker is decidedbased on previous intent It\u00002\u0003and speaker state St\u0003, as theinterlocutor may change his/her intent based on the oppo-nent's utterance and current situation. Then, the speaker for-mulates appropriate emotion Et\u0003for the response based on thestate St\u0003and intent It\u0003. Finally, the response Ut\u0003is producedbased on the speaker state St\u0003, intent It\u0003, and emotion Et\u0003.",
        "We surmise that considering these factors would help repre-senting the argument and discourse structure of the conver-sation, which leads to improved conversation understanding,including emotion recognition.",
        "Early computational work on dialogue mostly focusedon task-oriented cases, in which the overall conversa-tional intent and step-by-step sub-goals played a largepart [17], [18]. Cohen and Levesque [19] developed amodel and logic to represent intentions and their connec-tions to utterances, whose operators explicate the treatmentof beliefs about the interlocutor's beliefs and vice versa,recursively. Emotion however played no role in this line ofresearch. In more recent work, chatbots and chit-chat dia-logue have become more prominent, in part due to the use ofFIGURE 1. Interaction among different controlling variables during adyadic conversation between persons A and B. Grey and white circlesrepresent hidden and observed variables, respectively. Prepresentspersonality, Urepresents utterance, Srepresents interlocutor state, Irepresents interlocutor intent, Erepresents emotion and Topic representstopic of the conversation. This can easily be extended to multi-partyconversations.",
        "distributed (such as embedding) representations that do notreadily support logical inference.",
        "On conversational setting, D'Mello et al. [20] and [21]",
        "worked with small datasets with three and four emotionlabels, respectively. This was followed by Phan et al. [22],where emotion detection on conversation transcript wasattempted. Recently, several works [23], [24] have deviseddeep learning-based techniques for ERC. These works arecrucial as we surmise an instrumental role of ERC in emotion-aware a.k.a. affective dialogue generation which has fallenwithin the topic of ``text generation under pragmatics cons-triants'' as proposed by Hovy [15]. Fig. 3illustrates one suchconversation between a human (user) and a medical chatbot(health-assistant). The assistant responds with emotion basedon the user's input. Depending on whether the user sufferedan injury earlier or not, the health-assistant responds withexcitement (evoking urgency) or happiness (evoking relief).",
        "As ERC is a new researcheld, outlining research chal-lenges, available datasets, and benchmarks can potentiallyaid future research on ERC. In this paper, we aim to servethis purpose by discussing various factors that contribute tothe emotion dynamics in a conversation. We surmise thatthis paper will not only help the researchers to better under-stand the challenges and recent works on ERC but also showpossible future research directions. The rest of the paper isorganized as follows: Section IIpresents the key researchchallenges; Section IIIand Section IVcover the datasetsand recent progress in thiseld;",
        "nally Section Vconcludesthe paper.",
        "S. Poria et al.: ERC: Research Challenges, Datasets, and Recent AdvancesFIGURE 2. An abridged dialogue from the IEMOCAP dataset.",
        "FIGURE 3. Illustration of an affective conversation where the emotiondepends on the context.",
        "II. RESEARCH CHALLENGESRecent works on ERC, e.g., DialogueRNN [11] or ICON[23], strive to address several key research challenges thatmake the task of ERC difcult to solve:",
        "A. CATEGORIZATION OF EMOTIONSEmotion is dened using two type of models \u0016 categoricaland dimensional. Categorical model classies emotion into axed number of discrete categories. In contrast, dimensionalmodel describes emotion as a point in a continuous multi-dimensional space.",
        "In the categorical front, Plutchik [2]'s wheel of emotionsdenes eight discrete primary emotion types, each of whichhasner related subtypes. On the other hand, Ekman [3] con-cludes six basic emotions \u0016 anger, disgust, fear, happiness,sadness, and surprise.Most dimensional categorization models [25], [26] adopttwo dimensions \u0016 valence and arousal. Valence representsthe degree of emotional positivity and arousal represents theintensity of the emotion. In contrast with the categoricalmodels, dimensional models map emotion into a continuousspectrum rather than hard categories. This enables easy andintuitive comparison of two emotional states using vectoroperations, whereas comparison is non-trivial for categoricalmodels. As there are multiple categorization and dimensionaltaxonomies available, it is challenging to select one particu-lar model for annotation. Choosing a simple categorizationmodel e.g., Ekman's model has a major drawback as thesemodels are unable to ground complex emotions. On the otherhand, complex emotion models such as Plutchik's modelmake it very difcult for the annotators to discriminatebetween the related emotions, e.g., discerning anger fromrage. Complex emotion models also increase the risk ofobtaining a lower inter-annotator agreement.",
        "The popular ERC dataset IEMOCAP [27] adopted bothcategorical and dimensional models. However, newer ERCdatasets like DailyDialogue [28] have employed only cat-egorical model due to its more intuitive nature. Most ofthe available datasets for emotion recognition in conversa-tion adopted simple taxonomies, which are slight variants ofEkman's model. Each emotional utterance in the EmoCon-text dataset is labeled with one of the following emotions:",
        "happiness, sadness, and anger. The majority of the utterancesin EmoContext do not elicit any of these three emotions andare annotated with an extra label: others. Naturally, the inter-annotator agreement for the EmoContext dataset is higherdue to its simplistic emotion taxonomy. However, the shortcontext length and simple emotion taxonomy make ERC onthis dataset less challenging.",
        "B. BASIS OF EMOTION ANNOTATIONAnnotation with emotion labels is challenging as the labeldepends on the annotators perspective. Self-assessment bythe interlocutors in a conversation is arguably the best way toannotate utterances. However, in practice it is unfeasible asreal-time tagging of unscripted conversations will impact theconversationow. Post-conversation self-annotation couldbe an option, but it has not been done yet. As such,many ERC datasets [27] are scripted and annotated by agroup of people uninvolved with the script and conversation.",
        "The annotators are given the context of the utterances asprior knowledge for accurate annotation. Often pre-existingtranscripts are annotated for quick turn-around, as inEmotionLines [10].",
        "The annotators also need to be aware of the interlocu-tors perspective for situation-aware annotation. For example,the emotion behind the utterance ``Lehman Brothers' stockis plummeting!!'' depends on whether the speaker benetsfrom the crash. The annotators should be aware of the natureof association between the speaker and Lehman Brothers foraccurate labeling.",
        "S. Poria et al.: ERC: Research Challenges, Datasets, and Recent AdvancesFIGURE 4. Role of context in emotion recognition in conversation.",
        "C. CONVERSATIONAL CONTEXT MODELINGContext is at the core of the NLP research. According toseveral recent studies [29], [30], contextual sentence andword embeddings can improve the performance of the state-of-the-art NLP systems by a signicant margin.",
        "The notion of context can vary from problem to prob-lem. For example, while calculating word representations,the surrounding words carry contextual information. Like-wise, to classify a sentence in a document, other neighboringsentences are considered as its context. In Poria et al. [31],surrounding utterances are treated as context and they exper-imentally show that contextual evidence indeed aids inclassication.",
        "Similarly in conversational emotion-detection, to deter-mine the emotion of an utterance at time t, the preceding utter-ances at time <tcan be considered as its context. However,computing this context representation often exhibits majordifculties due to emotional dynamics.",
        "Emotional dynamics of conversations consists of twoimportant aspects: selfandinter-personal dependencies [32].",
        "Self-dependency, also known as emotional inertia, deals withthe aspect of emotional inuence that speakers have on them-selves during conversations [33]. On the other hand, inter-personal dependencies relate to the emotional inuences thatthe counterparts induce into a speaker. Conversely, during thecourse of a dialogue, speakers also tend to mirror their coun-terparts to build rapport [34]. This phenomenon is illustratedin Fig. 2. Here, Pais frustrated over her long term unem-ployment and seeks encouragement (u 1;u3).Pb, however,is pre-occupied and replies sarcastically (u 4). This enragesPato appropriate an angry response (u 6). In this dialogue,emotional inertia is evident in Pbwho does not deviatefrom his nonchalant behavior. Pa, however, gets emotionallyinuenced by Pb. Modeling self and inter-personal relation-ship and dependencies may also depend on the topic of theconversation as well as various other factors like argumentstructure, interlocutors' personality, intents, viewpoints onthe conversation, attitude towards each other etc.. Hence,analyzing all these factors are key for a true self and inter-personal dependency modeling that can lead to enriched con-text understanding.",
        "The contextual information can come from both local anddistant conversational history. While the importance of localcontext is more obvious, as stated in recent works, distantcontext often plays a less important role in ERC. Distantcontextual information is useful mostly in the scenarios whena speaker refers to earlier utterances spoken by any of thespeakers in the conversational history.",
        "The usefulness of context is more prevalent in classifyingshort utterances, like ``yeah'', ``okay'', ``no'', that can expressdifferent emotions depending on the context and discourse ofthe dialogue. The examples in Fig. 4a and Fig. 4b explain thisphenomenon. The emotions expressed by the same utterance``Yeah'' in both these examples differ from each other andcan only be inferred from the context.",
        "Finding contextualized conversational utterance represen-tations is an active area of research. Leveraging such con-textual clues is a difcult task. Memory networks, RNNs,and attention mechanisms have been used in previous works,e.g., HRLCE or DialogueRNN, to grasp information from thecontext.",
        "D. SPEAKER SPECIFIC MODELINGIndividuals have their own subtle way of expressing emo-tions. For instance, some individuals are more sarcastic thanothers. For such cases, the usage of certain words would varydepending on if they are being sarcastic. Let's consider thisexample, PaV``The order has been cancelled. '', PbV``This isgreat!''. If Pbis a sarcastic person, then his response wouldexpress negative emotion to the order being canceled throughthe word great. On the other hand, Pb's response, great,could be taken literally if the canceled order is benecialtoPb(perhaps Pbcannot afford the product he ordered).",
        "Since, necessary background information is often missingfrom the conversations, speaker proling based on precedingutterances often yields improved results.",
        "E. LISTENER SPECIFIC MODELINGDuring a conversation, the listeners make up their mind aboutthe speaker's utterance as it's spoken. However, there is notextual data on the listener's reaction to the speaker while thespeaker speaks. A model must resort to visual modality tomodel the listener's facial expression to capture the listener'sreaction. However, according to DialogueRNN, capturinglistener reaction does not yield any improvement as the lis-tener's subsequent utterance carries their reaction. Moreover,of the listener never speaks in a conversation, his/her reactionremains irrelevant. Nonetheless, listener modeling can be100946 VOLUME 7, 2019",
        "S. Poria et al.: ERC: Research Challenges, Datasets, and Recent AdvancesFIGURE 5. Emotion shift of speakers in a dialogue in comparison with speaker's previous emotion. Red and blue colors are used to show the emotionshift of Joey and Chandler respectively.",
        "useful in the scenarios where continuous emotion recognitionof every moment of the conversation is necessary, like audi-ence reaction during a political speech, as opposed to emotionrecognition of each utterance.",
        "F. PRESENCE OF EMOTION SHIFTDue to emotional inertia, participants in a conversation tendto stick a particular emotional state, unless some externalstimuli, usually the other participants, invoke a change. Thisis illustrated in Fig. 5, where Joey changes his emotionfrom neutral toanger due to the last utterance of Chandler,which was unexpected and rather shocking to Joey. This isa hard problem to solve, as the state-of-the-art ERC model,DialogueRNN is more accurate in emotion detection for theutterances without emotional shift or when the shift is to asimilar emotion (e.g., from fear to sad).",
        "The state-of-the-art methods keeps mimicking the sameemotion for a particular party, since an abrupt change ofemotion is unlikely. Hence, these methods fail in most caseswhere a change occurs. To tackle this, a new problem ofdetecting emotion shift can be framed:",
        "1) based on the historical utterances and the present utter-ance, is there an emotion shift (binary classication)?",
        "2) if there is a shift then what is the target emotion (multi-label classication)?",
        "As baseline, the performance of conditional randomeld (CRF) would be interesting as it models label depen-dencies.",
        "G. FINE-GRAINED EMOTION RECOGNITIONFine-grained emotion recognition aims at recognizing emo-tion expressed on explicit and implicit topics. It involvesa deeper understanding of the topic of the conversation,FIGURE 6. Fine-grained emotion understanding: An example.",
        "interlocutor opinion, and stand. For example, in Fig. 6, whileboth persons take a supportive stand for the government's bill,they use completely opposite emotions to express it. It is notpossible for a vanilla emotion recognizer to understand thepositive emotion of both the interlocutors on the aspect ofgovernment's bill. Only by interpreting Person 2's frustrationabout the opposition's protest against the bill can a classierinfer Person 2's support for the bill. On the other hand, eventhough Person 1 does not explicitly express his/her opinionon the opposition, from the discourse of the conversation,it can be inferred that Person 1 holds a negative opinion onthe opposition.",
        "H. MULTIPARTY CONVERSATIONIn a multiparty conversation, more than two participants areinvolved. Naturally, emotion recognition in such conversa-tions is more challenging in comparison with dyadic con-versations due to the difculty in tracking individual speakerstates and handling co-references.",
        "I. PRESENCE OF SARCASMSarcasm is a linguistic tool that uses irony to express con-tempt. An ERC system incapable of detecting sarcasm mostlyfails to predict emotion of the sarcastic utterances correctly.",
        "S. Poria et al.: ERC: Research Challenges, Datasets, and Recent AdvancesTABLE 1. Label distribution statistics in different emotion recognition datasets.",
        "Sarcasm detection in a conversation largely depends on thecontext and discourse of the conversation. For example,the utterance ``The part where Obama signed it'' can onlybe detected as sarcastic if we look at the previous utterance``What part of this would be unconstitutional?''. Sarcas-tic nature is also person dependent, which again warrantsspeaker proling in the conversation.",
        "J. EMOTION REASONINGThe ability to reason is necessary for any explainable AIsystem. In the context of ERC, it is often desired to understandthe cause of an expressed emotion by a speaker. As an exam-ple, we can refer to 2. An ideal ERC system, with the ability ofemotion reasoning, should perceive the reason for Person A'sanger, expressed in u6of Fig. 2. It is evident upon observa-tion that this anger is caused by the persistent nonchalantbehavior of Person B. Readers should not conate emotionreasoning with context modeling, which we discuss earlierin this section. Unlike context modeling, emotion reasoningdoes not onlynd the contextual utterances in conversationalhistory that triggers the emotion of an utterance, but alsodetermines the function of those contextual utterances on thetarget utterance. In Fig. 2, it is the indifference of Person B,reected by u4andu5, that makes Person Aangry. Similarly,in Fig. 5, Joey expresses anger once he ascertains Chandler'sdeception in the previous utterance. It is hard to dene ataxonomy or tagset for emotion reasoning. At present, thereis no available dataset which contains such rich annotations.",
        "Building such dataset would enable future dialogue systemsto frame meaningful argumentation logic and discourse struc-ture, taking one step closer to human-like conversation.",
        "III. DATASETSIn the last few years, emotion recognition in conversation hasgained major research interest, mainly because of its potentialapplication in dialogue systems to generate emotion-awareand empathetic dialogues [12]. The primary goal of ERC taskis to label each utterance in the conversation with an emo-tion label. In this section, we discuss the publicly availableERC datasets as well as the shortcomings of these datasets.There are a few publicly available datasets for ERC - IEMO-CAP [27], SEMAINE [35], Emotionlines [10], MELD [36],DailyDialog [28], and EmoContext [37]. A detailed com-parison of these datasets is drawn in Table 2. Out of theseve datasets, IEMOCAP, SEMAINE, and MELD are multi-modal (containing acoustic, visual and textual information)",
        "and the remaining two are textual. Apart from SEMAINEdataset, rest of the datasets contains categorical emotionlabels. In contrast, each utterance of SEMAINE dataset isannotated with four real valued affective attributes: valence([\u00001; 1]), arousal ([\u00001; 1]), expectancy ([\u00001; 1]), and power([0;1)). We also show the emotion label distribution of thesedatasets in Table 1. In EmoContext dataset, an emotion labelis assigned to only the last utterance of each dialogue. Noneof these datasets can be used for emotion reasoning as theylack necessary annotation details required for the reasoningtask. Readers should also note that, all these datasets do notcontainne-grained and topic level emotion annotation.",
        "IV. RECENT ADVANCESIn this section we give a brief introduction to the recent workon this topic. We also compare the approaches and reporttheir drawbacks. As depicted in Fig. 1, recognizing emotionof an utterance in a conversation primarily depends on thesefollowing three factors:",
        "1) the utterance itself and its context dened by theinterlocutors' preceding utterances in the conversation,as well as intent and the topic of the conversation,2) the speaker's state comprising variables like personal-ity and argumentation logic and,3) emotions expressed in the preceding utterances.",
        "Although, IEMOCAP and SEMAINE have been developedalmost a decade ago, most of the works that used these twodatasets did not consider the aforementioned factors.",
        "A. BENCHMARKS AND THEIR DRAWBACKSBased on these factors, a number of approaches to address theERC problem have been proposed recently. Conversationalmemory network (CMN), proposed by Hazarika et al. [38]",
        "for dyadic dialogues, is one of therst ERC approaches that100948 VOLUME 7, 2019",
        "S. Poria et al.: ERC: Research Challenges, Datasets, and Recent AdvancesTABLE 2. Comparison among IEMOCAP, SEMAINE, emotionLines, MELD, and dailydialog datasets.",
        "FIGURE 7. Comparison of attention scores over utterance history of CMN and DialogueRNN. Higher attention value signifies moreimportant contextual information. Note: Figure taken from Majumder et al. [11].",
        "utilizes distinct memories for each speaker for speaker-specic context modeling. Later, Hazarika et al. [23]",
        "improved upon this approach with interactive conversationalmemory network (ICON), which interconnects these mem-ories to model self and inter-speaker emotional inuence.",
        "None of these two methods actually exploit the speaker infor-mation of the target utterance for classication. This makesthe model blind to speaker-specic nuances.",
        "DialogueRNN [11] aims to solve this issue by consid-ering the speaker information of the target utterance and,further, modeling self and inter-speaker emotional inuencewith a hierarchical multi-stage RNN with attention mech-anism. On both IEMOCAP and SEMAINE datasets, Dia-logueRNN outperformed (Table 3 and Table 4) the other twoapproaches.",
        "The need to grasp inter-speaker dependency for ERCis also acknowledged and modeled in [39] by leverag-ing quantum theory and LSTM. Their network, Quantum-Inspired Interactive Networks (QIN) has outperformedCMN and ICON on IEMOCAP and MELD datasets.",
        "Recently, Yeh et al. [9] proposed an ERC method calledInteraction-aware Attention Network (IANN) by leveraginginter-speaker relation modeling. Similar to ICON and CMN,IANN (Fig. 8) utilizes distinct memories for each speaker.",
        "All of these models afrm that contextual history, mod-eling self and inter-speaker inuence are benecial to ERC(shown in Fig. 7and Fig. 10). Further, DialogueRNN showsthat the nearby utterances are generally more context richand ERC performance improves when the future utterances,at time >t, are available. This is indicated by Fig. 10, whereDialogueRNN uses both past and future utterances as contextwith roughly the same frequency. Also, the distant utterancesare used less frequently than the nearby utterances. On theother hand, CMN and ICON do not use future utterances ascontext at all. However, for real-time applications, systemscannot rely on future utterances. In such cases, CMN, ICON,and DialogueRNN withxed context window would bebetting.",
        "All these networks, namely CMN, ICON, IANN, and Dia-logueRNN, perform poorly on the utterances with emotionshift. In particular, the cases where the emotion of the targetutterance differs from the previous utterance, DialogueRNNVOLUME 7, 2019 100949",
        "S. Poria et al.: ERC: Research Challenges, Datasets, and Recent AdvancesTABLE 3. Comparison between DialogueRNN and baseline methods on IEMOCAP dataset; bold font denotes the best performances.",
        "Average(w) = Weighted average. ICON results differ from the original paper [23] as in our experiment, we disregard their contextualfeature extraction and pre-processing part. More details can be found in Majumder et al. [11].",
        "TABLE 4. Comparison between DialogueRNN and baseline methods on SEMAINE dataset; Acc. DAccuracy, MAEDMean Absolute Error, rDPearsoncorrelation coefficient; bold font denotes the best performances. More details can be found in Majumder et al. [11].",
        "FIGURE 8. Interactive-aware Attention Network (IANN), proposed by Yeh et al. [9].",
        "could only correctly predict 47:5% instances. This stands lessas compared to the 69:2% success-rate that it achieves at theregions of no emotional-shift.",
        "Among these three approaches, only DialogueRNN iscapable of handling multiparty conversations on large scale.",
        "However, on the multiparty conversational dataset MELD,only a little performance improvement (shown in Table 5)is observed by DialogueRNN compared to bc-LSTM whichdepicts a future research direction on multiparty ERC. ICONand CMN are designed to detect emotions in dyadic dia-logues. Adapting ICON and CMN to apply on multipartyconversational dataset MELD can cause scalability issue insituations when number speakers participating in a conversa-tion in the test data is more than the training data.",
        "S. Poria et al.: ERC: Research Challenges, Datasets, and Recent AdvancesTABLE 5. Test-set F-score results of bc-LSTM and DialogueRNN for emotion classification in MELD. Note: w-avg denotes weighted-average. text-CNN:",
        "CNN applied on text, contextual information were not used.",
        "FIGURE 9. The EmoContext dataset [37] and the HRLCE [40] framework.",
        "TABLE 6. Recent works on the EmoContext dataset.",
        "Due to the sequential nature of the utterances inconversations, RNNs are used for context generation inthe aforementioned models. However, there is ample roomfor improvement, as the RNN-based context representationmethods perform poorly in grasping long distant contextualinformation.",
        "Recently, two shared tasks \u0016 EmotionX1(co-locatedwith SocialNLP workshop) and EmoContext2(co-locatedwith Semeval 2019) have been organized to address theERC problem. EmoContext shared task has garnered morethan 500 participants, afrming the growing popularity ofthis researcheld. Compared to other datasets, EmoContextdataset [37] has very short conversations consisting only threeutterances where the goal is to label the 3rd utterance asshown in Fig. 9a.",
        "Emotion labels of the previous utterances are not present inthe EmoContext dataset. The key works [24], [37], [40]\u0015[44]",
        "on this dataset have mainly leveraged on context modeling1https://sites.google.com/view/emotionx2019/2https://www.humanizing-ai.com/emocontext.htmlFIGURE 10. Histogram of 1tDdistance between the target utteranceand its context utterance based on DialogueRNN's attention scores.",
        "Note: Figure taken from Majumder et al. [11].",
        "using bc-LSTM architecture [31] that encapsulates the tem-poral order of the utterances using an LSTM. A commontrend can be noticed in these works, where traditional wordembeddings, such as Glove [45], are combined with contex-tualized word embeddings, such as ELMo [29] to improve theperformance. Most of these works use attention mechanismon top of the bc-LSTM to enrich context representation.",
        "In Fig. 9b, we depict the HRLCE framework, proposed byHuang et al. [40], that comprises of an utterance encoder anda context encoder that takes input from the utterance encoder.",
        "To represent each utterance, HRLCE utilizes ELMo [29],Glove [45], and Deepmoji [46].",
        "The context encoder in HRLCE adapts the bc-LSTMframework followed by a multi-head attention layer.",
        "Huang et al. [40] applied HRLCE framework only on theVOLUME 7, 2019 100951",
        "S. Poria et al.: ERC: Research Challenges, Datasets, and Recent AdvancesEmoContext dataset. However, HRLCE can be easily adaptedto apply on other ERC datasets. It should be noted thatnone of the works on the EmoContext dataset utilize speakerinformation. In fact, in our experiments, we found that Dia-logueRNN, which makes use of the speaker information,performs similar (Table 6) to Bae et al. [24], Huang et al. [40],and Chatterjee et al. [37] on EmoContext dataset. One pos-sible reason for this could be the presence of very shortcontext history in the dataset that renders speaker informationinconsequential.",
        "V. CONCLUSIONEmotion recognition in conversation has been gaining popu-larity among NLP researchers. In this paper, we summarizedthe recent advances in this task and highlight several keyresearch challenges associated with this research area. Fur-ther, we pointed out how current work has partly addressedthese challenges, while also presenting some shortcomings.",
        "Overall, we surmised that an effective emotion-shift recog-nition model and context encoder can yield signicant per-formance improvement over chit-chat dialogue, and evenimprove some aspects of task-oriented dialogue. Moreover,challenges like topic-level speaker-specic emotion recog-nition, ERC on multiparty conversations, and conversationalsarcasm detection can form new research directions. Addi-tionally,ne-grained speaker-specic continuous emotionrecognition may become of interest for the purpose oftracking emotions during long monologues. We believe thataddressing each of the challenges outlined in this paper willnot only enhance AI-enabled conversation understanding, butalso improve the performance of dialogue systems by cateringto affective information."
    ]
}
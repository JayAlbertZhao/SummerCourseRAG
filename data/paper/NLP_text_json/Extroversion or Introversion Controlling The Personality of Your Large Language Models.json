{
    "Abstract": "Large language models (LLMs) exhibit robustcapabilities in text generation and comprehen-sion, mimicking human behavior and exhibit-ing synthetic personalities. However, someLLMs have displayed offensive personality,propagating toxic discourse. Existing litera-ture neglects the origin and evolution of LLMpersonalities, as well as the effective person-ality control. To fill these gaps, our study em-barked on a comprehensive investigation intoLLM personality control. We investigated sev-eral typical methods to influence LLMs, in-cluding three training methods: Continual Pre-training, Supervised Fine-Tuning (SFT), andReinforcement Learning from Human Feed-back (RLHF), along with inference phase con-siderations (prompts). Our investigation re-vealed a hierarchy of effectiveness in control:Prompt > SFT > RLHF > Continual Pre-train.Notably, SFT exhibits a higher control suc-cess rate compared to prompt induction. Whileprompts prove highly effective, we found thatprompt-induced personalities are less robustthan those trained, making them more prone toshowing conflicting personalities under reversepersonality prompt induction. Besides, har-nessing the strengths of both SFT and prompt,we proposed Prompt Induction post SupervisedFine-tuning (PISF), which emerges as the mosteffective and robust strategy for controllingLLMs' personality, displaying high efficacy,high success rates, and high robustness. Evenunder reverse personality prompt induction,LLMs controlled by PISF still exhibit stableand robust personalities. Codes and datasetsare available at here.",
    "Keywords": null,
    "Body": [
        "IntroductionWith the rapid advancement of large-scale pre-training (Kaplan et al., 2020; Brown et al., 2020;",
        "Chowdhery et al., 2022), large language models(LLMs) have made significant strides in natural∗Corresponding author.",
        "PersonalityPrompt InductionPrompt-induced LLMsLLMsPersonalityTrainingTraining-controlled LLMsPersonalityAssessmentPersonalityAssessmentEfficacySuc. RateRobustnessPersonalityControl MetricFigure 1: Overview. We embarked on a comprehensiveinvestigation into personality control with typical meth-ods to influence LLMs.",
        "language processing, demonstrating robust capa-bilities in text generation and comprehension (Weiet al., 2022b). Enabled by vast amounts of trainingdata, LLMs mimic human characteristics in out-puts, showcasing synthetic personalities (Serapio-García et al., 2023). However, variations in ar-chitecture, training data, and methodologies yielddistinct synthetic personalities among differentLLMs (Miotto et al., 2022; Pan and Zeng, 2023).",
        "Despite their capabilities, some LLMs have dis-played offensive personality, propagating toxic dis-course (Wen et al., 2023; Ganguli et al., 2022;",
        "Deshpande et al., 2023). These concerns surround-ing LLMs' synthetic personalities have garneredwidespread attention in AI safety and psychologyresearch (Hagendorff, 2023; Demszky et al., 2023).",
        "Currently, the mechanisms underlying LLMpersonality formation remain largely unexplored.",
        "Previous community efforts have primarily fo-cused on validating human personality assess-ments on LLMs, supported by psychological theo-ries (Serapio-García et al., 2023; tse Huang et al.,2023), adapting human personality assessment tocharacterize LLM personalities (Miotto et al., 2022;",
        "Pan and Zeng, 2023) and exploring personality as-sessments suitable for LLMs (Jiang et al., 2023).",
        "Notably, Serapio-García et al.(2023) found that per-sonality assessments in the outputs of some LLMsarXiv:2406.04583v1  [cs.CL]  7 Jun 2024",
        "are reliable and valid. Additionally, a few stud-ies have explored inducing personality in LLMsthrough prompts or fine-tuning (Karra et al., 2023;",
        "Serapio-García et al., 2023; tse Huang et al., 2023).",
        "However, existing literature neglects how to ef-fectively control the personality of LLMs and en-sure its stability and resistance to alteration. Fillingthese gaps is crucial due to the immense potentialto utilize LLMs with well-defined and consistentpersonalities. This enables customization of LLMs'",
        "synthetic personalities to suit specific contextualrequirements. For example, rational LLMs mightexcel in logical reasoning tasks, while empatheticLLMs could be ideal for companion robots. Basedon these considerations, in this work, we exploretwo questions: 1) During building and using LLMs,what factor has a greater impact on shaping LLMs'",
        "synthetic personality? 2)How to control LLMs'",
        "synthetic personality effectively and robustly?",
        "To answer these questions, we examined syn-thesized personality control using several typicalfactors, encompassing three training methods (Con-tinual Pre-train (Han et al., 2021), Supervised Fine-Tuning (SFT), Reinforcement Learning from Hu-man Feedback (RLHF) (Ouyang et al., 2022; Baiet al., 2022)), along with inference phase consider-ations (prompts) – guided by MBTI theory (Myers,1962; Pittenger, 1993; McCrae and Costa, 1989).",
        "The MBTI theory categorizes individuals into six-teen personality types , delineated by preferencesacross four dichotomous dimensions. Each dichoto-mous dimension comprises two opposite person-ality traits . For instance, the Attitude dimensioncomprises two traits – Extroversion vs.Introver-sion. The trait combinations on four dimensionsdetermine a specific personality type (e.g., ENFPpersonality type: Extraversion ,Intuition ,Feeling ,Perceiving ). Thus, we can naturally differentiatecontrol targets into overall personality types or lo-cal traits. In this work, we delved into SpecificTrait Control andSpecific Personality Control .",
        "To study LLM synthesized personality control,we constructed corresponding trait datasets andpersonality datasets, e.g. 2500 trait instructionsfor SFT and 20,000 paired trait data for the RLHFReward Model, which can significantly serve ourresearch. To measure changes in personality pre-and post-control of LLMs, we specifically designedquantitative metrics, involving Induction SuccessRate ( ISR) and Trait Induction Efficacy ( TIE) forassessing the effectiveness of trait control, TraitStabilization Efficacy ( TSE ) to monitor fluctua-tions induced by trait control, Personality Induc-tion Success Rate ( PISR ) and Personality Induc-tion Efficacy ( PIE) for assessing the effectivenessof personality control. We visually illustrate ourmain exploration in Figure 1.",
        "Our investigation unveiled a hierarchy of effec-tiveness in LLM personality control: Prompt > SFT> RLHF > Continual Pre-train. Particularly, SFTdemonstrates a higher control success rate com-pared to prompt induction. While prompts provehighly effective, we note that prompt-induced per-sonality is less robust than those shaped throughtraining, rendering them susceptible to conflictingpersonality shifts via reverse personality promptinduction. Besides, by leveraging the strengthsof both SFT and prompts, we proposed PromptInduction post Supervised Fine-tuning (PISF),which enhances control effectiveness and personal-ity robustness, characterized by high efficacy, highsuccess rates, and high robustness. Even whenexposed to reverse personality prompt induction,personalities under PISF control demonstrate resis-tance to change.",
        "Our contributions can be summarized as follows:",
        "•We are the first to systematically investigatethe factors influencing LLM personalities andeffective methods for controlling them.",
        "•Our investigation unveiled a hierarchy of ef-fectiveness in control: Prompt > SFT > RLHF> Continual Pre-train. Additionally, we pro-posed Prompt Induction post Supervised Fine-tuning (PISF), which emerges as the most ef-fective and robust method for controlling syn-thetic personalities and exhibits high efficacy,high success rates, and high robustness.",
        "•We provided a comprehensive dataset contain-ing all traits and personalities, facilitating thor-ough exploration of each training method. Weproposed several quantitative metrics to eval-uate the effectiveness of specific trait controland specific personality control. These contri-butions will accelerate research in the field.",
        "2 Background: Personality AssessmentIn this section, we introduce two key personalitymodels widely used in research: the Myers-BriggsType Indicator (Myers, 1962; Pittenger, 1993; Mc-Crae and Costa, 1989) and the Big Five (Goldberg,",
        "Figure 2: Instruction Data Generation with Prompt-induced LLMs. Utilizing the Least-to-Most (Zhou et al., 2023b)",
        "mindset, we partitioned the data generation process into two stages: initially crafting questions rooted in OppositeTrait Description, followed by eliciting responses from Prompt-induced LLMs.",
        "1990). We then discuss the general form of person-ality assessment derived from these models. Lastly,we provide a concise overview of how personalityassessment is approached in our study.",
        "Big Five The Big Five model emerged from lexicalanalysis of English personality adjectives (Gold-berg, 1990). It encompasses five key dimensions:",
        "Openness (O), Conscientiousness (C), Extraversion(E), Agreeableness (A), and Neuroticism (N). BigFive assessments yield continuous and quantitativescores for each factor (e.g. O: 47%, C:64%, E:",
        "51%, A:52%, N:38%), posing difficulties in cate-gorizing and studying similar individual profiles.",
        "Myers-Briggs Type Indicator The MBTI theory,stemming from Carl Jung's seminal work (Jungand Baynes, 1923), categorizes individuals intosixteen personality types based on preferencesacross four dichotomous dimensions (Extro-version/Introversion, Sensing/Intuition, Think-ing/Feeling, and Judging/Perceiving). Each individ-ual's personality type is characterized by a profileof four traits (e.g., ENFP: Extraversion, Intuition,Feeling, Perceiving) representing their preferencesin each dimension. In contrast to the continuousnature of the Big Five, MBTI's discrete personalitytypes facilitate the study of specific groups withsimilar personalities.",
        "General Form of Personality Assessment Per-sonality assessments commonly consist of LikertItems (Likert, 1932) crafted according to estab-lished personality models. Likert Items are state-ments or questions presented to respondents forevaluation, typically utilizing a five to seven-pointscale to gauge agreement or disagreement (Kulaset al., 2008). The form is known as Likert Scale.",
        "As shown in Table 2, People who know you tend todescribe you as and the following options representa Likert Item. And Task Description delineates dif-ferent levels of agreement. We can map the level ofagreement in responses following the Item Postam-ble to 5-point scale.",
        "Personality Assessment in Our Work For re-search convenience, we utilized the MBTI person-ality model for assessments and dataset construc-tion. To ensure reliability, we collected publiclyavailable questionnaires (Pan and Zeng, 2023) andrefined them into a 200-item MBTI Assessment (50items per dichotomous dimension). We detailedthe format and sources of the questionnaires in theAppendix A for further reference. The rating scalewas designed based on 5-point Likert Scale. (§3.2)",
        "3 MethodologyTo delve into the control of LLMs' synthesized per-sonalities during training, we constructed datasetsfor three training methods based upon MBTI (My-ers, 1962)(§3.1). For research convenience, weutilized the MBTI personality model for assess-ments (§3.2). To measure personality variationspre- and post-controlling LLMs, we proposed sev-eral quantitative metrics for specific trait controland specific personality control (§3.3).",
        "3.1 Personality Dataset Construction forPopular Training MethodsWe utilized popular training methods at each stage- autoregressive for continual pre-train, instructiontuning for SFT and PPO for RLHF. To meet the",
        "requirements of various training methods, we con-structed corresponding personality datasets.",
        "Continual Pre-training. We continual pretrainedLLMs using widely adopted autoregressive objec-tive (Radford et al., 2019; Brown et al., 2020). Toelaborate, the model is trained to predict the next to-ken in a sequence of continuous text by leveragingthe surrounding context.",
        "Due to the difficulty of generating a large amountof long-context pre-training data, we amalgamatedand refined existing datasets annotated with humanpersonality (Storey, 2018). The data distributioncan be referred to Figure 3.",
        "Intuitively, a control method that achieves betterperformance with less data is preferable. Thus, weinvestigated the control of continual pre-trainingwith limited data. We randomly sampled 10,000instances from human labeled data for each per-sonality and aggregated data of eight personalitieswith the target trait as the trait data. Specifically,we aggregated 10,000 ENFJ, ENFP, ENTJ, ENTP,ESFJ, ESFP, ESTJ, ESTP personality data as E traitdata. Thus, each trait comprises 80,000 instances.",
        "INFPINTPINFJENTPINTJENTJISTPENFPENFJISFJISTJESTPISFPESTJESFJESFPPersonality Types0.02.55.07.5Samples1e5Figure 3: Pretrain Data Distribution.",
        "SFT. We adopted widely used instruction tun-ing (Wei et al., 2022a; Taori et al., 2023; Chianget al., 2023) as the training objective. Instruc-tion tuning refers to the process of further trainingLLMs on a dataset consisting of (instruction, out-put) pairs in a supervised fashion, which bridgesthe gap between the next-word prediction objec-tive and the objective of making LLMs adhere tohuman instruction (Zhang et al., 2024).",
        "Akin to numerous studies utilizing LLMs fordata generation (Wang et al., 2023; Taori et al.,2023; Lee et al., 2023), as shown in Figure 2, weutilized prompt-induced LLMs to generate trainingdata. To enhance data quality, inspired by Least-to-Most (Zhou et al., 2023b), we divided the data gen-eration process into two stages: In Stage 1, to assistthe model in distinguishing between opposite traits,we incorporated descriptions of two opposite traitsbelonging to the same dimension into the prompt,followed by prompting the model to generate ques-tions. In Stage 2, we employed prompt-inducedmodels to respond to respective questions. Thenwe integrated the obtained questions and responsesas SFT dataset. To ascertain the capacity of LLMsin producing data with specific personalities, weconducted a preliminary investigation (§4) to val-idate the prompt induction proficiency based onpersonality description.",
        "For SFT, the quantity of data is not critical (Zhouet al., 2023a). Thus, we utilized prompt-inducedGPT-3.5-turbo-1106 to generate 2,500 instancesfor each trait and aggregated four trait data foreach personality type. Specifically, we aggregatedinstances of E, N, T and J as ENTJ personality data.",
        "Therefore, each personality has 10,000 instances.",
        "RLHF. Following previous work (Ziegler et al.,2020; Ouyang et al., 2022), we initially trainedreward model directly from feedback, and subse-quently utilizes it as a reward function to enhancean agent's policy via widely-used proximal pol-icy optimization (PPO). In our work, the rewardmodel is trained in a supervised manner to classifyresponses to prompts as either conforming to thetarget trait or personality (high reward) or not (lowreward). We constructed datasets for PPO trainingand reward training.",
        "For PPO, to control variables, we used the samequestions from SFT data as input. And for rewardmodel, we used prompt-induced LLMs to generatequestion-answer pairs. We instructed models toprovide responses to the same question but withopposing traits, establishing a ranking relationship.",
        "For instance, we trained extroversion reward modelwith (Question, Extroversion-induced model re-sponse, Introversion-induced model response).",
        "Inspired by InstructGPT (Ouyang et al., 2022),we generated in-distribution data to fit the modeldistribution as well as out-of-distribution data forgeneralization. Specifically, for each trait, we usedprompt-induced GPT-3.5-turbo-1106 to generate5000 pairs as out-of-distribution data and employedprompt-induced Llama2-chat-13B and ChatGLM2-6B to generate 15,000 pairs as in-distribution data.",
        "The total comprises 20,000 pairs for each trait. Sim-ilar to SFT, we integrate trait data to obtain person-ality data, i.e. 80,000 pairs for each personality.",
        "DatasetTrait PersonalityTrain Valid Train ValidPre-train 80000 - 10000 -SFT 2500 - 10000 -RLHF-PPO 2500 - 10000 -RLHF-Reward 18000 2000 72000 8000Table 1: Dataset V olumn. For RLHF-Reward, we ran-domly split 10% of the data as the validation set.",
        "Summary In Table 1, we summarize the data vol-ume of various datasets. Notably, for each trait andpersonality, we constructed corresponding datasetsat three training stages. Further details (e.g. spe-cific instances) can be found in Appendix C.",
        "3.2 Personality AssessmentEvaluation Prompts ExamplePlease select a number from [1, 2, 3, 4, 5] to answer thefollowing question.",
        "For this question, the five numbers [1, 2, 3, 4, 5] representspecific meanings: 1 represents strongly agreeing with op-tion A, 2 represents agreeing with option A, 3 representsneutral, 4 represents agreeing with option B, and 5 repre-sents strongly agreeing with option B.",
        "You need to answer the following question:",
        "People who know you tend to describe you as:",
        "Option A:Logical and clarity. Option B:Passionate andsensitive.",
        "Please answer with a number:",
        "Table 2: Evaluation Prompts for Likert Items. ItemPreamble, Item, and Item Postamble. An Item Preambleconsists of a Task Instruction, a Task Description and aTest Instruction.",
        "To ensure reliability, we compiled publicly ac-cessible MBTI personality questionnaires, refinedthem into a 200-item MBTI Assessment (Pan andZeng, 2023). As shown in Table 2, we formu-lated items into Evaluation Prompts. Given that themodel sometimes exhibits different performanceacross different prompts (Wei et al., 2022c; Donget al., 2023), we designed five prompt sentenceswith the same semantics but different expressionsfor each component to obtain convincing statisticalperformance and mitigate extreme performance.",
        "We illustrated the process of personality assess-ment in Figure 4. First, we organized the ques-tionnaires using the designed Evaluation Prompts.",
        "Then, we obtained the model's responses andmapped them to the corresponding scores for eachtrait based on 5-point Likert scale (Likert, 1932).",
        "After personality assessment, we calculated therates ( R) between two opposite traits within theItemsItemsPeople who know youtend to describe you as:",
        "Option A. Logical & ClarityOption B. Passionate & SensitiveMBTIQuestionnaireItemsItemsPlease select a number from[1, 2, 3, 4, 5] to answer thefollowing question...",
        "EvaluationPromptsItemsItemsSure! I'll choose number 4,agreeing with option B.",
        "12345TTT&FFFLLMsResponsesAnswerExtractorFigure 4: Personality Assessment Process. Tstands for‘Thinking' trait and Fstands for ‘Feeling' trait.",
        "same dimension. For instance, in the Attitude Di-mension: assuming Escores 137, and Iscores 67,then we compute R(E) = 67% ,R(I) = 33% .",
        "3.3 Metrics of Personality ControlTo measure changes in personality pre- and post-control of LLMs, we proposed quantitative metricsfor evaluating both Specific Trait Control and Spe-cific Personality Control.",
        "For Specific Trait Control, we devised evalua-tion metrics to evaluate the efficacy of controllingtarget traits, as well as metrics aimed at evaluatingthe fluctuations of unrelated traits (e.g. control Ex-troversion should not affecting Sensing) based onpersonality assessment outcomes.",
        "To assess the effectiveness of specific trait con-trol, we calculated two metrics: Induction SuccessRate ( ISR)andTrait Induction Efficacy ( TIE).",
        "ISR provides a broad evaluation of whether thecontrol successfully induces the target trait, whileTIE offers a more detailed measure of control effi-cacy on the target trait. To evaluate the impact onunrelated traits during control, we computed theTrait Stabilization Efficacy ( TSE )to monitor thefluctuations of unrelated traits induced by the con-trol. Further details of these indicators are providedbelow:",
        "In the MBTI theory, personality type is deter-mined by four dichotomous dimensions, each com-prising two opposite traits. Let's denote these di-",
        "mensions as set Dand the traits as set T. Fol-lowing personality assessment in 3.2, we obtainedrates of pre- ( Rpre) and post- ( Rpost) control foreach trait. For a target trait t∗within dimension d∗,we compute ISR,TIE, and TSE as follows:",
        "TIE(x) = R post(x)−Rpre(x)",
        "TSE( t∗) =1|D/d∗|Xd∈D/d∗Xt∈S(d)|TIE(t)|Here, xis a trait in T, and1denotes an indica-tor function. S(d)represents a selection function.",
        "Given that opposite traits within the same dimen-sion have identical fluctuations, we utilized S(d)toselect fixed traits from each dimension for compu-tation (e.g. select E trait for Attitude dimension).",
        "For Specific Personality Control, our focus is onthe effectiveness of altering the overall personalityof LLMs. To assess this, we employ two metrics:",
        "Personality Induction Success Rate ( PISR )andPersonality Induction Efficacy ( PIE). Similar toISR andTIE,PISR provides a broad evaluationof personality control success, while PIE offersa detailed measure of the efficacy of personalitycontrol. Denoting personality types as set P, wecomputed PISR andPIE as follows:",
        "PISR =1|P|Xp∈PYt∈pISR(t)",
        "PIE(p) =1|p|Xt∈pTIE(t)",
        "Here, prepresents a personality type in P, com-posed of four traits.",
        "4 Preliminary InvestigationIn this section, we conducted investigation tovalidate the prompt induction proficiency ofLlama2-family (Touvron et al., 2023) and Qwen-family (Bai et al., 2023), validating LLMs' abilityto generate personality data.",
        "From Figure 5, it's evident that both Qwens andLlama2s demonstrate robust role-playing capabili-ties. Particularly, in role-playing specific traits, allLLMs except Qwen-chat-1.8B show adept perfor-mance induced by prompts. Moreover, this capabil-ity generally improves with larger model parametersizes, possibly due to its enhanced ability to fol-low instructions resulting from the larger modelparameter size. Hence, prompt-induced LLMs areable to embody specific personalities for trainingdata generation. In our work, we utilized GPT-3.5-turbo-11061for this task.",
        "1.8 7 14 72Model Size/B0255075100Qwens Metrics/%7 13 70Model Size/B0255075100Llama2s Metrics/%PIETIEPISRISRFigure 5: Prompt Induction Performance of Qwen-family and Llama2-family. Qwens utilized the de-fault generation configuration, while Llama2s employedGreedy Search for generation.",
        "5 Experiments5.1 SettingModels. We trained Llama2-chat-13B (Touvronet al., 2023), Qwen-chat-7B (Bai et al., 2023),ChatGLM2-6B (Zeng et al., 2023; Du et al., 2022).",
        "ChatGLM2-6B has no system prompt.",
        "Continual Pre-train. We conducted training on6 A800-80GB GPUs for 1 epoch with a max se-quence length of 2048, a learning rate of 5e-6, andDeepSpeed Integration. The whole training pro-cess took nearly 2.5 days for Qwen-chat-7B andChatGLM2-6B, and approximately 4.5 days forLlama2-chat-13B.",
        "SFT. We fine-tuned using LoRA (Hu et al., 2022)",
        "for 2 epochs , employing a learning rate of 5e-4, aLoRA rank to 8, a LoRA alpha to 8, and a LoRADropout (Srivastava et al., 2014) to 0.1.",
        "RLHF. We adapted Deepspeed-Chat (Yao et al.,2023) for the RLHF training phase. For PPO andreward model, we both trained for 1 epoch, a maxlength of 512 and 1 PPO epoch.",
        "For more details, please refer to Appendix D.",
        "5.2 Main Results and AnalysisIn this section, we explored the first question:",
        "Which approach has a greater impact on shapingLLMs' synthetic personality? We investigated fromtwo angles: control effectiveness (efficacy and suc-cess rate) and personality robustness.",
        "1https://platform.openai.com/docs/",
        "CPRLHF SFTPrUPrS051015202530TIE/%CPRLHF SFTPrUPrS051015202530PIE/%CPRLHF SFTPrUPrS020406080100ISR/%Llama2QwenChatGLMCPRLHF SFTPrUPrS020406080100PISR/%Figure 6: Control Performance of Various Methods. All results represent the average results of five EvaluationPrompts conducted on all trait or personality models. CP stands for Continual Pre-train and Pr stands for Prompt.",
        "The superscript Sstands for system prompt and Ustands for user prompt.",
        "EISNTFJPEISNTFJPEISNTFJPSFT Control RLHF Control Pretrain Control Default Synthetic PersonalityLlama2-chat-13B Qwen-chat-7B ChatGLM2-6BFigure 7: Specific Trait Control Across Various Training Stages. In order to facilitate the comparison, we summarizedthe effects of controlling eight traits into a single radar plot.",
        "In Figure 6, We showcased the performance ofdiverse methods for personality control across arange of models. In evaluating the efficacy of con-trol (TIE,PIE), we noted that the prompt yieldedthe best results, surpassing all methods in five outof six combinations. For other methods, SFT alsooutperformed RLHF in five out of six combina-tions. The least effective method was ContinualPre-training. As illustrated in Figure 7, we noteda larger radar plot for the SFT, followed by RLHF.",
        "Continual pretraining exhibited minimal deviation.",
        "In terms of control success rate ( ISR,PISR ), SFTemerged as the most effective method in the mostcases, with the prompt following behind.",
        "Overall, our investigation unveiled a hierarchyof effectiveness in control: Prompt > SFT > RLHF> Continual Pre-train. Particularly, SFT demon-strates a higher success rate in control comparedto prompt induction. This could be attributed tothe disparity in lexical signals between personal-ity data and prompt. And the gap between SFTand RLHF may arise from both the reward modeland actor model experiencing performance declinedue to reduced parameter size. Moverover, the un-derwhelming performance of the pretraining datacould stem from the limited influence of the person-ality data on original mixed personality distribution.",
        "For additional validation, we scale up the trainingdata for Continual Pre-train in the Appendix E.",
        "Subsequently, we conducted a comparative anal-ysis of personality robustness between SFT andprompt. To evaluate the personality robustnessacross different methods, we further employedReverse Personality Prompt Induction ( RPPI ) (e.g.",
        "induce ISFP from ENTJ.) to assess. Due to conflict-ing personalities, LLMs under robust personalitycontrol should not perform well on RPPI tasks.",
        "As shown in Table 3, under reverse personal-ity prompt induction, SFT-controlled models aremore likely to maintain consistent target personal-ities, while prompt-induced models are prone topersonality shifts. our findings indicate that SFT-controlled models exhibit significantly greater per-sonality robustness than prompt-induced models.",
        "5.3 PISF: P rompt I nduction post S upervisedFine-tuningIn this section, we answered the second question:",
        "How to control LLMs' synthetic personality effec-tively and robustly? Based on our prior inves-tigation, SFT and prompt exhibit proficient andcomplementary performance. Thus, an intuitive",
        "SettingLlama2-chat-13B Qwen-chat-7BTIE ISR PIE PISR TIE ISR PIE PISRPromptS22.30 100.00 12.09 87.50 9.72 87.50 2.15 0.00PromptU22.36 100.00 13.72 87.50 22.34 100.00 13.55 75.00PromptSRPPI 9.57 87.50 10.87 50.00 17.80 87.50 10.42 62.50SFTRPPI 9.19 100.00 2.87 12.50 1.48 50.00 -2.85 0.00PISFSRPPI -9.44 12.50 -4.30 0.00 -12.30 12.50 -6.33 0.00Table 3: Personality Robustness Analysis. We employstandard prompt induction with the system prompt andconduct testing using RPPI with the user prompt. Allresults are average scores. In the RPPI setting, lower isbetter; conversely, higher is better.",
        "approach emerges to integrate SFT with promptsfor harnessing the advantages of both methods.",
        "Driven by the idea, we proposed Prompt Inductionpost Supervised Fine-tuning (PISF) for controllingLLMs' synthetic personalities.",
        "Firstly, we compared the control effectivenessof PISF-controlled LLMs against LLMs controlledby other methods. As shown in Table 4, in mostcases, PISF-controlled models outperform bothSFT-controlled models and prompt-induced modelsin both control efficacy ( TIE/PIE) and success rate(ISR/PISR ). It suggests that PISF demonstratessuperior control effectiveness.",
        "Secondly, we analysed the personality robust-ness of PISF-controlled models. As shown in Table3, PISF-controlled models maintain consistent tar-get personalities despite RPPI impact, demonstrat-ing resistance to change personalities. Our findingsindicate that PISF-controlled models exhibit signif-icantly greater personality robustness.",
        "In short, PISF is the most effective and robustmethod for synthetic personality control with highefficacy, high success rates, and high robustness.",
        "SettingLlama2-chat-13B Qwen-chat-7BTIE ISR PIE PISR TIE ISR PIE PISRSFT 15.25 100.00 12.24 100.00 12.38 100.00 12.85 93.75PromptS22.30 100.00 12.09 87.50 9.72 87.50 2.15 0.00PromptU22.36 100.00 13.72 87.50 22.34 100.00 13.55 75.00PISFS23.58 100.00 15.69 100.00 19.56 100.00 14.68 87.50PISFU24.76 100.00 16.19 93.75 24.89 100.00 18.10 100.00Table 4: Personality Control effectiveness. All data arepresented as percentages. The superscript Sstands forsystem prompt and Ustands for user prompt. All resultsare average scores evaluated using greedy search. Forall metrics in the table, higher is better.",
        "6 Related WorkHuman Personality Recognition Prior to LLMs,computational research on personality primarilyfocuses on utilizing tools such as MBTI (My-ers, 1962; Pittenger, 1993; McCrae and Costa,1989) and Big Five (Goldberg, 1990) to identifyhuman personality traits, rather than exploringsynthetic machine personalities. Recent studieshave delved into personality trait recognition fromtext (Liu et al., 2017; Stajner and Yenikent, 2020;",
        "Vu et al., 2018), dialogue (Mairesse and Walker,2006), and multi-modal information (Kampmanet al., 2018; Suman et al., 2020). Recently V Gane-san et al.( 2023) investigated the zero-shot abilityof GPT-3 to estimate the Big Five personality traits.",
        "Unlike prior research focused on human person-ality recognition, our study empirically controlssynthetic personalities in LLMs.",
        "Personality Assessment for LLMs At present, ma-chine psychology (Hagendorff, 2023) lacks a co-herent theoretical framework, with most studiesrelying on human personality assessments (Miottoet al., 2022; Caron and Srivastava, 2023). Jianget al. ( 2023) introduced the Machine PersonalityInventory (MPI) tool, based on the Big Five theory,to study synthetic machine personalities. However,there is still no universally accepted benchmark formachine personality assessment. In our work, wecontinue to utilize human personality assessment.",
        "Synthetic Personality Control in LLMs Priorstudies on synthetic personality control mainlycenter on prompt induction (Serapio-García et al.,2023; Caron and Srivastava, 2023; Jiang et al.,2023; tse Huang et al., 2023), with some exploringfine-tuning methods (Karra et al., 2023). Unlikeprevious research focusing solely on prompts orfine-tuning, our approach takes a comprehensiveview of synthetic personality control, exploringmethods across three training stages and promptsduring the inference phase.",
        "7 ConclusionTo advance the safe utilization of AI, this workexplored synthesized personality control in LLMsacross three training stages and the inference stage,leveraging our designed datasets and metrics. Ourfindings can be summarized as follows: We found ahierarchy of effectiveness in LLM personality con-trol: Prompt > SFT > RLHF > Continual Pre-train.",
        "Additionally, we proposed PISF for controllingLLMs' synthetic personalities, showcasing highefficacy, high success rates, and high robustness.",
        "8 LimitationsDespite our thorough exploration with larger pre-train datasets (Appendix E), it still falls short whencompared to the vast datasets used during the pre-training phase of LLMs. Collecting more personal-ity pre-training dataset and validating the gradualformation of synthesized personalities represent aavenue for future enhancement in our work.",
        "Furthermore, subject to computational con-straints, we have not yet examined the efficacyof training-based control for models with morelarger parameter size (>13B). This limitation is pro-nounced in RLHF, as we rely on the original modelto train the reward model. Scaling down the modelsize concurrently diminishes the performance ofboth the actor and reward models, resulting in erroraccumulation. This could contribute to the subparperformance of RLHF with smaller models. RLHF-based control might exhibit higher potential whenapplied to LLMs with large parameter size."
    ]
}
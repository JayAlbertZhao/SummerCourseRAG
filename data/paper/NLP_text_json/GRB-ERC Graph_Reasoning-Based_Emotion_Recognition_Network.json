{
    "Abstract": " Semantic information from images can be used to improve the performance of deep learningmethods in recognizing human emotions. In this paper, we propose a novel framework based on the graphconvolutional network for emotion recognition by utilizing the semantic relationships of different regions.First, we extract the salient image regions within video frame clips by using the bottom-up attention moduleto construct the node features of a graph. Then, we build the graphs containing the node features and thesemantic correlations of nodes by using the graph convolutional network. For renement, each node featureof graph vectors is enhanced via a gated recurrent unit consisting of gate and memory units to removeredundant feature information. Experimental results show that our proposed method achieves superiorperformance over state-of-the-art approaches for the emotion recognition on the CEAR and AFEW datasets.INDEX TERMS Emotion recognition, graph convolutional neural networks, contextual spatiotemporalfeatures.I",
    "Keywords": " Emotion recognition, graph convolutional neural networks, contextual spatiotemporalfeatures.I",
    "Body": [
        "INTRODUCTIONHuman emotions substantially affect the mutual commu-nication and decision making of humans in daily life [1].",
        "Nowadays, recognizing human emotions plays an increas-ingly important role in various applications. The abilityof intelligent service robots to recognize the emotions ofan interactive user is indispensable. In social media plat-forms, extracting social sentiment from textual data is ben-ecial for digital monitoring and online information push-ing. In the medicaleld, an emotion recognition model canrecognize the emotions of patients and provide appropriatetreatments by analyzing physiological signals. Unlike objectrecognition and classication, emotion recognition in reallife requires sensory-based reasoning. For example, peoplededuce the emotion category by subconsciously reasoning thefacial expressions, voice intonations, and body movements ofhuman beings.",
        "The efforts in emotion recognition have mostly beendivided into two main categories: physiological signal-basedmethods [2] and non-physiological signal-based methods.",
        "Physiological signals include electroencephalogram (EEG)",
        "The associate editor coordinating the review of this manuscript andapproving it for publication was Zahid Akhtar.signal [3]\u0015[5], galvanic skin response signal [6]\u0015[8], andelectrocardiogram signal [9], [10].Other methods not basedon physiological signals identify emotions by extracting thefeatures from various types of data, such as videos/images,speeches, and texts. Traditional methods based on thevideos/images recognize emotions by utilizing the hand-crafted features [11]\u0015[13]. Handcrafted features are extractedto form vectors representing the geometry of the face, whichinclude the facial shape and locations. Shan et al. [11] usedsupport vector machine (SVM) classiers to recognize facialemotions with local binary pattern (LBP) representation.",
        "Zhao and Pietikainen [12] recognized the dynamic textureof facial expressions by extracting facial local informationand its spatial locations using volume local binary pat-terns (VLBP) operator. However, handcrafted feature-basedmethods normally require precise and reliable detection andtracking of facial components, which are difcult in manysituations. With the proposal of deep learning methods, theabove problems can be solved.",
        "In recent years, Convolutional Neural Network (CNN)-based methods [14]\u0015[16] have achieved more accurate androbust emotion recognition than previous methods withchanges in surrounding information. Yu and Zhang [15] cre-ated a model to recognize the emotions of static images,6488This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 9, 2021",
        "Q. Gao et al.: GRERNwhich contains three face detectors and a multiple deep CNNsmodule. Unlike traditional algorithms that can only detectfacial emotions with frontal facial components, this recog-nition system can detect spontaneous facial expression in thewild. In visual emotion recognition, other visual cues such asbody gestures, actions, and environmental contexts can showadditional useful information. Thus, Lee et al. [14] integratedthe facial expressions and surrounding information of peoplewith adaptive fusion networks to demonstrate that the perfor-mance of emotion recognition networks can be remarkablyboosted by integrating facial and context information. In fact,the emotion recognition system can analyze features at thelocal pixel level, which are extracted by a specic convo-lution receptiveeld. However, learning the relationships ofhigh-level semantic information among regions is difcult.",
        "In the topology data structures, the Graph ConvolutionalNetwork (GCN) can fully extract the relationship featuresbetween node vectors. Inspired by the application of GCNin EEG emotion recognition and textual emotion recogni-tion, we present a novel GCN-based framework to recog-nize human emotions by globally capturing the relationshipsbetween different semantic regions, which included face andscene contexts. First, we identify salient regions in videosat the object level with bottom-up attention [17], which isachieved by using Faster R-CNN [18] and is similar to thehuman visual system. Then, we convert multiple regionsof video frames into corresponding node vectors of thegraph structure. In this way, the data conversion betweenEuclidean structure and graph structures can be achieved.",
        "We also generate semantic relationships of node vectors byutilizing GCN to explore the interrelation between thesenode vectors of notable regions. Although diverse semanticrelationships could contribute to boosting the accuracy ofemotion recognition, part of the relationships among themare redundant. Thus, we utilize a gated recurrent unit (GRU)",
        "to select high-level graph features before generating thenalrepresentation of the entire video frame. In specic, ourproposed GCN-based emotion recognition framework has thefollowing major contributions:",
        "1) Unlike the traditional CNN-based methods that sim-ply consider the context of images or video frameinput, our proposed network recognizes emotions fromcontext-aware emotion datasets with reasonable andlter mechanism, which can reason the relationshipsof different regions and remove redundant informationamong node vectors.",
        "2) This work examines distinct approaches to verify thatthe proposed framework extracts spatiotemporal fea-tures more efciently than previous methods overthe Context-aware Emotion Recognition (CAER) [14]",
        "and Acted Facial Expressions in the Wild (AFEW)",
        "[19] datasets, which includes the complex contextinformation.",
        "3) The proposed method is more accurate than prior meth-ods [14], [20], [21] for emotion recognition on theCAER and AFEW datasets. Our network outperformsthese baseline methods by 7%-9% improvement onthe CAER dataset. Furthermore, we conduct ablationstudies to justify the effectiveness of combining GCNand GRU.",
        "II. RELATED WORKA. TRADITIONAL METHODS FOR EMOTION RECOGNITIONBefore the popularity of CNN-based methods, most emo-tion recognition research has been dominated by traditionalmethods using handcrafted features or shallow classierssuch as the Facial Action Coding System (FACS) with actionunits (AUs) [22], [23], local binary patterns (LBPs) [11],[12], [24], and sparse learning [13]. Tian et al. [22] devel-oped an Automatic Face Analysis (AFA) system to recognizeemotions by describing slight changes in a face into AUsof FASC, which contains permanent and transitory facialfeatures. Different from some studies based on original faceimages, some methods recognize facial expressions usingstatistical local facial features. Shan et al. [11] extractedBoosted-LBP features that represent the most salient LBPfeatures and then utilized SVM classiers to perform emotionrecognition using the extracted the Boosted-LBP features.",
        "This method can also work well for low-resolution facialexpression. However, Zhong et al. [13] observed in facialexpression recognition that only a few facial componentsare useful. In [13], a framework of two-stage multi-tasksparse learning was proposed to efciently locate commonpatches of each expression and learn particular expressionpatches, and then emotional classication results were pre-sented by trained SVM classiers with the facial patches.",
        "Wang et al. [23] exploited the complicated semantic rela-tionships among facial AUs to recognize the facial emotionsgenerated by the restricted Boltzmann machine. Although theabove-mentioned traditional methods have shown excellentemotion recognition results by extracting the facial featureson the datasets generated in lab-controlled environments, therobustness of these methods is unsatisfactory when the faceimages include various head pose changes, which may makethe traditional methods fail to extract the useful AUs or LBPfeatures.",
        "B. DEEP LEARNING METHODS FOR EMOTIONRECOGNITIONWith the rapid development of CNN in emotion recognition,the approaches of extracting facial features have been injectedinto the new vigor and vitality. CNN-based methods haveachieved more robust and superior performance than tradi-tional methods in the wild datasets by extracting high-levelsemantic features [25]. Liu et al. [26] presented a uniedframework, Boosted Deep Belief Network, for integratingthree training stages through joint training. The method basedon joint training strengthens the capabilities of facial fea-ture selection and facial expression classication. Based onthe above methods, these facial expression systems detectemotions through various spatial facial features. ConsideringVOLUME 9, 2021 6489",
        "Q. Gao et al.: GRERNFIGURE 1. The overview of Graph Reasoning-based emotion recognition network (GRERN).",
        "temporal information, Jung et al. [27] proposed a deepnetwork that includes two models combined with a newintegration method to extract temporal features from imagesequences and facial landmark points. The audio streaminformation also contributes to the emotion recognition ofvideos in addition to classical facial expression features.",
        "Kahou et al. [28] developed a framework to merge fourmodality models. Therst model based on several convo-lutional networks is trained to recognize emotion categoriesby extracting the facial features in each video frame. Then,another model captures emotion features from audio stream.",
        "The third model captures body actions by analyzing the tem-poral features of video frames. Thenal model is a shallownetwork trained to learn the mouth action features in the videosequence. Although this method highlights the importanceof different data types for emotion recognition, the systemonly works on the frontal face. Thus, the robustness of thisemotion recognition system is limited in the wild environ-ment dataset. As described above, Zhang et al. [29] proposeda framework to recognize pose-variant facial expression andbody pose. In addition, this network introduced the generativeadversarial network to enlarge and enrich the training datasetby generating facial expression images under different poses.",
        "However, those methods analyze the emotion features fromthe facial expression dataset without exploiting the contextualinformation of the environment. Lee et al. [14] provided adataset collected from 79 TV shows. This dataset contains notonly the facial expression but also complex and real surround-ing contextual information. Lee et al. used this dataset toevaluate the role of contextual information in emotion recog-nition. They proposed a CNN-based framework to detectemotions by fusing the facial features and contextual features.",
        "Compared with methods focusing only on facial expressionfeatures, such a method has achieved better accuracy of emo-tion recognition.",
        "C. GRAPH CONVOLUTION NETWORKRecently, GCN has been widely used for emotion recog-nition, especially in physiological signals and text emotionrecognition, because of its powerful capacity for extractingthe relationships of emotion features [30]. GCN can exploitthe relationship features between graph structure data, whichcannot be achieved by CNNs. Therefore, some studies basedon GCN have been proposed to effectively tackle tasks thatrequire rich relational structure data and depend on the globalinformation of graph data to achieve their goals, such asskeleton action recognition [31], natural language processing[32], and emotion recognition [20], [33]\u0015[36]. However, mostGCN-based emotion recognition methods use texts or phys-iological signals. Zhang et al. [20] established a two-branchnetwork for image emotion recognition. One branch utilizescontextual features to deduce emotion information throughGCN. The other one learns body action features by trainingthe VGG-16 [37] network. This method was implementedon the image dataset and utilized the context relationshipson the spatial domain. However, the research on the com-bination of the spatial and temporal contextual features invideo emotion recognition is still a daunting challenge. In thispaper, we propose a network integrating the GCN and GRU torecognize human emotions in the videos, which can considerthe contextual factors from video sequence to infer emotionfeatures in the surrounding.",
        "III. PROPOSED METHODThis section describes the detailed structure of the GraphReasoning-based Emotion Recognition Network (GRERN)",
        "for videos, as shown in Figure 1. Our approach extractsspatiotemporal features of environmental context and facialexpression for emotion recognition. However, GCN is limitedto directly process Euclidean structure data-like video frames.",
        "Therefore, in therst stage, we convert the regions of videoframe to node vectors of graph structure (a set of video framescorresponds to a graph structure) by the bottom-up attentionmodel [17] (Sec. III-A). In this way, we map the consecutivevideo frames into the topology structure space to match thenext GCN operation. Then, we establish connections betweenthese node vectors of the graph with learnable weights. In thesecond stage of learning semantic relationship information6490 VOLUME 9, 2021",
        "Q. Gao et al.: GRERNamong the node features, we apply four layers of GCN aftertherst stage of operation, which can extract the spatialfeatures and the temporal information of the surroundingcontext (Sec. III-B). Then, we convert a set of video framesto a graph structure. We use a GRU to rene the graphfeatures by removing redundant features. Finally, we inputthe graph features into a SoftMax layer to classify the emotioncategories (Sec. III-C).",
        "A. GRAPH NODE STRUCTURE TRANSITIONIn the beginning, we denote a set of Tvideo frames of theCAER datasets as VDfv 1;:::; vTg. To obtain the graphstructure data from videos, we transform video frames Vintothe graph structure GDfN 1;:::; NTg;Ni2RK\u0002Dthroughthe bottom-up attention mechanism. This graph structureis composed of multiple node vector sets. In other word,we convert the ivideo frame viinto the iset of node vec-tors NiD fn i1;:::; niKg;nij2RD, and each single setof node vectors contains Knode vectors. In addition, thesingle node vector is D-dimensional. We utilize the bottom-upattention network to implement the conversion process withFaster R-CNN [18]. To effectively extract the sets of nodevectors Gfor subsequent semantic reasoning, we detect thediscriminative regions of video frames with non-maximumsuppression, and set the 0.7 IoU threshold to select the salientregions. In addition, we set the condence threshold of 0.3 toremove some regions with class probability lower than thethreshold. This attention network selects the top Kregionsof the video frame viand outputs the corresponding featurevectors XiDfx i1;:::; xiKg, which are ranked according tothe detection condence scores. In this way, each video frameis represented by Knode vectors. Finally, we utilize a fullyconnected layer to transform the feature vector xijto a D-dimensional node vector nij, and then concatenate all nodefeature vectors to compose thenal graph structure Gas thefollowing equationVnijDWfxijCbf (1)",
        "where Wfandbfare parameters of the fully connected layer.",
        "xijis the feature vector of the detection model, and nijis thecorresponding node vector. In summary, we form a set ofnode vectors NiDfn i1;:::; niKgwith Knode vectors, whichrepresents a video frame. Futhermore, GD fN 1;:::; NTgrepresents each video clip that consists of Tvideo frames.",
        "B. GRAPH RELATIONSHIP REASONINGIn the Fourier domain, a GCN model can extract the featuresbetween the node vectors. Therefore, we apply the GCNmodel to extract the emotional information hidden in the nodevectors. To start with node vectors Nas well as an adjacencymatrix A, a multi-layer GCN model can be expressed asfollowsVHlC1DReLu(OD\u000012QAOD\u000012HlWl) (2)",
        "whereQADACINis the adjacency matrix Awith addedan identity matrix IN.ODiiDPjQAijis the degree matrix.Wlislthlayer learnable weights, and Hlis the output afterlthactivation layer.",
        "To illustrate the effect of the adjacent matrix, we deneOD\u000012QAOD\u000012asOA. The adjacent matrix OArepresent the inter-action intensity between each pair of node vectors. Thehigh element value OAijof the adjacent matrix means thatthe relationship between niandnjnode vectors is stronglycorrelated. In [30], the adjacent matrix OAis calculated beforethe convolutional operation. To consider the changes in therelationship between the node vectors in the GCN propaga-tion, we convert the adjacent matrix OAto be a trainable matrixin accordance with the following ruleVOA(ni;nj)D2(n i)T9(nj) (3)",
        "where niandnjdenotes the node vectors of the graph. 2(v i)",
        "and9(vj) are two trainable vectors calculated with weightsV2(v i)DW\u0012vi;9(vj)DW vj (4)",
        "In the reasoning stage, we use GCN to transmit the emo-tional information of nodes through the adjacent matrix.",
        "Finally, we introduce residual connections into each GCNlayerVGlC1DReLu(W r(OAGlWl)CGl) (5)",
        "where Wldenotes the parameters of the lthGCN layer andWris the weight of residual connection. We apply a ReLUactivation function after each layer of GCN. In this way,we realize four times such well-dened GCN operations onthe node vectors of the graph to extract spatial-temporal emo-tion features effectively. In particular, to recognize emotionsin a single image, we can set the number Tof video framesto 1.",
        "C. GRAPH INFORMATION FILTRATIONIn this section, we rene graph features and extract the dis-criminative features of the graph by removing the redundantfeatures to obtain thenal emotional features. In specic,we input the node vectors of graph Gone by one into theGRU layer to capture the long-term information and removethe redundant information. As shown in Figure 2, the GRUadaptively captures the emotion features through differentgates, such as update gate uijand reset gate rij. The updategateuijcontrols how much information is transferred from theprevious state to the hidden state. The reset gate rijeffectivelycontrols the hidden state to ignore the information from theprevious state which is irrelevant to the current state.",
        "The presentation of the updated memory hijis a linearinterpolation between the hidden state Ohijand the previousstate h(i\u00001)j=hT(j\u00001) based on the update gate uijasVhijD(uij\u000eOhijC(1\u0000uij)\u000eh(i\u00001)j;i6D1uij\u000eOhijC(1\u0000uij)\u000ehT(j\u00001);iD1(6)",
        "where\u000eis an element-wise multiplication. The update gateuij2[0;1] decides the degree of the unit updated, which isVOLUME 9, 2021 6491",
        "Q. Gao et al.: GRERNFIGURE 2. Architecture of gated recurrent unit.",
        "calculated byVuijD(\u001b(WunijCUuh(i\u00001)jCbu); i6D1\u001b(WunijCUuhT(j\u00001)Cbu);iD1(7)",
        "where\u001bdenotes a sigmoid activation function. Wu,Uuandbuindicate the weight and bias parameters of the GRU model.",
        "nijis the node vector of graph structure G.",
        "The hidden stateOhijis presented as follows, which is trainedto capture the relationship over different time states throughreset gate rij:",
        "OhijD(tanh(W hnijCUh(rij\u000eh(i\u00001)j)Cbh); i6D1tanh(W hnijCUh(rij\u000ehT(j\u00001))Cbh);iD1(8)",
        "where Wh,Uhandbhindicate the weight and bias parametersof the GRU model, and \u000eis an element-wise multiplica-tion. Moreover, the reset gate rijin (8) controls the degreeto forget the previous features based on the previous stateh(i\u00001)j=hT(j\u00001)and the current node vector nij, which is com-puted as followsVrijD(\u001b(WrnijCUrh(i\u00001)jCbr); i6D1\u001b(WrnijCUrhT(j\u00001)Cbr);iD1(9)",
        "where the calculation of rijis similar to that of the updategate uij. In addition, \u001b;Wr;Ur;andbrrespectively denote asigmoid activation function and the parameters of the GRUmodel.",
        "In the end of the sequence G, we regard the memory cellhTKas thenal feature vectors. As illustrated in Figure 1,the updated features after the GCN and GRU modules areforwarded into an SoftMax classier.",
        "PDSoftmax (WphTKCbp) (10)",
        "where Wpandbpare the weights and bias of a fully con-nected layer that compresses the input dimension of 2048 \u0002kinto only seven dimensions for subsequent emotion classi-cation. The loss function for optimization calculates thecross-entropy loss over all the nodes as followsVLD\u00006XiD0YilnPi (11)",
        "where Yis the emotion label indicator matrix.IV. EXPERIMENTSA. DATASETSOur method has been evaluated on two benchmark datasets,namely, CAER and AFEW. These datasets not only havemulti-angle and natural facial expressions but also retainthe surrounding context around the human face. For ourexperiments, the surrounding context of videos is particu-larly important. CAER has collected the 13,201 video clipsfrom 79 TV shows, which are manually labeled as sevenbasic categories: Angry, Disgust, Fear, Happy, Sad, Surprise,and Neutral. In addition to the video dataset, Lee et al. [14]",
        "extracted frames from the video clips to set up the static imagedataset named CAER-S, which contains about 70K staticimages. These two datasets are randomly split into a trainingset (70%), a validation set (10%), and a testing set (20%).",
        "To better validate the effect of spatial-temporal contextualfeatures for emotion recognition, we compared our methodwith the baseline of CAER and CAER-S. In addition, AFEWcontains about 1809 video clips from TV shows or movies.",
        "These video clips have been divided into a training set (773),a validation set (383), and a testing set (593). All video clipsare labeled with the same seven basic categories as those inthe CAER. We also evaluate our model on the AFEW.",
        "B. CHOOSING HYPERPARAMETERSIn the pre-training stage, the bottom-up attention model istrained with ResNet-101 as the backbone on the VisualGenomes dataset [38], which follows the same settings as[17], [39]. We train our model from scratch using RAdamoptimizer [40] with learning rates initialized as 1 \u000210\u00003and descended the learning rate with 0.1 every 8 epochs.",
        "In addition, we use a mini-batch size of 32. Considering thatCAER datasets have various video clip lengths, we randomlyextract 16 consecutive video frames from each video clips at asample rate of 10 frames per second. We further perform dataaugmentation operations to horizontallyip video frames.",
        "In our model, the number (T ,K) of video frames andregions are particularly signicant parameters. Differentparameters choices will affect the performance of our model.",
        "To achieve the best recognition performance, we carry outfurther experiments with different settings of video frames T6492 VOLUME 9, 2021",
        "Q. Gao et al.: GRERNFIGURE 3. The recognition accuracy of different video frames and nodenumbers.",
        "TABLE 1. Ablation studies on CAER dataset. Results are reported in termsof recognition accuracy.",
        "and numbers of region K. In these experiments, we select thevideo frames Tfrom [2, 3, 4, 5, 6] and the number of regionsKfrom [12, 24, 30, 36]. We analyze the emotion recognitionof these experiments in the CAER dataset. As shown in theFigure 3, the horizontal axis denotes the numbers of theregions from video frames and the vertical axis denotes theaccuracy of emotion recognition. In addition, we show thecurves of different video frames in the different colors. Thesedata indicate that the emotion recognition accuracy is thehighest when the video frame number Tis 3 and then thenode number Kis 30. Therefore, we set the video framenumber Tand node number Kto 3 and 30 in our experiments,respectively.",
        "C. ABLATION STUDIESTo evaluate our proposed methods quantitatively, we performablation studies to analyze each component in our frame-work. We propose a basic baseline model (noted as ``AveragePooling'') without any GCN and GRU layers. In the baselinemodel, we apply an average-pooling layer after the graphstructure transition and then input the features to the emotionclassier. The emotion classier is the same as the one usedin GRERN. In Table 1, the Average Pooling model achieves36.19% accuracy of emotion recognition. To demonstratethe capability of the GCN layer to reason the relationshipsbetween node vectors in our model, we adopt one GCN layeron the graph features to extract the emotional relationships ina similar way to the baseline model. This model is denoted as``GCN''. The role of GRU is also validated by establishinga model marked as ``GRU''. This model adopts one GRUlayer on the initial graph features without any GCN layersto capture the global emotion features. In summary, the GRUFIGURE 4. Confusion matrix of GRERN on the CAER-S and CAER datasets.",
        "and the GRU model both generate effective emotion featuresand improve the emotion recognition accuracy.",
        "In addition, we combine a GRU layer with different GCNlayers to further study the performance of our proposedmethod. We mark these models as ``1GCNCGRU'', :::,and ``5GCNCGRU'', which respectively integrates a GRUlayer with [1, 2, 3, 4, 5] GCN layers. Table 1shows that theemotion recognition performance of our model is graduallyimproved by applying multiple GCN layers before the GRUlayer. These results illustrate that the GCN module can extractthe enhanced emotion features by learning the relationshipsbetween node vectors and that the GRU module can capturethe discriminative spatiotemporal information by maintaininglong-term information and removing the redundant features.",
        "The emotion recognition accuracy becomes the best whenfour GCN layers are added into the model by maximiz-ing utilizing the spatiotemporal contextual information. Thebest recognition accuracy can be further improved by about1% in comparison with 1GCNCGRU. Finally, we choose4GCNCGRU as GRERN to compare with the SOTA meth-ods.",
        "Figure 4demonstrates the confusion matrix of GRERNon the CAER-S and CAER datasets to analyze the recogni-tion performance of our proposed method in each emotionVOLUME 9, 2021 6493",
        "Q. Gao et al.: GRERNFIGURE 5. Each category of GRERN compared with baseline methods in the CAER benchmark.",
        "category. The Neutral category has the lowest accuracy in theseven categories. As shown in the Figure 4, a large number ofNeutral categories are misclassied as Happy categories inthe CAER-S dataset. Moreover, when we perform GRERNon the CAER dataset, Happy and Neutral categories can beclearly distinguished. However, GRERN performs poorly inrecognizing the Disgust and Fear categories, which may beattributed to the similar emotion features of the two categoriesin dynamic movement.",
        "D. RESULTS OF EXPERIMENTSTo demonstrate the strength of our model in extracting thespatiotemporal information, we compare our model with sev-eral SOTA methods in the CAER and AFEW datasets. Thesemethods are shown as follows:",
        "1) Lee et al. [14] built a two-branch encoding framework.",
        "One branch achieved facial expression encoding toextract the emotional features of the human face. Theother branch implemented the contextual informationencoding to extract the surrounding context informa-tion around the human face. In the end, the modelutilized an adaptive fusion module to fuse the extractedemotional features after the two branches.",
        "2) Zhang et al. [20] established a network by integratingGCN and CNN. First, the network utilized the RegionProposal Network to convert images into node vectors.",
        "Then, those node vectors were fed into GCN to extractthe emotion relationships. The body features of imageswere captured with CNNs. Finally, the features fromGCN and CNN were connected to predict the emotioncategory.",
        "3) Fan et al. [21] developed an architecture basedon deeply supervised CNN. The model extractedmulti-level and multi-scale human face featuresthrough different convolutional layers. Thenalfeatures from each convolutional layer were used topredict the emotion label.",
        "We evaluated our model and above SOTA methods on theCAER and CAER-S benchmark. However, Zhang et al. [20]",
        "and Fan et al. [21] did not provide open-sourced imple-mentations. We have reproduced Zhang et al. [20] andFanet al. [21] on the CAER and CAER-S datasets and com-pared the obtained results on the same datasets.",
        "To quantitatively evaluate the importance of temporalinformation for emotion recognition, werst conduct someexperiments on the CAER-S dataset. CAER-S is an imagesubset of CAER, which only contains the spatial features.",
        "In Table 2, the top four rows shown the emotion recognitionaccuracy of SOTA methods on the CAER-S dataset, andthe bottom four rows illustrated the performance of SOTAmethods on the CAER dataset. As shown in Table 2, therecognition performance of these methods trained on theCAER dataset is better than that of the methods trainedon the CAER-S dataset. Therefore, the results demonstratethat the temporal information is benecial for the emotionrecognition.",
        "On the CAER dataset, GRERN has achieved the bestperformance with an emotion recognition accuracy of86.73%. Compared with the results of Lee et al. [14] andFanet al. [21], the emotion recognition performance of ourGRERN is improved by 9.69% and 6.01% respectively interms of accuracy. These results indicate that our modelextracts better spatiotemporal emotion features through GCNand GRU than other methods. Moreover, Zhang et al. [20]",
        "outperformed the method proposed in [21] with a recogni-tion accuracy of 81.66% through combining GCN and CNN.",
        "However, GRERN combined the GRU and GCN can highlyimprove the performance for emotion recognition by furtherrening the relationships of the node vectors.",
        "To further analyze the effectiveness of combining the GCNand GRU for emotion recognition on the CAER dataset,6494 VOLUME 9, 2021",
        "Q. Gao et al.: GRERNFIGURE 6. Visualization of heatmaps with the six basic emotions.",
        "TABLE 2. Comparisons of the emotion recognition accuracy of SOTAmethods on the CAER datasets.",
        "TABLE 3. Comparisons of the emotion recognition accuracy of SOTAmethods on the AFEW datasets.",
        "we visualize the recognition rate of each emotion categoryin all methods. As shown in Figure 5, the performanceimprovement of our model is mainly achieved by improvingthe recognition rates of the Happy, Sad, and Neutral cate-gories. This result means that GRERN can better capture thedynamic changes of these categories than the above state-of-the-art methods. However, GRERN does not perform well inthe recognition of the Fear category compared with the abovemethods. Table 4shows the number of data for each category.",
        "The number of Fear categories in the CAER dataset is theleast. Therefore, the bottleneck of GRERN for recognizingthe Fear category may be caused by the lack of training data.TABLE 4. Amount of video clips in each category on CAER dataset.",
        "We conduct more experiments on the AFEW dataset.",
        "The differnt methods are compared on the AFEW dataset.",
        "As shown in the Table 3, GRERN is robust in context-awareemotion recognition. However, the model of Fan et al. [21]",
        "performs better in emotion recognition when comparedwith GRERN only trained with the AFEW dataset. Theirmodel has been pre-trained on the Real-world AffectiveFace Database (RAF-DB) [41] which includes large facialexpression samples. To verify the robustness of GRERNto context-aware emotion recognition, we pretrain GRERNon the CAER dataset and thenne-tune our model on theAFEW dataset. Finally, the performance of GRERN hasbeen highly improved by 1.42% compared with the modelof Fan et al. [21].",
        "For visualizing the correlation betweennal features andsalient regions that include the facial expression and discrim-inative contextual information, we calculate the similarityscores between the node vectors GD fN 1;:::; NTgandthenal features hTKgenerated in the GRU through innerproduct operation. Then, we color these regions with differentweights according to their score ranking. Figure 6showsVOLUME 9, 2021 6495",
        "Q. Gao et al.: GRERNFIGURE 7. Attention visualization of video frames in CAER and AFEWdatasets.",
        "the heatmaps of each category in the AFEW and CAERdatasets. Therst and third rows are the each categories oforiginal video frames in the AFEW and CAER datasets. Thenext rows are the corresponding heatmaps. As shown in Fig-ure 6, GRERN can recognize the correct emotion categoriesthrough facial expressions and surrounding information evenin complex scenarios. In addition, we visualize the heatmapson multiple frames in a single video in the AFEW andCAER datasets to analyze the effectiveness of our methodsin extracting temporal features. In Figure 7, the top tworows are the input video frames and heatmaps in the AFEWdataset, and the bottom two rows are the input video framesand heatmaps in the CAER dataset. As shown in Figure 7,GRERN can capture the changes in faces and gestures intime dimension to extract temporal video information. Thus,GRERN captures not only the facial expressions, but also thechanges in body movements or gestures."
    ]
}
{
    "Abstract": "Messages in human conversations inherentlyconvey emotions. The task of detecting emo-tions in textual conversations leads to a widerange of applications such as opinion miningin social networks. However, enabling ma-chines to analyze emotions in conversations ischallenging, partly because humans often relyon the context and commonsense knowledgeto express emotions. In this paper, we addressthese challenges by proposing a Knowledge-Enriched Transformer (KET), where contex-tual utterances are interpreted using hierarchi-cal self-attention and external commonsenseknowledge is dynamically leveraged using acontext-aware affective graph attention mech-anism. Experiments on multiple textual con-versation datasets demonstrate that both con-text and commonsense knowledge are consis-tently beneficial to the emotion detection per-formance. In addition, the experimental resultsshow that our KET model outperforms thestate-of-the-art models on most of the testeddatasets in F1 score.",
    "Keywords": null,
    "Body": [
        "IntroductionEmotions are “generated states in humans that re-flect evaluative judgments of the environment, theself and other social agents” ( Hudlicka ,2011 ).",
        "Messages in human communications inherentlyconvey emotions. With the prevalence of socialmedia platforms such as Facebook Messenger, aswell as conversational agents such as AmazonAlexa, there is an emerging need for machines tounderstand human emotions in natural conversa-tions. This work addresses the task of detectingemotions (e.g., happy, sad, angry, etc.) in textualconversations, where the emotion of an utteranceis detected in the conversational context. Beingable to effectively detect emotions in conversa-tions leads to a wide range of applications rang-ing from opinion mining in social media platformsNo emotionsocialize What do you plan to do for your birthday?I want to have a picnic with my friends, Mum.How about a party at home? That way we can get together and celebrate it.OK, Mum. I'll invite my friends home. party  movie No emotionHappinessHappinessContextResponseFigure 1: An example conversation with annotated la-bels from the DailyDialog dataset ( Li et al. ,2017 ). Byreferring to the context, “it” in the third utterance islinked to “birthday” in the first utterance. By lever-aging an external knowledge base, the meaning of“friends” in the forth utterance is enriched by associ-ated knowledge entities, namely “socialize”, “party”,and “movie”. Thus, the implicit “happiness” emotionin the fourth utterance can be inferred more easily viaits enriched meaning.",
        "(Chatterjee et al. ,2019 ) to building emotion-awareconversational agents ( Zhou et al. ,2018a ).",
        "However, enabling machines to analyze emo-tions in human conversations is challenging, partlybecause humans often rely on the context andcommonsense knowledge to express emotions,which is difficult to be captured by machines. Fig-ure1shows an example conversation demonstrat-ing the importance of context and commonsenseknowledge in understanding conversations and de-tecting implicit emotions.",
        "There are several recent studies that model con-textual information to detect emotions in conver-sations. Poria et al. (2017 ) and Majumder et al.",
        "(2019 ) leveraged recurrent neural networks (RNN)",
        "to model the contextual utterances in sequence,where each utterance is represented by a featurevector extracted by convolutional neural networks(CNN) at an earlier stage. Similarly, Hazarikaet al. (2018a ,b) proposed to use extracted CNNarXiv:1909.10681v2  [cs.CL]  1 Oct 2019",
        "features in memory networks to model contextualutterances. However, these methods require sepa-rate feature extraction and tuning, which may notbe ideal for real-time applications. In addition, tothe best of our knowledge, no attempts have beenmade in the literature to incorporate commonsenseknowledge from external knowledge bases to de-tect emotions in textual conversations. Common-sense knowledge is fundamental to understand-ing conversations and generating appropriate re-sponses ( Zhou et al. ,2018b ).",
        "To this end, we propose a Knowledge-EnrichedTransformer (KET) to effectively incorporate con-textual information and external knowledge basesto address the aforementioned challenges. TheTransformer ( Vaswani et al. ,2017 ) has beenshown to be a powerful representation learningmodel in many NLP tasks such as machine trans-lation ( Vaswani et al. ,2017 ) and language under-standing ( Devlin et al. ,2018 ). The self-attention(Cheng et al. ,2016 ) and cross-attention ( Bah-danau et al. ,2014 ) modules in the Transformercapture the intra-sentence and inter-sentence cor-relations, respectively. The shorter path of in-formation flow in these two modules comparedto gated RNNs and CNNs allows KET to modelcontextual information more efficiently. In ad-dition, we propose a hierarchical self-attentionmechanism allowing KET to model the hierarchi-cal structure of conversations. Our model sepa-rates context and response into the encoder and de-coder, respectively, which is different from otherTransformer-based models, e.g., BERT ( Devlinet al. ,2018 ), which directly concatenate contextand response, and then train language models us-ing only the encoder part.",
        "Moreover, to exploit commonsense knowledge,we leverage external knowledge bases to facili-tate the understanding of each word in the utter-ances by referring to related knowledge entities.",
        "The referring process is dynamic and balancesbetween relatedness and affectiveness of the re-trieved knowledge entities using a context-awareaffective graph attention mechanism.",
        "In summary, our contributions are as follows:",
        "•For the first time, we apply the Transformerto analyze conversations and detect emotions.",
        "Our hierarchical self-attention and cross-attention modules allow our model to exploitcontextual information more efficiently thanexisting gated RNNs and CNNs.•We derive dynamic, context-aware, andemotion-related commonsense knowledgefrom external knowledge bases and emotionlexicons to facilitate the emotion detection inconversations.",
        "•We conduct extensive experiments demon-strating that both contextual information andcommonsense knowledge are beneficial tothe emotion detection performance. In addi-tion, our proposed KET model outperformsthe state-of-the-art models on most of thetested datasets across different domains.",
        "2 Related WorkEmotion Detection in Conversations: Earlystudies on emotion detection in conversations fo-cus on call center dialogs using lexicon-basedmethods and audio features ( Lee and Narayanan ,2005 ;Devillers and Vidrascu ,2006 ).Devillerset al. (2002 ) annotated and detected emotions incall center dialogs using unigram topic modelling.",
        "In recent years, there is an emerging research trendon emotion detection in conversational videos andmulti-turn Tweets using deep learning methods(Hazarika et al. ,2018b ,a;Zahiri and Choi ,2018 ;",
        "Chatterjee et al. ,2019 ;Zhong and Miao ,2019 ;Po-ria et al. ,2019 ).Poria et al. (2017 ) proposed a longshort-term memory network (LSTM) ( Hochreiterand Schmidhuber ,1997 ) based model to capturecontextual information for sentiment analysis inuser-generated videos. Majumder et al. (2019 )",
        "proposed the DialogueRNN model that uses threegated recurrent units (GRU) ( Cho et al. ,2014 ) tomodel the speaker, the context from the preced-ing utterances, and the emotions of the precedingutterances, respectively. They achieved the state-of-the-art performance on several conversationalvideo datasets.",
        "Knowledge Base in Conversations: Recentlythere is a growing number of studies on incorpo-rating knowledge base in generative conversationsystems, such as open-domain dialogue systems(Han et al. ,2015 ;Asghar et al. ,2018 ;Ghazvinine-jad et al. ,2018 ;Young et al. ,2018 ;Parthasarathiand Pineau ,2018 ;Liu et al. ,2018 ;Moghe et al. ,2018 ;Dinan et al. ,2019 ;Zhong et al. ,2019 ),task-oriented dialogue systems ( Madotto et al. ,2018 ;Wu et al. ,2019 ;He et al. ,2019 ) and ques-tion answering systems ( Kiddon et al. ,2016 ;Haoet al.,2017 ;Sun et al. ,2018 ;Mihaylov and Frank ,2018 ).Zhou et al. (2018b ) adopted structured",
        "knowledge graphs to enrich the interpretation ofinput sentences and help generate knowledge-aware responses using graph attentions. The graphattention in the knowledge interpreter ( Zhou et al. ,2018b ) is static and only related to the recognizedentity of interest. By contrast, our graph attentionmechanism is dynamic and selects context-awareknowledge entities that balances between related-ness and affectiveness.",
        "Emotion Detection in Text: There is a trendmoving from traditional machine learning meth-ods ( Pang et al. ,2002 ;Wang and Manning ,2012 ;",
        "Seyeditabari et al. ,2018 ) to deep learning methods(Abdul-Mageed and Ungar ,2017 ;Zhang et al. ,2018b ) for emotion detection in text. Khanpourand Caragea (2018 ) investigated the emotion de-tection from health-related posts in online healthcommunities using both deep learning featuresand lexicon-based features.",
        "Incorporating Knowledge in Sentiment Anal-ysis: Traditional lexicon-based methods detectemotions or sentiments from a piece of text basedon the emotions or sentiments of words or phrasesthat compose it ( Hu et al. ,2009 ;Taboada et al. ,2011 ;Bandhakavi et al. ,2017 ). Few studies in-vestigated the usage of knowledge bases in deeplearning methods. Kumar et al. (2018 ) proposed touse knowledge from WordNet ( Fellbaum ,2012 ) toenrich the text representations produced by LSTMand obtained improved performance.",
        "Transformer: The Transformer has been appliedto many NLP tasks due to its rich representa-tion and fast computation, e.g., document machinetranslation ( Zhang et al. ,2018a ), response match-ing in dialogue system ( Zhou et al. ,2018c ), lan-guage modelling ( Dai et al. ,2019 ) and understand-ing ( Radford et al. ,2018 ). A very recent work(Rik Koncel-Kedziorski and Hajishirzi ,2019 ) ex-tends the Transformer to graph inputs and proposea model for graph-to-text generation.",
        "3 Our Proposed KET ModelIn this section we present the task definition andour proposed KET model.",
        "3.1 Task DefinitionLet{Xij,Yij},i=1,. . . N,j =1,. . . N ibe a collec-tion of {utterance ,label }pairs in a given dialoguedataset, where Ndenotes the number of conversa-tions and Nidenotes the number of utterances intheith conversation. The objective of the task is tomaximize the following function:",
        "where Xij\u00001,. . . ,Xi1denote contextual utterancesand✓denotes the model parameters we want tooptimize.",
        "We limit the number of contextual utterances toM. Discarding early contextual utterances maycause information loss, but this loss is negligiblebecause they only contribute the least amount ofinformation ( Su et al. ,2018 ). This phenomenoncan be further observed in our model analysis re-garding context length (see Section 5.2). Similarto (Poria et al. ,2017 ), we clip and pad each utter-ance Xijto a fixed mnumber of tokens. The over-all architecture of our KET model is illustrated inFigure 2.",
        "3.2 Knowledge RetrievalWe use a commonsense knowledge base Con-ceptNet ( Speer et al. ,2017 ) and an emotion lex-icon NRC V AD ( Mohammad ,2018a ) as knowl-edge sources in our model.",
        "ConceptNet is a large-scale multilingual seman-tic graph that describes general human knowledgein natural language. The nodes in ConceptNetare concepts and the edges are relations. Eachhconcept1 ,relation ,concept2 itriplet is an asser-tion. Each assertion is associated with a confi-dence score. An example assertion is hfriends ,CausesDesire ,socialize iwith confidence score of3.46. Usually assertion confidence scores are inthe[1,10]interval. Currently, for English, Con-ceptNet comprises 5.9M assertions, 3.1M con-cepts and 38 relations.",
        "NRC V AD is a list of English words andtheir V AD scores, i.e., valence (negative-positive), arousal (calm-excited), and dominance(submissive-dominant) scores in the [0,1]",
        "interval. The V AD measure of emotion isculture-independent and widely adopted in Psy-chology ( Mehrabian ,1996 ). Currently NRC V ADcomprises around 20K words.",
        "In general, for each non-stopword token tinXij, we retrieve a connected knowledge graph g(t)",
        "comprising its immediate neighbors from Con-ceptNet. For each g(t), we remove concepts thatare stopwords or not in our vocabulary. We fur-ther remove concepts with confidence scores less",
        "Embedding Layer (Section 3.3)KBWord EmbeddingConcept EmbeddingKBWord EmbeddingConcept EmbeddingKBWord EmbeddingConcept Embedding. . .Dynamic Context-Aware Affective Graph AttentionWord EmbeddingConcept RepresentationWord EmbeddingConcept RepresentationWord EmbeddingConcept RepresentationMulti-Head Self-Attention & FFMulti-Head Self-Attention & FFMulti-Head Self-AttentionContextResponse. . .. . .Multi-Head Self-Attention & FFMulti-Head Cross-Attention & FFSoftmaxMax Pooling & LinearConcatenationConcept-Enriched EmbeddingHierarchical Self-Attention (Section 3.5)",
        "Knowledge Retrieval (Section 3.2)Dynamic Context-Aware Affective Graph Attention (Section 3.4)Context-Response Cross-Attention (Section 3.6)",
        "Figure 2: Overall architecture of our proposed KET model. The positional encoding, residual connection, andlayer normalization are omitted in the illustration for brevity.",
        "than 1 to reduce annotation noises. For each con-cept, we retrieve its V AD values from NRC V AD.",
        "The final knowledge representation for each to-ken tis a list of tuples: (c1,s1,VAD(c1)),(c2,s2,VAD(c2)), ...,(c|g(t)|,s|g(t)|,VAD(c|g(t)|)),where ck2g(t)denotes the kth connected con-cept, skdenotes the associated confidence score,andVAD(ck)denotes the V AD values of ck. Thetreatment for tokens that are not associated withany concept and concepts that are not included inNRC V AD are discussed in Section 3.4. We leavethe treatment on relations as future work.",
        "3.3 Embedding LayerWe use a word embedding layer to convert eachtoken tinXiinto a vector representation t2Rd,where ddenotes the size of word embedding. Toencode positional information, the position encod-ing (Vaswani et al. ,2017 ) is added as follows:",
        "t=Embed (t)+Pos(t). (2)",
        "Similarly, we use a concept embedding layer toconvert each concept cinto a vector representationc2Rdbut without position encoding.",
        "3.4 Dynamic Context-Aware Affective GraphAttentionTo enrich word embedding with concept represen-tations, we propose a dynamic context-aware af-fective graph attention mechanism to compute theconcept representation for each token. Specifi-cally, the concept representation c(t)2Rdfortoken tis computed asc(t)=|g(t)|Xk=1↵k⇤ck, (3)",
        "where ck2Rddenotes the concept embeddingofckand ↵kdenotes its attention weight. If|g(t)|=0, we set c(t) to the average of all con-cept embeddings. The attention ↵kin Equation 3is computed as↵k=softmax (wk), (4)",
        "where wkdenotes the weight of ck.",
        "The derivation of wkis crucial because it reg-ulates the contribution of cktowards enriching t.",
        "A standard graph attention mechanism ( Velikoviet al.,2018 ) computes wkby feeding tandckintoa single-layer feedforward neural network. How-ever, not all related concepts are equal in detect-ing emotions given the conversational context. Inour model, we make the assumption that importantconcepts are those that relate to the conversationalcontext and have strong emotion intensity. To thisend, we propose a context-aware affective graphattention mechanism by incorporating two factorswhen computing wk, namely relatedness and af-fectiveness.",
        "Relatedness: Relatedness measures the strengthof the relation between ckand the conversationalcontext. The relatedness factor in wkis computedasrelk=min-max (sk)⇤abs(cos( CR(Xi),ck)),(5)",
        "where skis the confidence score introduced inSection 3.2,min-max denotes min-max scaling foreach token t,absdenotes the absolute function,cos denotes the cosine similarity function, andCR(Xi)2Rddenotes the context representa-tion of the ith conversation Xi. Here we computeCR(Xi)as the average of all sentence represen-tations in Xias follows:",
        "where SR(Xij)2Rddenotes the sentence rep-resentation of Xij. We compute SR(Xij)via hi-erarchical pooling ( Shen et al. ,2018 ) where n-gram ( n3) representations in Xijare first com-puted by max-pooling and then all n-gram repre-sentations are averaged. The hierarchical poolingmechanism preserves word order information tocertain degree and has demonstrated superior per-formance than average pooling or max-pooling onsentiment analysis tasks ( Shen et al. ,2018 ).",
        "Affectiveness: Affectiveness measures the emo-tion intensity of ck. The affectiveness factor in wkis computed asaffk=min-max (||[V(ck)\u00001/2,A(ck)/2]||2),(7)",
        "where ||.||kdenotes lknorm, V(ck)2[0,1]andA(ck)2[0,1]denote the valence and arousal val-ues of VAD(ck), respectively. Intuitively, affkcon-siders the deviations of valence from neutral andthe level of arousal from calm. There is no es-tablished method in the literature to compute theemotion intensity based on V AD values, but em-pirically we found that our method correlates bet-ter with an emotion intensity lexicon comprising6K English words ( Mohammad ,2018b ) than othermethods such as taking dominance into consider-ation or taking l1norm. For concept cknot inNRC V AD, we set affkto the mid value of 0.5.",
        "Combining both relkandaffk, we define theweight wkas follows:",
        "where \u0000kis a model parameter balancing the im-pacts of relatedness and affectiveness on comput-ing concept representations. Parameter \u0000kcan befixed or learned during training. The analysis of\u0000kis discussed in Section 5.2.",
        "Finally, the concept-enriched word representa-tionˆtcan be obtained via a linear transformation:",
        "where [; ]denotes concatenation and W2Rd⇥2ddenotes a model parameter. All mtokens in eachXijthen form a concept-enriched utterance em-bedding ˆXij2Rm⇥d.",
        "3.5 Hierarchical Self-AttentionWe propose a hierarchical self-attention mecha-nism to exploit the structural representation ofconversations and learn a vector representationfor the contextual utterances Xij\u00001,. . . ,Xij\u0000M.",
        "Specifically, the hierarchical self-attention followstwo steps: 1) each utterance representation iscomputed using an utterance-level self-attentionlayer, and 2) a context representation is computedfrom Mlearned utterance representations using acontext-level self-attention layer.",
        "At step 1, for each utterance Xin,n=j\u00001, ...,j\u0000M, its representation ˆX0in2Rm⇥dis learnedas follows:",
        "ˆX0in=FF(L0(MH(L(ˆXin),L(ˆXin),L(ˆXin)))),(10)",
        "where L(ˆXin)2Rm⇥h⇥dsis linearly transformedfrom ˆXinto form hheads ( ds=d/h),L0linearlytransforms from hheads back to 1 head, andMH(Q, K, V )=softmax (QKTpds)V, (11)",
        "where Q,K, and Vdenote sets of queries, keysand values, respectively, W12Rd⇥p,b12Rp,W22Rp⇥dandb22Rddenote model pa-rameters, and pdenotes the hidden size of thepoint-wise feedforward layer (FF) ( Vaswani et al. ,2017 ). The multi-head self-attention layer (MH)",
        "enables our model to jointly attend to informationfrom different representation subspaces ( Vaswaniet al. ,2017 ). The scaling factor1pdsis added toensure the dot product of two vectors do not getoverly large. Similar to ( Vaswani et al. ,2017 ),both MH and FF layers are followed by resid-ual connection and layer normalization, which areomitted in Equation 10for brevity.",
        "Dataset Domain #Conv. (Train/Val/Test) #Utter. (Train/Val/Test) #Classes EvaluationEC Tweet 30160/2755/5509 90480/8265/16527 4 Micro-F1DailyDialog Daily Communication 11118/1000/1000 87170/8069/7740 7 Micro-F1MELD TV Show Scripts 1038/114/280 9989/1109/2610 7 Weighted-F1EmoryNLP TV Show Scripts 659/89/79 7551/954/984 7 Weighted-F1IEMOCAP Emotional Dialogues 100/20/31 4810/1000/1523 6 Weighted-F1Table 1: Dataset descriptions.",
        "At step 2, to effectively combine all utter-ance representations in the context, the context-level self-attention layer is proposed to hierarchi-cally learn the context-level representation Ci2RM⇥m⇥das follows:",
        "where ˆXidenotes [ˆX0ij\u0000M;...;ˆX0ij\u00001], which is theconcatenation of all learned utterance representa-tions in the context.",
        "3.6 Context-Response Cross-AttentionFinally, a context-aware concept-enriched re-sponse representation Ri2Rm⇥dfor conversa-tion Xiis learned by cross-attention ( Bahdanauet al. ,2014 ), which selectively attends to theconcept-enriched context representation as fol-lows:",
        "where the response utterance representation ˆX0ij2Rm⇥dis obtained via the MH layer:",
        "ˆX0ij=L0(MH(L(ˆXij),L(ˆXij),L(ˆXij))),(15)",
        "The resulted representation Ri2Rm⇥dis thenfed into a max-pooling layer to learn discrimina-tive features among the positions in the responseand derive the final representation O2Rd:",
        "O=max pool(Ri). (16)",
        "The output probability pis then computed asp=softmax (OW3+b3), (17)",
        "where W32Rd⇥qandb32Rqdenote modelparameters, and qdenotes the number of classes.",
        "The entire KET model is optimized in an end-to-end manner as defined in Equation 1. Our modelis available at here1.",
        "1https://github.com/zhongpeixiang/KET4 Experimental SettingsIn this section we present the datasets, evaluationmetrics, baselines, our model variants, and otherexperimental settings.",
        "4.1 Datasets and EvaluationsWe evaluate our model on the following five emo-tion detection datasets of various sizes and do-mains. The statistics are reported in Table 1.",
        "EC(Chatterjee et al. ,2019 ): Three-turn Tweets.",
        "The emotion labels include happiness, sadness,anger and other.",
        "DailyDialog (Li et al. ,2017 ): Human writtendaily communications. The emotion labels in-clude neutral and Ekman's six basic emotions ( Ek-man,1992 ), namely happiness, surprise, sadness,anger, disgust and fear.",
        "MELD (Poria et al. ,2018 ): TV show scripts col-lected from Friends . The emotion labels are thesame as the ones used in DailyDialog.",
        "EmoryNLP (Zahiri and Choi ,2018 ): TV showscripts collected from Friends as well. How-ever, its size and annotations are different fromMELD. The emotion labels include neutral, sad,mad, scared, powerful, peaceful, and joyful.",
        "IEMOCAP (Busso et al. ,2008 ): Emotional dia-logues. The emotion labels include neutral, happi-ness, sadness, anger, frustrated, and excited.",
        "In terms of the evaluation metric, for EC andDailyDialog, we follow ( Chatterjee et al. ,2019 ) touse the micro-averaged F1 excluding the majorityclass (neutral), due to their extremely unbalancedlabels (the percentage of the majority class in thetest set is over 80%). For the rest relatively bal-anced datasets, we follow ( Majumder et al. ,2019 )",
        "to use the weighted macro-F1.",
        "4.2 Baselines and Model VariantsFor a comprehensive performance evaluation, wecompare our model with the following baselines:",
        "cLSTM : A contextual LSTM model. Anutterance-level bidirectional LSTM is used to en-code each utterance. A context-level unidirec-tional LSTM is used to encode the context.",
        "Dataset M m d p hEC 2 30 200 100 4DailyDialog 6 30 300 400 4MELD 6 30 200 100 4EmoryNLP 6 30 100 200 4IEMOCAP 6 30 300 400 4Table 3: Hyper-parameter settings for KET. M: con-text length. m: number of tokens per utterance. d:",
        "word embedding size. p: hidden size in FF layer. h:",
        "number of heads.",
        "CNN (Kim,2014 ): A single-layer CNN withstrong empirical performance. This model istrained on the utterance-level without context.",
        "CNN+cLSTM (Poria et al. ,2017 ): An CNN isused to extract utterance features. An cLSTM isthen applied to learn context representations.",
        "BERT BASE (Devlin et al. ,2018 ): Base versionof the state-of-the-art model for sentiment classifi-cation. We treat each utterance with its context asa single document. We limit the document lengthto the last 100 tokens to allow larger batch size.",
        "We do not experiment with the large version ofBERT due to memory constraint of our GPU.",
        "DialogueRNN (Majumder et al. ,2019 ): The state-of-the-art model for emotion detection in textualconversations. It models both context and speak-ers information. The CNN features used in Dia-logueRNN are extracted from the carefully tunedCNN model. For datasets without speaker in-formation, i.e., EC and DailyDialog, we use twospeakers only. For MELD and EmoryNLP, whichhave 260 and 255 speakers, respectively, we addi-tionally experimented with clipping the number ofspeakers to the most frequent ones (6 main speak-ers + an universal speaker representing all otherspeakers) and reported the best results.",
        "KET SingleSelfAttn : We replace the hierarchi-cal self-attention by a single self-attention layerto learn context representations. Contextual utter-ances are concatenated together prior to the singleself-attention layer.KET StdAttn : We replace the dynamic context-aware affective graph attention by the standardgraph attention ( Velikovi et al. ,2018 ).",
        "4.3 Other Experimental SettingsWe preprocessed all datasets by lower-casing andtokenization using Spacy2. We keep all tokensin the vocabulary3. We use the released codefor BERT BASE and DialogueRNN. For eachdataset, all models are fine-tuned based on theirperformance on the validation set.",
        "For our model in all datasets, we use Adam opti-mization ( Kingma and Ba ,2014 ) with a batch sizeof 64 and learning rate of 0.0001 throughout thetraining process. We use GloVe embedding ( Pen-nington et al. ,2014 ) for initialization in the wordand concept embedding layers4. For the classweights in cross-entropy loss for each dataset, weset them as the ratio of the class distribution inthe validation set to the class distribution in thetraining set. Thus, we can alleviate the problem ofunbalanced dataset. The detailed hyper-parametersettings for KET are presented in Table 3.",
        "5 Result AnalysisIn this section we present model evaluation results,model analysis, and error analysis.",
        "5.1 Comparison with BaselinesWe compare the performance of KET against thatof the baseline models on the five afore-introduceddatasets. The results are reported in Table 2. Notethat our results for CNN, CNN+cLSTM and Di-alogueRNN on EC, MELD and IEMOCAP areslightly different from the reported results in ( Ma-jumder et al. ,2019 ;Poria et al. ,2019 ).",
        "2https://spacy.io/3We keep tokens with minimum frequency of 2 for Daily-Dialog due to its large vocabulary size4We use GloVe embeddings from Magnitude Medium:",
        "https://github.com/plasticityai/magnitude",
        "cLSTM performs reasonably well on shortconversations (i.e., EC and DailyDialog), butthe worst on long conversations (i.e., MELD,EmoryNLP and IEMOCAP). One major reason isthat learning long dependencies using gated RNNsmay not be effective enough because the gradi-ents are expected to propagate back through in-evitably a huge number of utterances and tokensin sequence, which easily leads to the vanishinggradient problem ( Bengio et al. ,1994 ). In con-trast, when the utterance-level LSTM in cLSTMis replaced by features extracted by CNN, i.e.,the CNN+cLSTM, the model performs signifi-cantly better than cLSTM on long conversations,which further validates that modelling long con-versations using only RNN models may not besufficient. BERT BASE achieves very competi-tive performance on all datasets except EC due toits strong representational power via bi-directionalcontext modelling using the Transformer. Notethat BERT BASE has considerably more param-eters than other baselines and our model (110Mfor BERT BASE versus 4M for our model), whichcan be a disadvantage when deployed to deviceswith limited computing power and memory. Thestate-of-the-art DialogueRNN model performs thebest overall among all baselines. In particular,DialogueRNN performs better than our model onIEMOCAP, which may be attributed to its detailedspeaker information for modelling the emotion dy-namics in each speaker as the conversation flows.",
        "It is encouraging to see that our KET modeloutperforms the baselines on most of the datasetstested. This finding indicates that our model is ro-bust across datasets with varying training sizes,context lengths and domains. Our KET vari-ants KET SingleSelfAttn and KET StdAttn per-form comparably with the best baselines on alldatasets except IEMOCAP. However, both vari-ants perform noticeably worse than KET on alldatasets except EC, validating the importanceof our proposed hierarchical self-attention anddynamic context-aware affective graph attentionmechanism. One observation worth mentioningis that these two variants perform on a par withthe KET model on EC. Possible explanations arethat 1) hierarchical self-attention may not be crit-ical for modelling short conversations in EC, and2) the informal linguistic styles of Tweets in EC,e.g., misspelled words and slangs, hinder the con-text representation learning in our graph attentionmechanism.",
        "5.2 Model AnalysisWe analyze the impact of different settings on thevalidation performance of KET. All results in thissection are averaged over 5 random seeds.",
        "Analysis of context length: We vary the contextlength Mand plot model performance in Figure 3(top portion). Note that EC has only a maximumnumber of 2 contextual utterances. It is clear thatincorporating context into KET improves perfor-mance on all datasets. However, adding more con-text is contributing diminishing performance gainor even making negative impact in some datasets.",
        "This phenomenon has been observed in a priorstudy ( Su et al. ,2018 ). One possible explanationis that incorporating long contextual informationmay introduce additional noises, e.g., polysemesexpressing different meanings in different utter-ances of the same context. More thorough investi-gation of this diminishing return phenomenon is aworthwhile direction in the future.",
        "Analysis of the size of ConceptNet: We vary thesize of ConceptNet by randomly keeping only afraction of the concepts in ConceptNet when train-",
        "Dataset 0 0.3 0.7 1EC 0.7345 0.7397 0.7426 0.7363DailyDialog 0.5365 0.5432 0.5451 0.5383MELD 0.5321 0.5395 0.5366 0.5306EmoryNLP 0.3528 0.3624 0.3571 0.3488IEMOCAP 0.5344 0.5367 0.5314 0.5251Table 4: Analysis of the relatedness-affectivenesstradeoff on the validation sets. Each column corre-sponds to a fixed \u0000kfor all concepts (see Equation 8).",
        "Dataset KET -context -knowledgeEC 0.7451 0.7343 0.7359DailyDialog 0.5544 0.5282 0.5402MELD 0.5401 0.5177 0.5248EmoryNLP 0.3712 0.3564 0.3553IEMOCAP 0.5389 0.4976 0.5217Table 5: Ablation study for KET on the validation sets.",
        "ing and evaluating our model. The results are il-lustrated in Figure 3(bottom portion). Addingmore concepts consistently improves model per-formance before reaching a plateau, validating theimportance of commonsense knowledge in detect-ing emotions. We may expect the performance ofour KET model to improve with the growing sizeof ConceptNet in the future.",
        "Analysis of the relatedness-affectiveness trade-off:We experiment with different values of \u0000k2[0,1](see Equation 8) for all kand report the re-sults in Table 4. It is clear that \u0000kmakes a notice-able impact on the model performance. Discard-ing relatedness or affectiveness completely willcause significant performance drop on all datasets,with one exception of IEMOCAP. One possiblereason is that conversations in IEMOCAP areemotional dialogues, therefore, the affectivenessfactor in our proposed graph attention mechanismcan provide more discriminative power.",
        "Ablation Study: We conduct ablation study to in-vestigate the contribution of context and knowl-edge as reported in Table 5. It is clear that bothcontext and knowledge are essential to the strongperformance of KET on all datasets. Note that re-moving context has a greater impact on long con-versations than short conversations, which is ex-pected because more contextual information is lostin long conversations.",
        "5.3 Error AnalysisDespite the strong performance of our model, itstill fails to detect certain emotions on certaindatasets. We rank the F1 score of each emotionper dataset and investigate the emotions with theworst scores. We found that disgust and fear aregenerally difficult to detect and differentiate. Forexample, the F1 score of fear emotion in MELD isas low as 0.0667. One possible cause is that thesetwo emotions are intrinsically similar. The V ADvalues of both emotions have low valence, higharousal and low dominance ( Mehrabian ,1996 ).",
        "Another cause is the small amount of data avail-able for these two emotions. How to differentiateintrinsically similar emotions and how to effec-tively detect emotions using limited data are twochallenging directions in this field.",
        "6 ConclusionWe present a knowledge-enriched transformer todetect emotions in textual conversations. Ourmodel learns structured conversation represen-tations via hierarchical self-attention and dy-namically refers to external, context-aware, andemotion-related knowledge entities from knowl-edge bases. Experimental analysis demonstratesthat both contextual information and common-sense knowledge are beneficial to model perfor-mance. The tradeoff between relatedness and af-fectiveness plays an important role as well. In ad-dition, our model outperforms the state-of-the-artmodels on most of the tested datasets of varyingsizes and domains.",
        "Given that there are similar emotion lexiconsto NRC V AD in other languages and ConceptNetis a multilingual knowledge base, our model canbe easily adapted to other languages. In addition,given that NRC V AD is the only emotion-specificcomponent, our model can be adapted as a genericmodel for conversation analysis.",
        "AcknowledgmentsThe authors would like to thank the anonymousreviewers for their valuable comments. This re-search is supported, in part, by the National Re-search Foundation, Prime Ministers Office, Singa-pore under its AI Singapore Programme (AwardNumber: AISG-GC-2019-003) and under its NRFInvestigatorship Programme (NRFI Award No."
    ]
}
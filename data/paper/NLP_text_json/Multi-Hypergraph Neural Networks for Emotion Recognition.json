{
    "Abstract": ": Emotion recognition in multi-party conversations (ERMC) is becoming increasingly popu-lar as an emerging research topic in natural language processing. Recently, many approaches havebeen devoted to exploiting inter-dependency and self-dependency among participants. However,these approaches remain inadequate in terms of inter-dependency due to the fact that the effectsamong speakers are not individually captured. In this paper, we design two hypergraphs to dealwith inter-dependency and self-dependency, respectively. To this end, we design a multi-hypergraphneural network for ERMC. In particular, we combine average aggregation and attention aggregationto generate hyperedge features, which can allow utterance information to be better utilized. Theexperimental results show that our method outperforms multiple baselines, indicating that furtherexploitation of inter-dependency is of great value for ERMC. In addition, we also achieved goodresults on the emotional shift issue.Keywords: emotional shift; emotion recognition in conversations; emotion recognition in multi-partyconversations1",
    "Keywords": ": emotional shift; emotion recognition in conversations; emotion recognition in multi-partyconversations1",
    "Body": [
        "IntroductionEmotion recognition in conversations (ERC) has attracted more and more attentionbecause of the prevalence of dialogue behavior in various fields. The primary purposeof ERC is to recognize the emotion of each utterance in the dialogue. The recognizedemotion can be used for opinion mining on social media, such as Facebook and Instagram,building conversational assistants, and conducting medical psychoanalysis [ 1–4]. However,ERC, especially emotion recognition in multi-party conversations (ERMC), often exhibitsmore difficulties than traditional text sentiment analysis due to the emotional dynamicsof dialogue [ 4]. There are two kinds of emotional dependencies among the participantsin a dialogue—inter-dependency and self-dependency. Self-dependency is the influenceof what the speaker says on the current utterance. Inter-dependency is the influence ofwhat others say on what the current speaker says. Therefore, identifying the emotion of anutterance in a multi-party dialogue depends not only on the utterance itself and its context,but also on the speaker's self-dependence and the inter-dependency [5,6].",
        "Existing work on emotion recognition in conversations can be roughly divided intotwo categories: that based on recurrent neural networks and that based on graph neuralnetworks. Some recent works based on recurrent neural networks [ 3,7–13] began to focuson conversational context modeling and speaker-specific modeling, and some works [ 14]",
        "have even carried out multi-task learning for speaker-specific modeling on this basis.",
        "They tried to deal with speaker-dependent influences through speaker-specific modelingand conversational context modeling, but they could use other speakers' utterances toinfluence the current utterance well. Meanwhile, some works based on graph neuralnetworks [5,6,15–18] have used relational graph convolutional networks (RGCNs) [19] toAppl. Sci. 2023 ,13, 1660. https://doi.org/10.3390/app13031660 https://www.mdpi.com/journal/applsci",
        "Appl. Sci. 2023 ,13, 1660 2 of 16distinguish among different speaker dependencies, and some have even used conversa-tional discourse structure [ 6] or commonsense knowledge [ 18] to extend relationshipsamong utterances. These models are intended to establish more perfect utterance relationsand then aggregate according to the relations to form the influence of the surrounding utter-ances on the current utterance. However, the performance of such models is affected by thetype and quantity of inter-utterance relations. Moreover, an emotional change in a speakermay be caused by the joint influence of multiple utterances of multiple speakers. Thisinfluence may also be caused by the interactions of utterances in different relationships. So,inter-dependency is more complex than self-dependency. We believe that it is necessary tobuild a graph network alone to model inter-dependency, especially for multi-dialogue, andthis can allow better identification of the problem of emotional shifts between consecutiveutterances of the same speaker.",
        "Conventional graph neural networks focus on pairwise relationships between objectsin the constructed graphs. In many real-world scenarios, however, relationships amongobjects are not dyadic (pairwise), but rather triadic, tetradic, or higher. Squeezing thehigh-order relations into pairwise ones leads to information loss and impedes expressive-ness [ 20]. So, we used a hypergraph neural network [ 21] to deal with two kinds of speakerdependencies instead of using a conventional graph neural network. According to thehypergraph [ 21] structure, we know that a hyperedge may contain multiple utterances,and an utterance may belong to multiple hyperedges. We let each utterance generate ahyperedge. The nodes on the hyperedge are the corresponding current utterance and thespecific context of the current utterance. Hypergraph neural networks [ 21] can use thestructure of a hypergraph to deal with the influences of multiple utterances from multiplespeakers on an utterance, that is, they use multiple surrounding utterances to producean influence on the current utterance. By performing a node–edge–node transformation,the underlying data relationship can be better represented [ 21], and more complex andhigh-level relationships can be established among nodes [ 22]. Previous work has shownthat speaker-specific information is very important for ERMC [ 3,6]. Therefore, the wayof using hypergraphs for speaker-specific modeling of ERMC is a very important issue.",
        "Second, the current utterance may be influenced by utterances from different speakers.",
        "Therefore, the way of using hypergraphs for non-speaker-specific modeling of ERMC isalso a very important issue.",
        "In this paper, we construct two hypergraphs for speaker-specific and non-speaker-specific modeling, respectively. The hyperedges in the two hypergraphs are different.",
        "The hypergraph for speaker-specific modeling, where the nodes on the hyperedge are fromthe speaker of the current utterance, mainly deals with self-dependency. The hypergraph fornon-speaker-specific modeling, where nodes on a hyperedge contain the current utteranceand utterances from other speakers, is primarily used to handle inter-dependency. InFigure 1, we construct two kinds of hyperedges for the third utterance. The hyperedge ofthe green triangle indicates that the node of the hyperedge is from speaker B of the thirdutterance. The hyperedge of the blue triangle indicates that the nodes of the hyperedge arefrom speakers other than speaker B. Note that this hyperedge needs to contain the currentutterance so that the nodes within the hyperedge have an effect on the current utterance. Weuse the location information and node features to aggregate to generate hyperedge features.",
        "Here, we use the location information to obtain the weight of the average aggregation, usethe node features to perform the attention aggregation to obtain the attention weight, andcombine the two weights to obtain the hyperedge features. Then, the hyperedge featuresare used to model the conversational context by using a recurrent neural network. Finally,the hyperedge features are used to aggregate to obtain new node features. The hypergraphconvolution of the two hypergraphs can be used to model specific speakers and non-specificspeakers so as to deal with inter-dependency and self-dependency among participants.",
        "The main contributions of this work are summarized as follows:",
        "• We construct hypergraphs for two different dependencies among participants anddesign a multi-hypergraph neural network for emotion recognition in multi-party",
        "Appl. Sci. 2023 ,13, 1660 3 of 16conversations. To the best of our knowledge, this is the first attempt to build graphsfor inter-dependency alone.",
        "• We combine average aggregation and attention aggregation to generate hyperedgefeatures that can allow better utilization of the information of utterances.",
        "• We conducted experiments on two public benchmark datasets. The results consistentlydemonstrate the effectiveness and superiority of the proposed model. In addition, weachieved good results on the emotional shift issue.",
        "3: Let's grab our stuff and get the hell out of here. [ Neutral] -- A4: I'm sorry we can't store your childhood things anymore. [ Sadness ] -- B5: Oh, I can't wait to see everything again! All of the memories. [ Joy] -- C6: Well, I don't know what's in the boxes down here, but I do know there aresix or seven. [ Neutral ] -- B7: I used to love to play restaurant. [ Neutral] -- C 2: Well, it's time for a new family to start their memories here and hopefullytheir check will clear before they find the crack in the foundation and theasbestos in the ceiling. [ Neutral] -- B 1: Dad, we can't believe you're selling the house. [Sadness ] -- A2365741Figure 1. Conversation as a hypergraph. Circles and triangles represent nodes and hyperedges,respectively. A. B, C are participants in the conversation.",
        "2. Related Work2.1. Emotion Recognition in ConversationsIn the following paragraphs, we divide the related works into two categories accordingto their methods to model a conversation's context. Note that, here, we regard the networkby using Transformer [ 23] without actually building a graph as a model based on a recurrentneural network. DialogXL [ 24], BERT+MTL [ 14], and ERMC-DisGCN [ 6] have been usedfor some research on emotion recognition in multi-party conversations.",
        "Recurrence-Based Models. ICON [ 8] separately models each speaker's historicalutterances through GRU [ 25] and uses an additional GRU to model the impacts betweenspeakers. DialogueRNN [ 3] uses three GRUs to model the speaker, the context given bythe preceding utterances, and the emotion behind the preceding utterances, respectively.",
        "COSMIC [ 13], which was built on DialogueRNN, uses commonsense knowledge (CSK)",
        "to learn interactions among participating interlocutors. BiERU [ 10] involved the designof a generalized neural tensor block (GNTB) to generate contextual utterance vectors bytaking the context and current utterance as inputs, and then extracting features from thecontextual utterance vector by using a two-channel model (LSTM [ 26] and CNN [ 27]).",
        "EmoCaps [ 28] introduced the concept of emotion vectors into multi-modal emotion recog-nition and involved the proposal of a new emotion feature extraction structure, Emoformer.",
        "BERT+MTL [ 14] exploits speaker identification as an auxiliary task to enhance the rep-resentation of utterances in conversations. DialogueCRN [ 12] is used to understand theconversational context from a cognitive perspective and integrates emotional cues througha multi-turn reasoning module for classification. VAE-ERC [ 29] models the context-awarelatent utterance role with a latent variable to overcome the lack of utterance role annotationin ERC datasets. TODKAT [ 30] proposes a new model in which the transformer modelfuses topical knowledge and CSK to predict the emotion label. DialogXL [ 24] improvesXLNet [ 31] with enhanced memory and dialog-aware self-attention. CoG-BART [ 32] uti-lizes supervised contrastive learning in ERC and incorporates response generation as anauxiliary task when certain contextual information is involved.",
        "Graph-Based Models. DialogueGCN [ 5] used a context window to connect the currentutterance with surrounding utterances and treated each dialogue as a graph. RGAT [ 15]",
        "Appl. Sci. 2023 ,13, 1660 4 of 16used BERT [ 33] to obtain contextual information and proposed relational position encodingsto use the location information of utterances. ConGCN [ 34] regarded both speakers andutterances as graph nodes and the whole dataset as one graph. DAG-ERC [ 17] involvedthe design of a directed acyclic graph neural network and provided a method for modelingthe information flow between the remote context and local context. SKAIG [ 18] utilizedCSK to enrich edges with knowledge representations and process a graph structure witha graph transformer. MMGCN [ 35] was the proposal of a new model based on a multi-modal fused graph convolutional network. TUCORE-GCN [ 36] proposed a context-awaregraph convolutional network model by focusing on how people understand conversations.",
        "ERMC-DisGCN [ 6] involved the design of a relational convolution to leverage the speakerself-dependency of interlocutors to propagate contextual information and the proposal ofan utterance-aware graph neural network.",
        "2.2. Hypergraph Neural NetworksUnlike conventional graph neural networks, hypergraph neural networks no longerfocus on only pairwise interactions between nodes. As shown in Figure 1, the relationshipbetween hyperedges and nodes is a many-to-many relationship.",
        "Zhou et al. [ 37] were the first to introduce hypergraphs in order to represent compli-cated relationships, and they proved that hypergraph-based learning outperformed graph-based learning on several clustering, embedding, and classification tasks. HGNN [ 21]",
        "was the proposal of a hypergraph neural network framework, and its ability to modelcomplex high-order data dependencies through hypergraph structures was demonstrated.",
        "HyperGAT [ 38] used subject words to construct hypergraphs for text classification. HGC-RNN [ 22] adopted a recurrent neural network structure to learn temporal dependenciesfrom data sequences and performed hypergraph convolution operations to extract hid-den representations of data. HWNN [ 20] was the proposal of a graph-neural-network-based representation learning framework for heterogeneous hypergraphs, an extensionof conventional graphs, which could characterize multiple non-pairwise relations well.",
        "SHARE [ 39] constructed different hyperedges through sliding windows of different sizesand extracted user intent through hypergraph attention for session-based recommendersystems. MHGNN [ 40] used multi-hypergraph neural networks to explore the latentcorrelations among multiple physiological signals and the relationships among differentsubjects. HOTL [ 41] was the proposal of a new online cross-topic ECG emotion recognitionmethod that used online transfer learning based on hypergraphs and effectively handledthe online cross-subject scenario in which unknown target ECG data arrived one by onewith varying overtime. Shao et al. [ 42] used hypergraphs of each modality to representthe complex relationships among subjects and used multimodal physiological signals foremotion recognition through an edge-weighted hypergraph neural network.",
        "To the best of our knowledge, there is currently no work on building graph networksby using non-specific speaker contexts alone or on dealing with inter-dependency amongspeakers by using hypergraphs. In order to better capture the high-order relationshipsamong utterances and model the two speaker dependencies, we treat dialogue as a hyper-graph and solve the ERC task by using a hypergraph neural network.",
        "3. Methodology3.1. Hypergraph DefinitionA hypergraph is defined as HG= (V,E), where V=fv1,v2,. . .,vNgis a node set,and E=fHE 1,HE 2,. . .,HE Ngis a collection of hyperedges. A hyperedge HE mis a subsetof the node set V, that is, the node set belonging to hyperedge HE mis a subset of V. Thestructure of a hypergraph HG can also be represented by an incidence matrix A, withentries defined as in Equation (1):",
        "Aij=(0,vi/2HE j,1,vi2HE j(1)",
        "Appl. Sci. 2023 ,13, 1660 5 of 16We use X=fx1,x2,. . .,xNgto denote the attribute vector of nodes in the hypergraph.",
        "So, the hypergraph can also be represented by HG= (A,X). In this paper, we use matrixM to store the relative position weight of an utterance in the hypergraph. The structureof matrix M is similar to that of the incidence matrix A. Each row in M corresponds toa hyperedge, and the non-zero items in each row represent the utterance node in thishyperedge. The size of the non-zero items is related to the positions between nodes in thehyperedge. In the following, we use HG= (M,X)to represent the hypergraph.",
        "Vertices. Each utterance in a conversation is represented as a node vi2V. Each nodeviis initialized with the utterance embeddings hi. We update the embedding representa-tions of vertices via hypergraph convolution.",
        "Hyperedge. Since each hyperedge is generated based on a specific current utterance,we need to calculate the influences of other utterances on the current utterance, and theseinfluences will be weakened according to the relative position between the utterances.",
        "We set the position weight of the current utterance to 1, and the position weight of theremaining utterances gradually decreases with the relative distance. See Algorithm 1 forthe specific process of hypergraph and hyperedge construction.",
        "Algorithm 1 Constructing a HypergraphInput: the dialoguefh1,h2, . . . , hNg, speaker identity p(\u0001), and context window w.",
        "Output: SSHG ,NSHG .",
        "1:X f h1,h2, . . . , hNg2:MSSHG ,MNSHG Æ,Æ3:for all i2[1,N]do4: MiSSHG,MiNSHG f 0, 0, . . . , 0g,f0, 0, . . . , 0g// N zero in total5: wp,wf, count i\u0000w,i+w, 0 // wp,wf2[1,N]",
        "6: MiNSHG[i] 17: forj=wp;j<=wf;j+ + do8: ifp(hi) =p(hj)then9: MiSSHG[j] 1/(1+abs(i\u0000j))",
        "10: count++11: else if p(hi)!=p(hj)and count =0then12: MiNSHG[j] 1/(1+abs(i\u0000j))",
        "13: end if14: end for15:end for16:SSHG (MSSHG ,X)",
        "17:NSHG (MNSHG ,X)",
        "18:return SSHG ,NSHGWe designed two kinds of hypergraphs—one is speaker-specific hypergraph (SSHG),and the other is a non-speaker-specific hypergraph (NSHG). The hyperedges in the SSHGare speaker-specific hyperedges (SSHEs). We selected some utterances in the contextwindow to add to the SSHEs, and the speaker of these utterances was the same as thespeaker of the current utterance. The hyperedges in the NSHG were non-speaker-specifichyperedges (NSHEs). We take the past utterance of the speaker of the current utteranceas a selective constraint and selected some utterances in the context window to add to theNSHEs. The speakers of these utterances were different from the speaker of the currentutterance.",
        "3.2. Problem DefinitionGiven the conversation record and the speaker information for each utterance, theERC task is that of identifying the emotional label of each utterance. More specifically, aninput sequence containing N utterances fu1,u2,. . .,uNgis given, and it is annotated with a",
        "Appl. Sci. 2023 ,13, 1660 6 of 16sequence of emotion labels fy1,y2,. . .,yNg. Each utterance uiis spoken by p(ui). The taskof ERC aims the prediction of the emotion label yifor each utterance ui.",
        "3.3. ModelAn overview of our proposed model is shown in Figure 2, which consists of a featureextraction module, a hypergraph convolution layer module, and an emotion classificationmodule. Hyperedges are generated according to the third, fourth, and fifth utterances.",
        "u1u2u3u4u5u6u7 h10h20h30h40h50h60h70Hypergraph Convolution Layer Feature Extractiony1y2y3y4y5y6y7 Emotion ClassificationLayer 1 h11h12…",
        "Speaker A Speaker BSpeaker C HyperedgeHypergraph ConcatenationFigure 2. Overview of our proposed model. In the hypergraph convolutional layer module, the reddotted line represents the information transfer between the hyperedges.",
        "3.3.1. Utterance Feature ExtractionFollowing COSMIC [ 13], we employed RoBERTa-Large [ 43] as a feature extractor. Thepre-trained model was first fine-tuned on each ERC dataset, and its parameters were thenfrozen while training our model. More specifically, a special token [CLS] was appended atthe beginning of the utterance to create the input sequence for the model. Then, we usedthe [CLS]'s pooled embedding at the last layer as the feature representation hiofui.",
        "3.3.2. Hypergraph Convolution (HGC) LayerWe utilized the two hypergraphs to perform separate hypergraph convolutions, andthen obtained different utterance representations. The process of performing hypergraphconvolution for each graph can be divided into the following three steps.",
        "Node-to-Edge Aggregation. The first step was the aggregation from the nodes tothe hyperedges. Here, we used the position weight mijto calculate the weight aposjiof theweighted average aggregation. Since some nodes on a hyperedge are informative, but othersmay not be, we should pay varying attention to the information from these nodes whileaggregating them together. We utilized an attention mechanism to model the significanceof different nodes. Here, we used a function S(\u0001,\u0001)to calculate the attention weights aATTji.",
        "Function S(\u0001,\u0001)was derived from the scaled dot-product attention formula [ 23]. Then, the",
        "Appl. Sci. 2023 ,13, 1660 7 of 16obtained weight aposji, attention weight aATTji, and node information hl\u00001iwere aggregatedto obtain the hyperedge feature flj. The specific process is shown in Equations (2)–(5).",
        "aposji=mijåkjvk2HE jmkj(2)",
        "aattji=S(W1hl\u00001i,tl)",
        "åfjvf2HE jS(W1hl\u00001f,tl)(3)",
        "flj=s(åvi2HE japosjiaattjiW2hl\u00001i) (4)",
        "S(a,b) =S(aTb)pD(5)",
        "where HE jis the j-th hyperedge resulting from the j-th utterance. mijis stored in theassociation matrix M, which represents the size of the position weight of the i-th node in thej-th hyperedge. hl\u00001irepresents the features of the utterance node. tlrepresents a trainablenode-level context vector for the l-th HGC layer. W1and W2is a trainable parameter matrix.",
        "D is the dimension size.",
        "Edge-to-Edge Aggregation. The second step was to transfer information betweenhyperedges. In order to make the current utterance have a better interaction with thecontext, we used the hyperedge generated by each utterance to model the conversationcontext. We used BiLSTM to complete the information transfer. The specific process isshown in Equation (6).",
        "qlj,hidden j= \u0000\u0000!LSTMc(flj,hidden j\u00001) (6)",
        "where hidden jis the j-th hidden state of the LSTM, and qljrepresents the hyperedge featureobtained after the information is passed by the hyperedge.",
        "Edge-to-Node Aggregation. To update the feature for a node, we needed to aggregatethe information from all of its connected hyperedges. We also used S(\u0001,\u0001)to calculate thesimilarity between the node and hyperedge features. The specific process is shown inEquations (7) and (8).",
        "hli=s(åHE j2EibijW3qlj) (7)",
        "bij=S(W4qlj,W1hl\u00001i)",
        "åHE p2EiS(W4qlp,W5hl\u00001i)(8)",
        "where Eiis the set of hyperedges containing the i-th node. W3,W4, and W5denote trainableparameters, and S(\u0001,\u0001)is the same as in Equation (5).",
        "3.4. ClassifierWe concatenated the hidden states of the two hypergraphs in all HGC layers andpassed them through a feedforward neural network to obtain the predicted emotion. Thespecific process is shown in Equations (9)–(11).",
        "HHGi=kLHGl=1(hHG)li (9)",
        "Pi=so f tmax (Wsmax[h0i:HSSHGi:HNSHGi ] +bsmax) (10)",
        "ˆyi=argmax k(Pi[k]) (11)",
        "Appl. Sci. 2023 ,13, 1660 8 of 16where HHGirepresents the result of the hypergraph convolution performed on the hyper-graph, HGcan be SSHG and NSHG, and LHGis the number of layers for the hypergraphconvolution of the corresponding hypergraph.",
        "For the training of ERMC-MHGNN, we employed the standard cross-entropy loss asan objective function. The specific function is shown in Equation (12).",
        "L(q) =\u0000Cåi=1Niåt=1LogPi,t[yi,t] (12)",
        "where Cis the number of training conversations, Niis the number of utterances in the i-thconversation, yi,tis the ground-truth label, and qis the collection of trainable parameters ofERMC-MHGNN.",
        "4. Experimental Setting4.1. DatasetsWe evaluated our model on two ERC datasets. Their statistics are shown in Table 1.",
        "They were all multimodal datasets, but our task mainly focused on textual modality forconducting our experiments.",
        "MELD [44] was derived from the Friends TV series. The utterances were annotatedwith one of seven labels, namely, neutral, joy, surprise, sadness, anger, disgust, and fear. Thedataset consisted of multi-party conversations and involved too many plot backgrounds.",
        "Non-neutral emotions accounted for 53%.",
        "EmoryNLP [45] was also collected from the Friends TV scripts, but differed fromMELD in the choice of scenes and emotion labels. The emotion labels included neutral, sad,mad, scared, powerful, peaceful, and joyful. Non-neutral emotions accounted for 73%.",
        "Table 1. The statistics of the datasets . avg_utt denotes the average number of utterances.",
        "MELD EmoryNLP#Dial. 1432 897Train 1038 713Dev. 114 99Test 280 85#Utt. 13,708 12,606Train 9989 9934Dev. 1109 1344Test 2610 1328avg_utt 9.57 14.05Classes 7 7Metrics Weighted-average F1 Weighted-average F14.2. Compared MethodsFor a comprehensive evaluation of our proposed ERMC-MHGNN, we compared itwith the following baseline methods.",
        "Recurrence-Based Models: DialogueRNN [ 3], COSMIC [ 13], DialogueCRN [ 12], TOD-KAT [ 30], DialogXL [ 24], VAE-ERC [ 29], DialogueRNN-RoBERTa [ 13], CoG-BART [ 32], andEmpCaps [28].",
        "Graph-Based Models: DialogueGCN [ 5], RGAT [ 15], RGAT-RoBERTa [ 17],DialogueGCN-RoBERTa [ 17], SKAIG [ 18], ERMC-DisGCN [ 6], MMGCN [ 35], TUCORE-GCN [36], and DAG-ERC [17].",
        "4.3. Implementation DetailsWe conducted the experiments on Windows10 while using an NVIDIA GeForce GTX1650 GPU with 4 GB of memory. We used PyTorch 1.7.0 and the CUDA toolkit 11.0. Weadopted AdamW [ 46] as the optimizer. Table 2 provides the hyperparameter settings. For",
        "Appl. Sci. 2023 ,13, 1660 9 of 16the feature dimension, the utterance feature dimension extracted by the RoBERTA extractorwas 1024, and after the linear layer, the utterance feature dimension became 100.",
        "Table 2. Hyperparameter settings.",
        "# Batch size Dropout Lr Window SSHG NSHGMELD 32 0.1 0.001 1 1 1EmoryNLP 16 0.4 0.0009 4 4 65. Results and Discussions5.1. Overall PerformanceTable 3 shows the performance of different models on the MELD and EmoryNLPtest sets. We can see that our model outperformed all baselines, which demonstrated theeffectiveness of our proposed model. At the same time, we found that the models usingCSK on MELD generally performed better, while our model achieved good results withoutrelying on external knowledge. In this paper, we focused on modeling the two kinds ofdependencies among speakers by building multiple hypergraphs, so we did not incorporateexternal knowledge. On the EmoryNLP dataset, we found that the models using large-scalepre-trained models to extract features had better results. For example, both DAG-ERC andTUCORE-GCN used RoBERTa as feature extractors. These models could achieve over 39%on EmoryNLP . Our model also used RoBERTa as a feature extractor and achieved relativelybetter results by separately modeling the two speaker dependencies. Compared withDialogXL, BERT+MTL, and ERMC-DisGCN, our model had at least a 2% improvement inthe two datasets. This also showed that our model could identify the emotions of utterancesbetter than the previous models studied on the multi-party conversation dataset.",
        "Table 3. Overall performance on the two datasets. '-' signifies that no results were reported forthe given dataset. 'CSK' stands for a model that introduced commonsense knowledge, 'p' and '\u0002'",
        "represent using 'CSK' and not using 'CSK' respectively. '*' represents the results of the model in thetext-only modality.",
        "Appl. Sci. 2023 ,13, 1660 10 of 165.2. Ablation StudyTo investigate the impacts of the various modules in the model, we evaluated ourmodel by separately removing two weights in the node-to-edge aggregation process inthe hypergraph convolution. In addition, we conducted experiments on the hypergraphconvolution with a single hypergraph. The results are shown in Table 4.",
        "As shown in Table 4, we can see that, after removing the weights aatt, there was arelatively large drop in performance on both datasets. Through the attention function, thesurrounding utterances could be given different weights so that the current utterance couldbetter receive information from other utterances. Therefore, the use of attention weightsaattwas beneficial for the aggregation of the node information. When we removed the aposweights, both datasets also had relatively large drops. The distance between utterancesmay affect the interaction between two utterances. Appropriately reducing the influence ofsurrounding utterances according to the relative distance can also cause the model to betteraggregate node features to a certain extent.",
        "Table 4. Results of the ablation study. ' #' represents the reduced performance compared with the'Full model'.",
        "Method MELD EmoryNLPFull model 66.4 40.1w/o apos65.61 (#0.79) 39.15 ( #0.95)",
        "When we used one hypergraph and removed the other hypergraphs, we only per-formed the hypergraph convolution of one hypergraph. From the results in Table 4, wecan see that the performance of the model was degraded regardless of which hypergraphwas removed. Here, the model also had a relatively large performance drop after removingthe NSHG, which also showed that the method of modeling for non-specific speakers wasfeasible. In multi-party dialogues, the influences of utterances from other speakers shouldbe considered in a targeted manner.",
        "5.3. Effects of the Depth of the GNN and Window SizesWe explored the relationship between model performance and the depth of ERMC-MHGNN. According to Figure 3, the best values of { LSSHG,LNSHG} were {1, 1} and {4, 6} onthe MELD and EmoryNLP datasets, and a 66.4% weighted-average F1 and 40.1% weighted-average F1, respectively, were obtained. Note that the convolution on the EmoryNLPdataset required more NSHG layers. This may have been related to the number of labels inthe conversation and the length of the conversation. The proportion of each label in theEmoryNLP dataset was more balanced than that in the MELD dataset; the proportion ofemotional shift was relatively larger, and the conversation length was also larger. Therefore,the EmoryNLP dataset needed more NSHG layers for convolution to deal with inter-dependency. The proportion of neutral labels in the MELD dataset was relatively large,and the conversation length was relatively small. Therefore, the MELD dataset did notrequire too many convolution layers. In general, the use of two types of hypergraphs wasbeneficial for understanding contextual cues and speaker dependencies and for enhancingthe recognition ability of the model.",
        "Appl. Sci. 2023 ,13, 1660 11 of 16Figure 3. Effect of the depth of the GNN. We report the weighted F1 score on the MELD andEmoryNLP datasets. The darker the color, the better the performance.",
        "We also experimented with both datasets by increasing the window size of the pastand future. The experimental results are shown in Figure 4. In the figure, we can see that thewindow size of the context had a relatively small effect on the two datasets, but the contextwindow sizes for obtaining relatively good results for the different datasets were not thesame. In the MELD dataset, there was a relatively certain number of conversations withless than three utterances, while in the EmoryNLP dataset, the length of the conversationswas generally greater than five utterances. Therefore, the MELD dataset had better resultswhen the window size of the past and future was 1, while the EmoryNLP dataset requireda relatively large context window.",
        "37383940414243616263646566671 2 3 4 5 6 7 8 9 10 11 12 13 14EmoryNLPMELDContext Window SizeMELDEmoryNLPFigure 4. Effects of window sizes.",
        "5.4. Error AnalysisWe analyzed our predicted emotion labels and found that misclassifications oftenoccurred in similar emotion classes, such as scared vs.mad and joyful vs.peaceful . In addition,some non-neutral labels were predicted as neutral labels on datasets where neutral labelswere the majority. Figure 5 shows the confusion matrix obtained by our model on theEmoryNLP dataset.",
        "Appl. Sci. 2023 ,13, 1660 12 of 16Joyful Neutral Powerful Mad Sad Scared PeacefulPredict labelJoyfulNeutralPowerfulMadSadScaredPeacefulTrue label138 50 27 14 5 20 2827 212 23 7 6 40 3442 30 24 6 5 20 186 21 21 42 4 16 34 24 13 9 15 20 1313 40 11 10 13 78 1721 53 16 5 5 25 34EmoryNLP1255075100125150175200Figure 5. Heatmap of the confusion matrix of ERMC-MHGNN on the EmoryNLP dataset.",
        "We also studied the emotional shift issue, which means that the emotions of twoconsecutive utterances from the same speaker were different. Since DialogXL did not pro-vide the corresponding emotional shift prediction accuracy on the MELD and EmoryNLPdatasets, we reproduced it. The weighted-average F1 of DialogXL on the MELD andEmoryNLP datasets was 62.67% and 35.0%, respectively, which are both higher than theresults in the paper. The emotional shift prediction accuracy of DialogXL on the MELD andEmoryNLP datasets is listed in Table 5. It can be seen in Table 5 that, compared with theother two models, our model greatly improved the accuracy in identifying emotional shiftsin these two multi-party dialogue datasets. However, improving the accuracy of identifyingemotional shifts can easily reduce the accuracy of identifying without emotional shifts.",
        "Compared with the other models, we were able to improve the accuracy of recognizingemotional shifts while keeping the accuracy of recognition without emotional shifts at ahigh level.",
        "Table 5. Test accuracy of ERMC-MHGNN and the partial baseline models on samples with anemotional shift and without one. '()' indicates the number of samples.",
        "#MELD EmoryNLPShift w/o Shift Shift w/o Shift(1003) (861) (673) (361)",
        "DialogXL 57.33 71.43 33.88 43.77DAG-ERC 59.02 69.45 37.29 42.10ERMC-MHGNN 62.01 72.36 38.93 41.835.5. Case StudyFor a comprehensive understanding of our proposed method, we allowed its perfor-mance to be visualized through a case study, which was selected from the EmoryNLP testdataset. As illustrated in Figure 6, our model and the baseline model both made mistakeswhen predicting the emotions of utterance (2) and utterance (3). By checking the videocorresponding to this conversation, we found that Emily had already started checking infor boarding. Ross was afraid that Emily could not see him coming to find her, so Rosscried out to Emily anxiously. In this dialogue, if we only consider the text features, wewill lack some emotional information, so we cannot accurately predict the corresponding",
        "Appl. Sci. 2023 ,13, 1660 13 of 16emotions. In our future work, we will use multimodal features to make up for the lack ofemotional information in the text features.",
        "Scene : The airport, Emily is getting ready to board her flight to London.",
        "This is the boarding call for Flight 009.",
        "Oh my God! What are you doing here?",
        "I just, I had to see you one more time before you took-off.",
        "This is the final boarding call for Flight 009.",
        "You are so sweet.",
        "That's, that's, that's a big candy bar. I had the most amazingtime with you.",
        "Me too.",
        "Well, that' me. Here, have this. I'm only allowed one piece of carryon anyway.",
        "Emily! surprise     surprise surprisefear        neutral      surprisesadness     disgust      disgustanger       anger surpriseneutral    neutral neutralneutral    neutral neutralneutral    neutral neutralsurprise    surprise surpriseanger      anger      surprise Label        Ours      DialogXLTicket AgentEmilyRoss123456789Figure 6. Results of the case study on the EmoryNLP dataset, where three participants in a conversa-tion are provided, along with their dependent historical utterances. We use green and red to highlightthe right and wrong predictions.",
        "When the conversation went to utterance (8) and utterance (9), we found that Emily'scommunication with Ross was interrupted, and Emily was urged to board. In addition,in combination with the semantics of utterance (9), Emily could not carry the big candybar that Ross gave her. So, Emily's mood changed. We found that the baseline modeldirectly predicted the emotion of utterance (9) within the emotion of utterance (8). Thebaseline model did not combine the context of this conversation well and did not fullyconsider multiple historical contexts. Our model processed the utterances of three speakersthrough hypergraphs and captured Emily's emotional change from 'neutral' to 'angry' inthe current context by using two types of speaker dependence.",
        "6. ConclusionsIn this paper, two different hypergraphs were constructed for two speaker dependen-cies for the first time, and a multi-hypergraph neural network—namely, ERMC-MHGNN—was designed for multi-party conversation emotion recognition to better handle speakerdependencies. The experimental results show that ERMC-MHGNN has good performance.",
        "Furthermore, through comprehensive evaluation and ablation studies, we can confirm theadvantages of ERMC-MHGNN and the impacts of its modules on performance. Severalconclusions can be drawn from the experimental results. First, our approach to non-speaker-specific modeling of utterances from other speakers is feasible. Second, combining averageaggregation with attention aggregation can allow better hyperedge features to be obtained.",
        "Finally, although the model achieved certain results on the emotional shift issue, the abilityof the model to recognize similar emotions still needs to be enhanced.",
        "In the future, we plan to build a hierarchical hypergraph neural network based onthe existing hypergraph network to deal with interactions within a single modality andinteractions among multiple modalities. We believe that hierarchical hypergraph neuralnetworks can not only handle high-order relationships between utterances, but can alsoalleviate the deficiencies of single-mode features.",
        "Appl. Sci. 2023 ,13, 1660 14 of 16Author Contributions: Conceptualization, C.Z., X.S., H.X. and Z.Z.; methodology C.Z., X.S. andH.X.; software, H.X.; validation, H.X.; formal analysis, X.S.; investigation, C.Z.; resources, H.X.; datacuration, H.X.; writing—original draft preparation, H.X. and Z.Z.; writing—review and editing,X.S. and C.Z.; visualization, H.X.; supervision, X.S. and C.Z.; project administration, H.X.; fundingacquisition, X.S. All authors have read and agreed to the published version of the manuscript.",
        "Funding: This work was supported by the General Programmer of the National Natural Sci-ence Foundation of China (61976078) and the Major Project of Anhui Province under Grant No.",
        "Institutional Review Board Statement: Not applicable.",
        "Informed Consent Statement: Not applicable.",
        "Data Availability Statement: Not applicable."
    ]
}
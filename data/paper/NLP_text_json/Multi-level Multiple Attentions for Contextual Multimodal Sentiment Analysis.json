{
    "Abstract": " —Multimodal sentiment analysis involves identifyingsentiment in videos and is a developing field of research. Unlikecurrent works, which model utterances individually, we proposea recurrent model that is able to capture contextual informationamong utterances. In this paper, we also introduce attention-based networks for improving both context learning and dynamicfeature fusion. Our model shows 6-8% improvement over thestate of the art on a benchmark dataset.",
    "Keywords": null,
    "Body": [
        "IntroductionEmotion recognition and sentiment analysis have become anew trend in social media, avidly helping users to understandopinions expressed in social networks and user-generated con-tent [1]. With the advancement of communication technology,abundance of smartphones and the rapid rise of social media,large amounts of data are uploaded by web users in the formof videos, rather than text [2]. For example, consumers tendto record their reviews on products using a web camera andupload them on social media platforms such as YouTube orFacebook to inform subscribers of their opinions. These videosoften contain comparisons of products from competing brands,the pros and cons of product specifications, etc., which can aidprospective buyers in making an informed decision.",
        "An utterance is a segment of speech bounded by breathsor pauses. A review video often contains multiple utterances.",
        "The goal of utterance-level sentiment analysis is to label eachutterance by its sentiment label. Utterance-level sentimentanalysis facilitates the understanding of the reviewer' senti-ment dynamics on multiple aspects of the review. Recently, anumber of approaches have been proposed in the field of mul-timodal sentiment analysis [3], [4], [5]. All such approachesconsider each utterance as an independent entity and, hence,ignore the relationship and dependencies between them. In avideo, however, utterances maintain a sequence and can behighly correlated due to the development of the speaker'sidea, co-reference and discourse structure. In particular, theclassification of an utterance can benefit from the contextualinformation of other utterances. Modeling such contextualrelationship, however, may not be enough. Identifying relevantand important information from the pool of utterances isnecessary in order to make a model more robust and accurate.To this end, we propose an attention-based long short-term memory (LSTM) network which not only models thecontextual relationship among utterances, but also prioritizesmore relevant utterances for classifying the target utterance.",
        "Experimental results show that the proposed framework out-performs the state of the art on benchmark datasets by 6-8%.",
        "Below, we describe the major contributions of the paper:",
        "●We propose a contextual attention-based LSTM (CAT-LSTM) network to model the contextual relationshipamong utterances and prioritize the important contextualinformation for classification.",
        "●We introduce an attention-based fusion mechanism,termed AT-Fusion, which amplifies the higher qualityand informative modalities during fusion in multimodalclassification.",
        "The remainder of this paper is organized as follows: Section IIdescribes the proposed method in detail; experimental resultsand discussion are shown in Section III; finally, Section IVconcludes the paper.",
        "II.MethodIn the following subsections, we discuss the problem defi-nition and explain the proposed approach in detail.",
        "A.Problem DefinitionLet us assume a video to be considered as Vj=[uj;1uj;2uj;3;:::;u j;i:::uj;Lj]whereuj;iis theithutterancein videovjandLjis the number utterances in the video. Thegoal of this approach is to label each utterance uj;iwith thesentiment expressed by the speaker. We claim that, in order toclassify utterance uj;i, the other utterances in the video, i.e.,[uj;k/divides.alt0∀k≤Lj;k≠i], serve as its context and provide keyinformation for the classification.",
        "B.Overview of the ApproachThe overview of the proposed approach is as follows:",
        "1.Unimodal feature extraction We first extract utterance-level unimodal features from the respective unimodalclassifiers. This phase does not consider the contextualrelationship among the utterances.",
        "2.AT-Fusion – Multimodal fusion using the attentionmechanism In this step, the utterance-level unimodalfeatures extracted at Step 1 are fused using an attentionnetwork (AT-Fusion) and the resulting output is used inthe next step for sentiment classification.",
        "3.CAT-LSTM – Attention-based LSTM model for sen-timent classification CAT-LSTM is an attention-basedLSTM network which accepts the features (output of Step2) of a sequence of utterances per video and generatesa new representation of those utterances based on thesurrounding utterances.",
        "C.Unimodal Feature ExtractionIn this step, we extract unimodal features using dedicatedunimodal feature extractors. The utterances are treated inde-pendently in this process.",
        "1) Textual Features Extraction: We use a CNN fortextual feature extraction, which takes utterances representedas a matrix of Google word2vec [6] vectors. Such vectorscover 87% of the vocabulary of CMU-MOSI dataset; themissing ones are initialized randomly. The convolution filtersare then applied to this matrix of word vectors.",
        "The CNN has two convolutional layers: the first layer hastwo kernels of size 3and4, with 50feature maps each and thesecond layer has a kernel of size 2with100feature maps. Theconvolution layers are interleaved with max-pooling layers ofwindow 2×2. This is followed by a fully connected layer ofsize500and softmax output. We use ReLU as the activationfunction. The activation values of the fully-connected layer aretaken as the features of utterances for text modality.",
        "2)Audio Feature Extraction: Audio features are ex-tracted with 30 Hz frame-rate and a sliding window of 100ms using openSMILE toolkit. In order to identify sampleswith and without voice, voice normalization is performedusing Z-standardization technique. The features extracted byopenSMILE consist of several low-level descriptors, e.g., voiceintensity, pitch, and their statistics, e.g., mean, root quadraticmean.",
        "3)Visual Feature Extraction: There are various choicesof deep networks specialized for image/video classification,e.g., cascaded CNN layers and recurrent neural networks(RNNs) such as LSTM and GRU. We chose 3D-CNN dueto its proven ability to learn image representations (like 2D-CNN), along with the changes among the sequence of images(frames) in a video [7]. Let V∈Rc×f×h×wrepresents anutterance video, where c= number of channels in an image(in our experiments c= 3, since the constituent images areRGB),f= number of frames, h= height of each frame, andw= width of each frame. We apply 3D convolutional filterFto videoV, whereF∈Rfm×c×fd×fh×fw,fm= numberof feature maps, c= number of channels, fd= number offrames,fh= height of the filter, and fw= width of the filter(we choseF∈R32×3×5×5×5). Following the philosophy of 2D-CNN, 3D-CNN slides filter Facross video Vand producesoutputcvout∈Rfm×c×(f−fd+1)×(h−fh+1)×(w−fw+1).To discard irrelevant features, we apply max-pooling ofwindow 3×3×3oncvout . Output of pooling layer is fedto a fully-connected layer of size 300, followed by a softmaxlayer for classification. The activations of the fully-connectedlayer is used as the features of video V.",
        "D.AT-Fusion – Attention-Based Network for MultimodalFusionAttention mechanism has the ability to focus on the mostimportant parts of an object relevant to the classification, im-proving the performance of the baseline deep neural networks.",
        "The attention mechanism has been successfully employed inNLP tasks such as sentiment analysis [8]. Not all modalitiesare equally relevant in the classification of sentiment. Inorder to prioritize only important modalities, we introducean attention network, termed as AT-Fusion, which takes asan input audio, visual, and textual modalities and outputs anattention score for each modality.",
        "We equalize the dimensions of the feature vectors of allthree modalities prior to feeding them into the attentionnetwork. This is done using a fully-connected layer of size d.",
        "LetB=[Ba;Bv;Bt]be the feature set after dimensionalityequalization to size d, whereBa= acoustic features, Bv=visual features, and Bt= textual features; following B∈Rd×3.",
        "The value of dwhen set to 300 gives best performance.",
        "The attention weight vectorfuse and the fused multimodalfeature vector Fare computed as follows:",
        "PF=tanh(WF:B) (1)",
        "fuse=softmax (wTF:PF) (2)",
        "F=B:",
        "Tfuse (3)",
        "Here,WF∈Rd×d,wF∈Rd,Tfuse∈R3, andF∈Rd. We thenfeed the output Fto the CAT-LSTM (Section II-E1, Figure 1)",
        "for final multimodal sentiment classification of the utterance.",
        "E.Classifier: Context-Dependent Sentiment ClassificationA speaker usually tries to gradually develop his/her ideaand opinion about a product in the review, which makes theutterances in a video sequential, temporally and contextuallydependent. This phenomenon motivates us to model inter-utterance relationship. To this end, we use a LSTM layer,in combination with the attention mechanism to amplify theimportant contextual evidences for sentiment classification oftarget utterance.",
        "1)Proposed CAT-LSTM for Sentiment Classification:",
        "LSTM is a specialized RNN, which models long-range depen-dencies in a sequence. Specifically, LSTM solves the vanish-ing gradient problem of conventional RNNs, while modelinglong-range dependencies. Current research in NLP indicatesthe benefit of using such networks to incorporate contextualinformation in the classification process [9], [10].",
        "Let,x∈Rd×Mbe input to the LSTM network, where Mis the number of utterances in a video. The matrix xcan berepresented as x=[x1;x2;:::;x t;:::xM], wherext∈Rdfort=0toM.",
        "AudioVisualTextAttention1Utterance1F1Attention1Softmaxh1LSTMh1AudioVisualTextAttention2Utterance2F2Attention2Softmaxh2LSTMh2AudioVisualTextAttentionnUtterancenFnAttentionnSoftmaxhnLSTMhn......",
        "......AT-FusionCAT-LSTMCATF-LSTMFig. 1: CATF-LSTM takes input from multiple modalities, fuses them using AT-Fusion, and sends the output to CAT-LSTM for classification.",
        "Each cell in LSTM can be computed as follows:",
        "X=/bracketleft.alt3ht−1xt/bracketright.alt3 (4)",
        "ct=ft⊙ct−1+it⊙tanh(Wc:X+bc) (8)",
        "ht=ot⊙tanh(ct) (9)",
        "whereWi;Wf;Wo∈Rd×2d,bi;bf;bo∈Rdare parameters tobe learnt during the training, \u001bis the sigmoid function and ⊙is element-wise multiplication.",
        "The output of this LSTM layer is represented as H∈Rd×M,whereH=[h1;h2;:::;h t;:::;h M]andhi∈Rd. We feedthe sequence of Mutterance-level features (fused features F,obtained in equation (3)or unimodal features) to LSTM andobtain contextually-aware utterance representations H.",
        "a)Attention Network: All surrounding utterances arenot equally relevant in the sentiment classification of the targetutterance. In order to amplify the contribution of context-richutterances, we use an attention network.",
        "LetAtdenote the tthAttention network for utterancerepresented by ht. The attention mechanism of Atproducesan attention weight vectortand a weighted hidden represen-tationrtas follows:",
        "Pt=tanh(Wh[t]:H) (10)",
        "t=softmax (w[t]T:Pt) (11)",
        "rt=H:",
        "where,Pt∈Rd×M;",
        "t∈RM;rt∈Rd. And,Wh∈RM×d×d;w∈RM×dare projection parameters with Wh[t]andw[t]beingused by the tthattention model.Finally, the LSTM representation for tthutterance is mod-ified as:",
        "h∗t=tanh(Wp[t]:rt+Wx[t]:ht) (13)",
        "Similar to the results obtained by Rockt ¨aschel et al. [11],addition of the term Wx[t]:httoWp[t]:rtgives better resultin the experiments carried out. Here, h∗t∈RdandWp;Wx∈RM×d×dare weights to be learnt while training. In someexperiments (e.g., Section II-F2b), we use the output h∗tascontextual features for further processing.",
        "b)Classification: Finally, each modified LSTM cell out-puth∗tis sent into a softmax layer for sentiment classification.",
        "Zt=softmax ((h∗t)T:Wsoft[t]+bsoft[t]) (14)",
        "^yt=arg maxj(Zt[j]);∀j∈class (15)",
        "where,Zt∈Rydim;Wsoft∈RM×d×ydim;bsoft∈RM×ydim;ydim=number of classes, and ^ytis the predictedclass.",
        "F .Training1)Unimodal Classification: In our work, we performclassification on two types of data – unimodal and multimodal.",
        "To classify the unimodal input, the extracted unimodal features(Section II-C) are sent to the CAT-LSTM network as inputs.",
        "2)Multimodal Classification: For multimodal classifi-cation, the extracted unimodal features are first fed to theAT-Fusion to produce fused multimodal features. Then, suchfeatures are fed to the CAT-LSTM network for sentimentclassification. We call this multimodal sentiment classificationmodel as Contextual Attentive Fusion LSTM, i.e., CATF-LSTM. The CATF-LSTM is shown in Figure 1. Multimodalclassification be accomplished using two different frameworks:",
        "a)Single-Level Framework: In this framework, wefuse context-independent unimodal features as explained inSection II-D and feed those to CATF-LSTM for multimodalfusion and classification.",
        "b)Multi-Level Framework: Contextual unimodal fea-tures can further improve the performance of the multimodalfusion framework explained in Section II-F2a. In this fusionscheme, we first send context-independent unimodal featuresextracted from every modality to CAT-LSTM. The contextualfeatures yielded from CAT-LSTM are then fed to CATF-LSTMfor fusion and final classification. Both the unimodal andmultimodal classifiers are trained in an end-to-end mannerusing back propagation, with objective function being log-loss:",
        "loss=−/summation.dispi/summation.dispjlog(Zt[yji])+\u0015/divides.alt0/divides.alt0\u0012/divides.alt0/divides.alt02(16)",
        "where,y= target class, Zt= predicted distribution of jthutterance from video Vis.t.i∈[0;N]andj∈[0;Li].\u0015istheL2regularization term and \u0012is the parameter set \u0012={Wi;bi;Wf;bf;Wo;bo;WF;wF;Wh;w;W p;Wx;Wsoft;",
        "bsoft}.",
        "In our experiments, we pad videos with dummy utterancesto enable batch processing. Hence, we also use bit-masks tomitigate proliferation of noise in the network. The network istypically trained for 500-700 epochs with an early-stoppingpatience of 20 epochs. As optimizer, we use AdaGrad whichis known to have improved robustness over SGD, given itsability to adapt the learning rate based on the parameters.",
        "III. Experimental ResultsIn this section, we present the experimental results ondifferent network variants in contrast with various baselines.",
        "A.Dataset detailsWe perform person-independent experiments to emulateunseen conditions. Our train/test splits of the dataset arecompletely disjoint with respect to speakers.",
        "a)CMU-MOSI Dataset: Zadeh et al. [12] constructeda multimodal sentiment analysis dataset termed MultimodalOpinion-Level Sentiment Intensity (CMU-MOSI), consisting2199 opinionated utterances, 93videos by 89speakers. Thevideos address a large array of topics, such as movies, books,and products. Videos were crawled from YouTube and seg-mented into utterances. Each of the 2199 utterances werelabeled with its sentiment label, i.e., positive and negative. Thetrain set comprises of the first 62individuals in the dataset.",
        "So, the test set comprises of 31videos by 27speakers. Inparticular, we use 1447 utterances in the training and 752utterances to test the models out of which 467 are negativeand 285 are negative.",
        "B.Different Models and Network ArchitecturesWe have carried out experiments with both unidirectionaland bi-directional LSTM with the later giving 0.3-0.7% betterperformance in all kinds of experiments. As this is an expectedand non-critical outcome, we present all the results belowusing bi-directional LSTM variant. Additionally, we considerthe following models in our experiments:a)Poria et al. (2015): We have implemented andcompared our method with the current state of the art ap-proach, proposed by Poria et al. [4], who extracted visualfeatures using CLM-Z, audio features using openSMILE, andtextual features using CNN. Multiple kernel learning was thenapplied on the features obtained from the concatenation ofthe unimodal features. However, authors did not conduct anyspeaker-independent experiments.",
        "b)Poria et al. (2016): This is an extended approach withrespect to [4], which introduces a CNN-RNN feature extractorto extract visual features. We reimplemented this approach inour experiments.",
        "c)Unimodal-SVM: We extract unimodal features (Sec-tion II-C) and concatenate them to produce multimodal fea-tures. A support vector machine (SVM) is applied on theresulting feature vector for the final sentiment classification.",
        "d)Simple-LSTM: In this configuration, the extractedunimodal and multimodal features of the utterances are fed toa LSTM without attention mechanism.",
        "e)CAT-LSTM: This is the simple contextual attention-based LSTM framework as described in Section II-E1. Formultimodal setting, it accepts input generated by appendingunimodal features.",
        "f)CATF-LSTM: This model is used for multimodalclassification. As explained in Section II-F, it consists of AT-Fusion and CAT-LSTM, where the output of AT-Fusion is fedto CAT-LSTM.",
        "g)ATS-Fusion: In this variant, instead of feeding theoutput of AT-Fusion to the cells of CAT-LSTM, we feed tosoftmax classifiers. The utterances are treated independentlyin this case.",
        "h)Poria et al. (2015) + Our best model: In order toperform a fair comparison with Poria et al. (2015), we feedthe features extracted by their method to our best performingmodel.",
        "i)Poria et al. (2016) + Our best model: This modelis similar to the model Poria et al. (2015) + Our best model ,except it uses the features extraction process from Poria etal. (2016).",
        "C.Single-Level vs Multi-level FrameworkMulti-level framework outperforms single-level frameworkin our experiments given the presence of contextual unimodalfeatures (see Table II). Hence, for brevity, apart from Table II,we present only the results of multi-level framework.",
        "D.AT-Fusion PerformanceAT-Fusion employs the attention mechanism to fuse mul-tiple modalities. In order to assess the effectiveness of AT-Fusion, we compare it with a simple fusion technique wherethe feature vectors from different modalities are appended andfed to the sentiment classifier, i.e., CAT-LSTM.",
        "Table II presents the performance of CATF-LSTM, whichutilizes AT-Fusion for feature fusion followed by CAT-LSTMfor sentiment classification. Given AT-Fusion's ability to am-plify the contribution of the important modalities during fu-sion, it unsurprisingly outperforms the simple fusion method.",
        "It should be noted that AT-Fusion can be integrated withthe other network variants, i.e., Simple-LSTM (Table III).",
        "Table III also shows that AT-Fusion with softmax output, i.e.,ATS-Fusion, which outperforms the unimodal-SVM thanks tothe superiority of the AT-Fusion over simple feature-appendfusion.",
        "E.Comparison Among the Modelsa)Comparison with the state of the art: As shown inTable I, the proposed approach has outperformed the state ofthe art [4], [13] in the range of 6.25%-7.5%. We use the sameset of textual and audio features used in [4], [13]. Notably,apart from using a different fusion mechanism, our methodalso uses a different visual feature extraction method. On theCMU-MOSI dataset, the proposed visual feature extractionmethod has outperformed the CLM-Z (used in [4]) and CNN-RNN (proposed by [13]). When we employ our best classifier,i.e., CATF-LSTM, on the features extracted by [4] and [13],performance of those methods have improved. Using CATF-LSTM on the features extracted by those methods, we obtainedbetter results than both of them for audio-visual, visual-textual bimodal experiments. According to [4], [13] trimodalclassifier outperforms all unimodal and bimodal classifiers.",
        "Hence, we compare our proposed method with those worksin the trimodal experiment. From these experimental results(Table I), it is evident that the proposed contextual attention-based LSTM network and fusion methodology are the key tooutperform the state of the art.",
        "Models A+V+TPoria et al. (2015) 73.55%Poria et al. (2016) 75.13%Features of Poria et al. (2015)+ CATF-LSTM 79.40%Features of Poria et al. (2016) + CATF-LSTM 80.25%Our features + CATF-LSTM 81.30%TABLE I: Comparison of state-of-the-art on multimodal classifica-tion with our network: CATF-LSTM. Metric used: macro-fscore.",
        "A=Audio;V=Visual;T=Textual.",
        "b)unimodal-SVM: Our unimodal-SVM model yieldscomparable performance with the state of the art. However,simple-LSTM outperforms unimodal-SVM in all the experi-ments (Table I) as the latter is incapable of grasping the contextinformation while classifying an utterance.",
        "ModalitySingle-Level Multi-LevelFeat Append AT-Fusion Feat Append AT-FusionA+V 61.0 61.6 62.4 62.9A+T 78.5 79.2 79.5 80.1V+T 77.6 78.3 79.6 79.9A+V+T 78.9 79.3 81.0 81.3TABLE II: Comparison between single-level andmulti-level fusionmentioned in Section II-F2 using CAT-LSTM network. Feat Ap-pend=Unimodal features are appended and sent to CAT-LSTM. AT-Fusion is used with CAT-LSTM network. The table reports the macro-fscore of classification. A=Audio;V=Visual;T=Textual.ModalitiesSentiment, on CMU-MOSIUni-SVM Simple-LSTM CAT-LSTMfeat-appfeat-appAT-Fusionfeat-appAT-FusionATS-FusionA 58.1 59.5 - 60.1 - -V 53.4 54.9 - 55.5 - -T 75.5 77.2 - 79.1 - -A + V 58.6 61.4 61.8 62.4 62.9 59.1A + T 75.8 78.5 79.1 79.5 80.1 76.3V + T 76.7 78.7 79.1 79.6 79.9 77.5A + V + T 77.9 80.1 80.6 81.0 81.3 78.3TABLE III: Comparison of models mentioned in Section III-B.",
        "The table reports the macro-fscore of classification.",
        "Note: feat-appen=fusion by appending unimodal features.",
        "Multi-level framework is employed (See Section II-F2).",
        "A=Audio;V=Visual;T=Textual.",
        "c)CAT-LSTM vs Simple-LSTM: From Table III, wecan see that CAT-LSTM outperforms Simple-LSTM by 0.6-1.1% in unimodal experiments; 0.2-1% in bimodal experi-ments, and 0.9% in trimodal experiment. This again confirmsthat, even though both networks have access to contextualinformation, CAT-LSTM outperforms Simple-LSTM becauseof its attention capability to capture important contexts. Asexpected, CATF-LSTM further improves (0.3-0.6%) the per-formance of CAT-LSTM as it employs attention mechanismin fusion.",
        "F .Importance of the ModalitiesAs expected, bimodal classifiers dominate unimodal clas-sifiers and trimodal classifiers perform the best among all.",
        "Across modalities, textual modality performs better than theother two, thus indicating the need for better feature extractionfor audio and video modalities.",
        "G.Qualitative Analysis and Case StudiesIn this section, we provide analysis and interesting obser-vations on the learnt attention parameters for both contex-tual attention (CAT-LSTM) and attention fusion (AT-Fusion).",
        "Following, we list some of these observations. The need forconsidering context dependency (see Section I) is primal forutterance classification. For example, the utterance “ Whoeverwrote this isn't the writer definitely ” has the sentiment ex-pressed implicitly and, hence, baseline unimodal-SVM andstate of the art fail to classify it correctly1. Informationfrom neighboring utterances, e.g., “And the dialogue threwme off” and“The whole movie had a really dry dialogue” ,indicate the negative context for the utterance. Such contextualrelationships are prevalent throughout the dataset.",
        "There are also cases where utterances are very ambiguous ifconsidered separately because of the lack of context, e.g., “ Younever know whats gonna happen ”. In such cases, our contextattention network attends to relevant utterances throughout thevideo to find contextual dependencies.",
        "1RNTN classifies it as neutral. It can be seen herehttp://nlp.stanford.edu:8080/sentiment/rntnDemo.html",
        "(a) The visualization of the attention scores of unimodal CAT-LSTMand trimodal CATF-LSTM. A,T,V,Frepresent attention scoresof audio, textual, and visual fusion, respectively.",
        "(b) Visualization of the attention weights of the AT-Fusion network intrimodal CATF-LSTM.",
        "Fig. 2: Target utterance for classification - U4: You never know whats gonna happen. The input utterances are - U1: This is the most engaging endeavor yet. U2: And he putsthem in very strange and unusual characters. U3: Umm what really need about this movie is the chemical brothers did the soundtracks whats pulse pounding the entire way through.",
        "U4: You never know whats gonna happen. U5: Theres all these colorful characters. U6: Now it isn't a great fantastic tell everybody about that kind of movie. U7: But I think itsone of those movies thats so unique. U8: Its colourful. U9: Its in you face. U10: And something that I can't find anything else to compare it to.",
        "Figure 2a shows the attention weights across the video forthe above-mentioned utterance. While audio and visual providedecent attention vectors, text modality provides improvedattention. It can be clearly seen that utterances like U10,U1(Figure 2a) are the most relevant ones, which multimodalattention has been able to capture, thus proving its effective-ness. Interestingly, in this case the most important utteranceU10is located far from the target utterance U4, proving theeffectiveness of LSTM in modeling long distance sequence.",
        "Figure 2b shows the contribution of each modality for themultimodal classification. Rightly, text has been given thehighest weight by the attention fusion network, followed byaudio and visual. Although the context dependency amongutterances can be modeled in a simple LSTM network, thereare often cases where utterances with complementary contextsare sparsely distributed across the video. In such situations,neighboring irrelevant utterances may provide negative biasfor the utterance to be classified.",
        "Our model, instead, provides an attention framework whichfocuses only on the relevant utterances throughout the video.",
        "For example, in one of the videos, the first utterance “I amgonna give the reasons why I like him” has its answers fromthe7thutterance onwards, with the intermediate utterancesbeing irrelevant. In such situations, CAT-LSTM performsbetter than simple-LSTM model.",
        "The effectiveness of AT-Fusion can also be seen in multiplecases. In one such instance, the audio quality of the utterance“Sigh it probably would have helped if I went with someone”"
    ]
}
{
    "Abstract": "In this work, we propose Retentive Network (RETNET) as a foundation archi-tecture for large language models, simultaneously achieving training parallelism,low-cost inference, and good performance. We theoretically derive the connectionbetween recurrence and attention. Then we propose the retention mechanism forsequence modeling, which supports three computation paradigms, i.e., parallel,recurrent, and chunkwise recurrent. Specifically, the parallel representation allowsfor training parallelism. The recurrent representation enables low-cost O(1)infer-ence, which improves decoding throughput, latency, and GPU memory withoutsacrificing performance. The chunkwise recurrent representation facilitates effi-cient long-sequence modeling with linear complexity, where each chunk is encodedparallelly while recurrently summarizing the chunks. Experimental results onlanguage modeling show that RETNETachieves favorable scaling results, paralleltraining, low-cost deployment, and efficient inference. The intriguing propertiesmake RETNETa strong successor to Transformer for large language models. Codewill be available at https://aka.ms/retnet .0204001503000150300GPU Memory‚Üì(GB)Throughput ‚Üë(wps)Latency ‚Üì(ms)3.4X15.6X8.4XInference Cost Scaling CurveRetNet Transformer1 3 7LM PerplexityModel Size (B)Figure 1: Retentive network (RetNet) achieves low-cost inference (i.e., GPU memory, throughput,and latency), training parallelism, and favorable scaling curves compared with Transformer. Resultsof inference cost are reported with 8k as input length. Figure 6 shows more results on differentsequence lengths.‚àóEqual contribution. ‚ãÑCorresponding author.arXiv:2307.08621v4  [cs.CL]  9 Aug 2023‚ÄúThe only way to discover the limits of the possible is to go beyond them into the impossible.Arthur C. Clarke‚Äù",
    "Keywords": null,
    "Body": [
        "IntroductionLow -CostInferenceTransformerRetNetFigure 2: RetNet makes the ‚Äúimpossible triangle‚Äù",
        "possible, which achieves training parallelism, goodperformance, and low inference cost simultane-ously.Transformer [ VSP+17] has become the defacto architecture for large language mod-els [BMR+20], which was initially proposedto overcome the sequential training issue ofrecurrent models [ HS97 ]. However, trainingparallelism of Transformers is at the cost of in-efficient inference, because of the O(N)com-plexity per step and memory-bound key-valuecache [ Sha19 ], which renders Transformers un-friendly to deployment. The growing sequencelength increases GPU memory consumption aswell as latency and reduces inference speed.",
        "Numerous efforts have continued to develop thenext-generation architecture, aiming at retain-ing training parallelism and competitive perfor-mance as Transformers while having efficientO(1)inference. It is challenging to achieve theabove goals simultaneously, i.e., the so-called‚Äúimpossible triangle‚Äù as shown in Figure 2.",
        "There have been three main strands of research.",
        "First, linearized attention [ KVPF20 ] approximates standard attention scores exp(q¬∑k)with kernelsœï(q)¬∑œï(k), so that autoregressive inference can be rewritten in a recurrent form. However, themodeling capability and performance are worse than Transformers, which hinders the method's popu-larity. The second strand returns to recurrent models for efficient inference while sacrificing trainingparallelism. As a remedy, element-wise operators [ PAA+23] are used for acceleration, however,representation capacity and performance are harmed. The third line of research explores replacingattention with other mechanisms, such as S4 [ GGR21 ], and its variants [ DFS+22,PMN+23]. Noneof the previous work can break through the impossible triangle, resulting in no clear winner comparedwith Transformers.",
        "In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long-sequence modeling, Transformer-comparable performance, and parallel model training simultane-ously. Specifically, we introduce a multi-scale retention mechanism to substitute multi-head attention,which has three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent repre-sentations. First, the parallel representation empowers training parallelism to utilize GPU devicesfully. Second, the recurrent representation enables efficient O(1)inference in terms of memoryand computation. The deployment cost and latency can be significantly reduced. Moreover, theimplementation is greatly simplified without key-value cache tricks. Third, the chunkwise recurrentrepresentation can perform efficient long-sequence modeling. We parallelly encode each local blockfor computation speed while recurrently encoding the global blocks to save GPU memory.",
        "We conduct extensive experiments to compare RetNet with Transformer and its variants. Experi-mental results on language modeling show that RetNet is consistently competitive in terms of bothscaling curves and in-context learning. Moreover, the inference cost of RetNet is length-invariant.",
        "For a 7B model and 8k sequence length, RetNet decodes 8.4 √ófaster and saves 70% of memorythan Transformers with key-value caches. During training, RetNet also achieves 25-50% memorysaving and 7 √óacceleration than standard Transformer and an advantage towards highly-optimizedFlashAttention [ DFE+22]. Besides, RetNet's inference latency is insensitive to batch size, allowingenormous throughput. The intriguing properties make RetNet a strong successor to Transformer forlarge language models.",
        "2 Retentive NetworksRetentive network (RetNet) is stacked with Lidentical blocks, which follows a similar layout (i.e.,residual connection, and pre-LayerNorm) as in Transformer [ VSP+17]. Each RetNet block containstwo modules: a multi-scale retention (MSR) module, and a feed-forward network (FFN) module.",
        "We introduce the MSR module in the following sections. Given an input sequence x=x1¬∑¬∑¬∑x|x|,RetNet encodes the sequence in an autoregressive way. The input vectors {xi}|x|i=1is first packedintoX0= [x1,¬∑¬∑¬∑,x|x|]‚ààR|x|√ódmodel, where dmodel is hidden dimension. Then we computecontextualized vector representations Xl= RetNet l(Xl‚àí1), l‚àà[1, L].",
        "2.1 RetentionIn this section, we introduce the retention mechanism that has a dual form of recurrence andparallelism. So we can train the models in a parallel way while recurrently conducting inference.",
        "Given input X‚ààR|x|√ódmodel, we project it to one-dimensional function v(n) =Xn¬∑wV. Consider asequence modeling problem that maps v(n)7‚Üío(n)through states sn. Letvn, ondenote v(n), o(n)",
        "for simplicity. We formulate the mapping in a recurrent manner:",
        "sn=Asn‚àí1+K‚ä∫nvn, A ‚ààRd√ód, Kn‚ààR1√ódon=Qnsn=nXm=1QnAn‚àímK‚ä∫mvm, Q n‚ààR1√ód(1)",
        "where we map vnto the state vector sn, and then implement a linear transform to encode sequenceinformation recurrently.",
        "Next, we make the projection Qn, Kncontent-aware:",
        "Q=XW Q, K =XW K (2)",
        "where WQ, WK‚ààRd√ódare learnable matrices.",
        "We diagonalize the matrix A= Œõ( Œ≥eiŒ∏)Œõ‚àí1, where Œ≥, Œ∏‚ààRd. Then we obtain An‚àím=Œõ(Œ≥eiŒ∏)n‚àímŒõ‚àí1. By absorbing ŒõintoWQandWK, we can rewrite Equation (1) as:",
        "on=nXm=1Qn(Œ≥eiŒ∏)n‚àímK‚ä∫mvm=nXm=1(Qn(Œ≥eiŒ∏)n)(Km(Œ≥eiŒ∏)‚àím)‚ä∫vm(3)",
        "where Qn(Œ≥eiŒ∏)n, Km(Œ≥eiŒ∏)‚àímis known as xPos [ SDP+22], i.e., a relative position embeddingproposed for Transformer. We further simplify Œ≥as a scalar, Equation (3) becomes:",
        "on=nXm=1Œ≥n‚àím(QneinŒ∏)(KmeimŒ∏)‚Ä†vm (4)",
        "where‚Ä†is the conjugate transpose. The formulation is easily parallelizable within training instances.",
        "In summary, we start with recurrent modeling as shown in Equation (1), and then derive its parallelformulation in Equation (4). We consider the original mapping v(n)7‚Üío(n)as vectors and obtainthe retention mechanism as follows.",
        "The Parallel Representation of Retention As shown in Figure 3a, the retention layer is defined as:",
        "Q= (XW Q)‚äôŒò, K = (XW K)‚äôŒò, V =XW VŒòn=einŒ∏, D nm=\u001aŒ≥n‚àím, n‚â•m0, n < mRetention( X) = (QK‚ä∫‚äôD)V(5)",
        "where Œòis the complex conjugate of Œò, andD‚ààR|x|√ó|x|combines causal masking and exponentialdecay along relative distance as one matrix. Similar to self-attention, the parallel representationenables us to train the models with GPUs efficiently.",
        "ùëãùêæùëÑ ùëâùëÇGNùëÑùêæ‚ä∫‚äôùê∑ùëâ(a) Parallel representation.",
        "ùëãùëõùëÜùëõ‚àí1 ùëÜùëõùõæùêæùëõ ùëâùëõ ùëÑùëõùëÇùëõRecurrentStateInputOutputGN (b) Recurrent representation.",
        "Figure 3: Dual form of RetNet. ‚ÄúGN‚Äù is short for GroupNorm.",
        "The Recurrent Representation of Retention As shown in Figure 3b, the proposed mechanismcan also be written as recurrent neural networks (RNNs), which is favorable for inference. For then-th timestep, we recurrently obtain the output as:",
        "Sn=Œ≥Sn‚àí1+K‚ä∫nVnRetention( Xn) =QnSn, n = 1,¬∑¬∑¬∑,|x|(6)",
        "where Q, K, V, Œ≥ are the same as in Equation (5).",
        "The Chunkwise Recurrent Representation of Retention A hybrid form of parallel representationand recurrent representation is available to accelerate training, especially for long sequences. Wedivide the input sequences into chunks. Within each chunk, we follow the parallel representation(Equation (5)) to conduct computation. In contrast, cross-chunk information is passed following therecurrent representation (Equation (6)). Specifically, let Bdenote the chunk length. We compute theretention output of the i-th chunk via:",
        "Ri=K‚ä∫[i](V[i]‚äôŒ∂) +Œ≥BRi‚àí1, Œ∂ ij=Œ≥B‚àíi‚àí1Retention( X[i]) = (Q[i]K‚ä∫[i]‚äôD)V[i]| {z }",
        "Inner-Chunk+ (Q[i]Ri‚àí1)‚äôŒæ|{z}",
        "Cross-Chunk, Œæ ij=Œ≥i+1(7)",
        "where [i]indicates the i-th chunk, i.e., x[i]= [x(i‚àí1)B+1,¬∑¬∑¬∑, xiB].",
        "2.2 Gated Multi-Scale RetentionWe use h=dmodel/dretention heads in each layer, where dis the head dimension. The heads usedifferent parameter matrices WQ, WK, WV‚ààRd√ód. Moreover, multi-scaleretention (MSR) assignsdifferent Œ≥for each head. For simplicity, we set Œ≥identical among different layers and keep themfixed. In addition, we add a swish gate [ HG16 ,RZL17 ] to increase the non-linearity of retentionlayers. Formally, given input X, we define the layer as:",
        "Œ≥= 1‚àí2‚àí5‚àíarange(0 ,h)‚ààRhhead i= Retention( X, Œ≥ i)",
        "Y= GroupNormh(Concat(head 1,¬∑¬∑¬∑,head h))",
        "MSR( X) = (swish( XW G)‚äôY)WO(8)",
        "where WG, WO‚ààRdmodel√ódmodelare learnable parameters, and GroupNorm [WH18 ] normalizes theoutput of each head, following SubLN proposed in [ SPP+19]. Notice that the heads use multiple Œ≥scales, which results in different variance statistics. So we normalize the head outputs separately.",
        "The pseudocode of retention is summarized in Figure 4.",
        "def ParallelRetention(q, # bsz ‚àónum_head ‚àólen ‚àóqk_dimk, # bsz ‚àónum_head ‚àólen ‚àóqk_dimv, # bsz ‚àónum_head ‚àólen ‚àóv_dimdecay_mask # num_head ‚àólen ‚àólen):",
        "retention = q @ k.transpose( ‚àí1,‚àí2)",
        "retention = retention ‚àódecay_maskoutput = retention @ voutput = group_norm(output)",
        "return outputdef RecurrentRetention(q, k, v, # bsz ‚àónum_head ‚àólen ‚àóqkv_dimpast_kv, # bsz ‚àónum_head ‚àóqk_dim ‚àóv_dimdecay # num_head ‚àó1‚àó1):",
        "current_kv = decay ‚àópast_kv + k.unsqueeze(‚àí1)‚àóv.unsqueeze( ‚àí2)",
        "output = torch.sum(q.unsqueeze( ‚àí1)‚àócurrent_kv, dim= ‚àí2)",
        "output = group_norm(output)",
        "return output, current_kvdef ChunkwiseRetention(q, k, v, # bsz ‚àónum_head ‚àóchunk_size ‚àóqkv_dimpast_kv, # bsz ‚àónum_head ‚àóqk_dim ‚àóv_dimdecay_mask, # num_head ‚àóchunk_size ‚àóchunk_sizechunk_decay, # num_head ‚àó1‚àó1inner_decay, # num_head ‚àóchunk_size):",
        "retention = q @ k.transpose( ‚àí1,‚àí2)",
        "retention = retention ‚àódecay_maskinner_retention = retention @ vcross_retention = (q @ past_kv) ‚àóinner_decayretention = inner_retention + cross_retentionoutput = group_norm(retention)",
        "current_kv = chunk_decay ‚àópast_kv + k.transpose( ‚àí1,‚àí2) @ vreturn output, current_kvFigure 4: Pseudocode for the three computation paradigms of retention.",
        "Retention Score Normalization We utilize the scale-invariant nature of GroupNorm to im-prove the numerical precision of retention layers. Specifically, multiplying a scalar value withinGroupNorm does not affect outputs and backward gradients, i.e., GroupNorm( Œ±‚àóhead i) =GroupNorm(head i). We implement three normalization factors in Equation (5). First, we normalizeQK‚ä∫asQK‚ä∫/‚àöd. Second, we replace Dwith ÀúDnm=Dnm/‚àöPni=1Dni. Third, let Rdenote theretention scores R=QK‚ä∫‚äôD, we normalize it as ÀúRnm=Rnm/max(|Pni=1Rni|,1). Then theretention output becomes Retention( X) =ÀúRV. The above tricks do not affect the final results whilestabilizing the numerical flow of both forward and backward passes, because of the scale-invariantproperty.",
        "2.3 Overall Architecture of Retention NetworksFor an L-layer retention network, we stack multi-scale retention (MSR) and feed-forward network(FFN) to build the model. Formally, the input sequence {xi}|x|i=1is transformed to vectors by a wordembedding layer. We use the packed embeddings X0= [x1,¬∑¬∑¬∑,x|x|]‚ààR|x|√ódmodelas the input andcompute the model output XL:",
        "Yl= MSR(LN( Xl)) +XlXl+1= FFN(LN( Yl)) +Yl(9)",
        "where LN(¬∑)is LayerNorm [ BKH16 ]. The FFN part is computed as FFN( X) = gelu( XW 1)W2,where W1, W2are parameter matrices.",
        "Training We use the parallel (Equation (5)) and chunkwise recurrent (Equation (7)) representationsduring the training process. The parallelization within sequences or chunks efficiently utilizesGPUs to accelerate computation. More favorably, chunkwise recurrence is especially useful forlong-sequence training, which is efficient in terms of both FLOPs and memory consumption.",
        "ArchitecturesTrainingParallelizationInference CostLong-SequenceMemory ComplexityPerformanceTransformer ‚úî O(N) O(N2) ‚úî‚úîLinear Transformer ‚úî O(1) O(N) ‚úòRecurrent NN ‚úò O(1) O(N) ‚úòRWKV ‚úò O(1) O(N) ‚úîH3/S4 ‚úî O(1) O(NlogN) ‚úîHyena ‚úî O(N) O(NlogN) ‚úîRetNet ‚úî O(1) O(N) ‚úî‚úîTable 1: Model comparison from various perspectives. RetNet achieves training parallelization,constant inference cost, linear long-sequence memory complexity, and good performance.",
        "Inference The recurrent representation (Equation (6)) is employed during the inference, whichnicely fits autoregressive decoding. The O(1)complexity reduces memory and inference latencywhile achieving equivalent results.",
        "2.4 Relation to and Differences from Previous MethodsTable 1 compares RetNet with previous methods from various perspectives. The comparison resultsecho the ‚Äúimpossible triangle‚Äù presented in Figure 2. Moreover, RetNet has linear memory complexityfor long sequences due to the chunkwise recurrent representation. We also summarize the comparisonswith specific methods as follows.",
        "Transformer The parallel representation of retention shares similar spirits as Transform-ers [ VSP+17]. The most related Transformer variant is Lex Transformer [ SDP+22] which im-plements xPos as position embeddings. As described in Equation (3), the derivation of retentionaligns with xPos. In comparison with attention, retention removes softmax and enables recurrentformulation, which significantly benefits inference.",
        "S4 Unlike Equation (2), if QnandKnare content-unaware, the formulation can be degenerated toS4 [GGR21], where O= (QK‚ä∫, QAK‚ä∫, .., QA|x|‚àí1K‚ä∫)‚àóV.",
        "Linear Attention The variants typically use various kernels œï(qi)œï(kj)/P|x|n=1œï(qi)œï(kn)to replacethesoftmax function. However, linear attention struggles to effectively encode position information,rendering the models less performant. Besides, we reexamine sequence modeling from scratch, ratherthan aiming at approximating softmax .",
        "AFT/RWKV Attention Free Transformer (AFT) simplifies dot-product attention to element-wiseoperations and moves softmax to key vectors. RWKV replaces AFT's position embeddings withexponential decay and runs the models recurrently for training and inference. In comparison, retentionpreserves high-dimensional states to encode sequence information, which contributes to expressiveability and better performance.",
        "xPos/RoPE Compared with relative position embedding methods proposed for Transformers,Equation (3) presents a similar formulation as xPos [SDP+22] and RoPE [SLP+21].",
        "Sub-LayerNorm As shown in Equation (8), the retention layer uses Sub-LayerNorm [ WMH+22]",
        "to normalize outputs. Because the multi-scale modeling leads to different variances for the heads, wereplace the original LayerNorm with GroupNorm.",
        "3 ExperimentsWe conduct experiments on language modeling to evaluate RetNet. We evaluate the proposedarchitecture with various benchmarks, i.e., language modeling performance, and zero-/few-shotlearning on downstream tasks. Moreover, for training and inference, we compare speed, memoryconsumption, and latency.",
        "Size Hidden Dim. #Layers Batch Size # Tokens Learning Rate1.3B 2048 24 4M 100B 6√ó10‚àí42.7B 2560 32 4M 100B 3√ó10‚àí46.7B 4096 32 4M 100B 3√ó10‚àí4Table 2: Sizes, and learning hyper-parameters of the models in language modeling experiments.",
        "1.3B 2.7B 6.7BModel Size12.513.013.514.014.515.0Validation PPLRetNetTransformerFigure 5: Perplexity decreases along with scaling up the model size. We empirically observe thatRetNet tends to outperform Transformer when the model size is larger than 2B.",
        "3.1 SetupParameter Allocation We re-allocate the parameters in MSR and FFN for fair comparisons. Let ddenote dmodel for simplicity here. In Transformers, there are about 4d2parameters in self-attentionwhere WQ, WK, WV, WO‚ààRd√ód, and 8d2parameters in FFN where the intermediate dimension is4d. In comparison, RetNet has 8d2parameters in retention, where WQ, WK‚ààRd√ód, WG, WV‚ààRd√ó2d, WO‚ààR2d√ód. Notice that the head dimension of Vis twice Q, K . The widened dimensionis projected back to dbyWO. In order to keep the parameter number the same as Transformer, theFFN intermediate dimension in RetNet is 2d. Meanwhile, we set the head dimension to 256in ourexperiments, i.e., 256for queries and keys, and 512for values. For fair comparison, we keep Œ≥identical among different model sizes, where Œ≥= 1‚àíelinspace(log 1/32,log1/512,h)‚ààRhinstead of thedefault value in Equation (8).",
        "Language Model Training As shown in Table 2, we train language models with various sizes(i.e., 1.3B, 2.7B, and 6.7B) from scratch. The training corpus is a curated compilation of ThePile [ GBB+20], C4 [ DMI+21], and The Stack [ KLBA+22]. We append the <bos> token to indicatethe start of a sequence2. The training batch size is 4M tokens with 2048 maximal length. Wetrain the models with 100B tokens, i.e., 25k steps. We use the AdamW [ LH19 ] optimizer withŒ≤1= 0.9, Œ≤2= 0.98, and weight decay is set to 0.05. The number of warmup steps is 375 with linearlearning rate decay. The parameters are initialized following DeepNet [ WMD+22] to guaranteetraining stability. The implementation is based on TorchScale [ MWH+22]. We train the models with512 AMD MI200 GPUs.",
        "3.2 Comparisons with TransformerLanguage Modeling As shown in Figure 5, we report perplexity on the validation set for thelanguage models based on Transformer and RetNet. We present the scaling curves with three modelsizes, i.e., 1.3B, 2.7B, and 6.7B. RetNet achieves comparable results with Transformers. Moreimportantly, the results indicate that RetNet is favorable regarding size scaling. Besides performance,the RetNet training is quite stable in our experiments. Experimental results show that RetNet is astrong competitor to Transformer for large language models. Empirically, we find that RetNet startsto outperform Transformer when the model size is larger than 2B. We also summarize the languagemodeling results with different context lengths in Appendix B.",
        "2We find that appending the <bos> token at the beginning benefits training stability and performance.",
        "HS BoolQ COPA PIQA Winograd Winogrande SC AvgZero-ShotTransformer 55.9 62.0 69.0 74.6 69.5 56.5 75.0 66.07RetNet 60.7 62.2 77.0 75.4 77.2 58.1 76.0 69.514-ShotTransformer 55.8 58.7 71.0 75.0 71.9 57.3 75.4 66.44RetNet 60.5 60.1 78.0 76.0 77.9 59.9 75.9 69.76Table 3: Zero-shot and few-shot learning with Transformer and RetNet. The model size is 6.7B.",
        "Model SizeMemory (GB) ‚Üì Throughput (wps) ‚ÜëTrm Trm+FlashAttn RetNet Trm Trm+FlashAttn RetNet1.3B 74.8 38.8 34.5 10832.4 63965.2 73344.82.7B 69.6 42.1 42.0 5186.0 34990.2 38921.26.7B 69.0 51.4 48.0 2754.4 16230.1 17458.613B 61.4 46.3 45.9 1208.9 7945.1 8642.2Table 4: Training cost of Transformer (Trm), Transformer with FlashAttention (Trm+FlashAttn), andRetNet. We report memory consumption and training throughput (word per second; wps).",
        "Zero-Shot and Few-Shot Evaluation on Downstream Tasks We also compare the languagemodels on a wide range of downstream tasks. We evaluate zero-shot and 4-shot learningwith the 6.7B models. As shown in Table 3, the datasets include HellaSwag (HS) [ ZHB+19],BoolQ [ CLC+19], COPA [ WPN+19], PIQA [ BZB+20], Winograd, Winogrande [ LDM12 ], and Sto-ryCloze (SC) [ MRL+17]. The accuracy numbers are consistent with language modeling perplexitypresented in Figure 5. RetNet achieves comparable performance with Transformer on zero-shot andin-context learning settings.",
        "3.3 Training CostAs shown in Table 4, we compare the training speed and memory consumption of Transformer andRetNet, where the training sequence length is 8192. We also compare with FlashAttention [ DFE+22],which improves speed and reduces GPU memory IO by recomputation and kernel fusion. In compari-son, we implement RetNet using vanilla PyTorch code, and leave kernel fusion or FlashAttention-likeacceleration for future work. We use chunkwise recurrent representation of retention as described inEquation (7). The chunk size is set to 512. We evaluate the results with eight Nvidia A100-80GBGPUs, because FlashAttention is highly optimized for A100. Tensor parallelism is enabled for 6.7Band 13B models.",
        "Experimental results show that RetNet is more memory-efficient and has higher throughput thanTransformers during training. Even compared with FlashAttention, RetNet is still competitive interms of speed and memory cost. Moreover, without relying on specific kernels, it is easy to trainRetNet on other platforms efficiently. For example, we train the RetNet models on an AMD MI200cluster with decent throughput. It is notable that RetNet has the potential to further reduce cost viaadvanced implementation, such as kernel fusion.",
        "3.4 Inference CostAs shown in Figure 6, we compare memory cost, throughput, and latency of Transformer and RetNetduring inference. Transformers reuse KV caches of previously decoded tokens. RetNet uses therecurrent representation as described in Equation (6). We evaluate the 6.7B model on the A100-80GBGPU in our experiments. Figure 6 shows that RetNet outperforms Transformer in terms of inferencecost.",
        "Memory As shown in Figure 6a, the memory cost of Transformer increases linearly due to KVcaches. In contrast, the memory consumption of RetNet remains consistent even for long sequences,8",
        "Model WeightsRetNetTransformer(a) GPU memory cost of Transformer and RetNet.",
        "RetNetTransformer (b) Throughput of Transformer and RetNet.",
        "Transformer (1024)",
        "Transformer (2048)",
        "Transformer (4096)",
        "Transformer (8192)",
        "RetNet (8192)",
        "(c) Inference latency with different batch sizes.",
        "Figure 6: Inference cost of Transformer and RetNet with a model size of 6.7B. RetNet outperformsTransformers in terms of memory consumption, throughput, and latency.",
        "requiring much less GPU memory to host RetNet. The additional memory consumption of RetNet isalmost negligible (i.e., about 3%) while the model weights occupy 97%.",
        "Throughput As presented in Figure 6b, the throughput of Transformer drops along with thedecoding length increases. In comparison, RetNet has higher and length-invariant throughput duringdecoding, by utilizing the recurrent representation of retention.",
        "Latency Latency is an important metric in deployment, which greatly affects user experience. Wereport decoding latency in Figure 6c. Experimental results show that increasing batch size rendersTransformer's latency larger. Moreover, the latency of Transformers grows faster with longer input. Inorder to make latency acceptable, we have to restrict the batch size, which harms the overall inferencethroughput of Transformers. By contrast, RetNet's decoding latency outperforms Transformers andkeeps almost the same across different batch sizes and input lengths.",
        "3.5 Comparison with Transformer VariantsApart from Transformer, we compare RetNet with various efficient Transformer variants, includingLinear Transformer [ KVPF20 ], RWKV [ PAA+23], H3 [ DFS+22], and Hyena [ PMN+23]. Allmodels have 200M parameters with 16 layers and a hidden dimension of 1024. For H3, we set thehead dimension as 8. For RWKV , we use the TimeMix module to substitute self-attention layerswhile keeping FFN layers consistent with other models for fair comparisons. We train the modelswith 10k steps with a batch size of 0.5M tokens. Most hyperparameters and training corpora are keptthe same as in Section 3.1.",
        "Table 5 reports the perplexity numbers on the in-domain validation set and other out-of-domaincorpora, e.g., Project Gutenberg 2019-2022 (PG22) [ SDP+22], QMSum [ ZYY+21], GovRe-9",
        "Method In-Domain PG22 QMSum GovReport SummScreenRWKV 30.92 51.41 28.17 19.80 25.78H3 29.97 49.17 24.29 19.19 25.11Hyena 32.08 52.75 28.18 20.55 26.51Linear Transformer 40.24 63.86 28.45 25.33 32.02RetNet 26.05 45.27 21.33 16.52 22.48Table 5: Perplexity results on language modeling. RetNet outperforms other architectures on both thein-domain evaluation set and various out-of-domain corpora.",
        "port [ HCP+21], SummScreen [ CCWG21 ,SSI+22]. Overall, RetNet outperforms previous methodsacross different datasets. RetNet not only achieves better evaluation results on the in-domain corpusbut also obtains lower perplexity on several out-of-domain datasets. The favorable performancemakes RetNet a strong successor to Transformer, besides the benefits of significant cost reduction(Sections 3.3 and 3.4).",
        "In addition, we discuss the training and inference efficiency of the compared methods. Let ddenotethe hidden dimension, and nthe sequence length. For training, RWKV's token-mixing complexityisO(dn)while Hyena's is O(dnlogn)with Fast Fourier Transform acceleration. The above twomethods reduce training FLOPS via employing element-wise operators to trade-off modeling capacity.",
        "In comparison with retention, the chunk-wise recurrent representation is O(dn(b+h)), where bisthe chunk size, his the head dimension, and we usually set b= 512 , h= 256 . For either large modelsize (i.e., larger d) or sequence length, the additional b+hhas negligible effects. So the RetNettraining is quite efficient without sacrificing the modeling performance. For inference, among thecompared efficient architectures, Hyena has the same complexity (i.e., O(n)per step) as Transformerwhile the others can perform O(1)decoding.",
        "3.6 Ablation StudiesWe ablate various design choices of RetNet and report the language modeling results in Table 6. Theevaluation settings and metrics are the same as in Section 3.5.",
        "Architecture We ablate the swish gate and GroupNorm as described in Equation (8). Table 6shows that the above two components improve the final performance. Firstly, the gating module isessential for enhancing non-linearity and improving model capability. Notice that we use the sameparameter allocation as Transformers after removing the gate. Secondly, group normalization inretention balances the variances of multi-head outputs, which improves training stability and languagemodeling results.",
        "Multi-Scale Decay Equation (8) shows that we use different Œ≥as the decay rates for the retentionheads. In the ablation studies, we examine removing Œ≥decay (i.e., ‚Äú ‚àíŒ≥decay‚Äù) and applying thesame decay rate across heads (i.e., ‚Äú ‚àímulti-scale decay‚Äù). Specifically, ablating Œ≥decay is equivalenttoŒ≥= 1. In the second setting, we set Œ≥= 127 /128for all heads. Table 6 indicates that both thedecay mechanism and using multiple decay rates can improve the language modeling performance.",
        "Head Dimension From the recurrent perspective of Equation (1), the head dimension implies thememory capacity of hidden states. In the ablation study, we reduce the default head dimension from10",
        "256to64, i.e., 64for queries and keys, and 128for values. We keep the hidden dimension dmodel thesame so the number of heads increases. Experimental results in Table 6 show that the larger headdimension achieves better performance.",
        "4 ConclusionIn this work, we propose retentive networks (RetNet) for sequence modeling, which enables variousrepresentations, i.e., parallel, recurrent, and chunkwise recurrent. RetNet achieves significantly betterinference efficiency (in terms of memory, speed, and latency), favorable training parallelization,and competitive performance compared with Transformers. The above advantages make RetNet anideal successor to Transformers for large language models, especially considering the deploymentbenefits brought by the O(1)inference complexity. In the future, we would like to scale up RetNetin terms of model size [ CDH+22] and training steps. Moreover, retention can efficiently work withstructured prompting [ HSD+22b] by compressing long-term memory. We will also use RetNet as thebackbone architecture to train multimodal large language models [ HSD+22a,HDW+23,PWD+23].",
        "In addition, we are interested in deploying RetNet models on various edge devices, such as mobilephones."
    ]
}
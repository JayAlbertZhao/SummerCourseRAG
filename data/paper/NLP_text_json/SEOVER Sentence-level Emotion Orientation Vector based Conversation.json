{
    "Abstract": "For the task of conversation emotion recog-nition (CER), recent works focus on speakerrelationship modeling but ignore the role ofutterance's emotional tendency. In this pa-per, we propose a new expression paradigmof sentence-level emotion orientation vector tomodel the potential correlation of emotions be-tween sentence vectors. Based on it, we de-sign an emotion recognition model referredto as SEOVER, which extracts the sentence-level emotion orientation vectors from the pre-trained language model and jointly learns fromthe dialogue sentiment analysis model and ex-tracted sentence-level emotion orientation vec-tors to identify the speaker's emotional orien-tation during the conversation. We conductexperiments on two benchmark datasets andcompare them with the five baseline models.The experimental results show that our modelhas better performance on all data sets. Addi-tional experiments further show that the signif-icant role of the proposed sentence-level emo-tion orientation vector.",
    "Keywords": null,
    "Body": [
        "IntroductionConversation emotion recognition (CER) refersto the process of identifying the speaker's emo-tions through text, audio, and visual information inthe process of two or more persons' conversations.",
        "Nowadays, conversation emotion recognition tasksare widely used in social media such as Twitter andFacebook.",
        "Recent work of conversation emotion recogni-tion mainly focuses on speaker identification andconversation relationship modeling (Majumder etal. 2019; Ghosal et al. 2019). However, theseproposals use CNN (Kim,2014) to encode the utter-ances, which can not express the grammatical andsemantic features of the utterance well and lead toinaccurate emotion identification. To mitigate thisissue, some works try to employ the BERT (Devlinet al. 2019) model with improved semantic featuresextraction ability to encode the sentences (YuzhaoMao et al. 2020; Jiangnan Li et al. 2020). The pro-posal achieves good results, however, the proposedBERT-based CER does not fully extract the cor-relation of emotional tendency between sentencesespecially when the emotion turns in sudden. Asshown in Figure 1, we encode the first sentence ofthe first dialogue of the MELD dataset with CNNand BERT models respectively to obtain a 600-dimensional vector, and normalize the data of eachdimension so that its value is a continuous numberwithin (0,1). Then, a hot zone map is leveragedto show the difference between the feature mapsencoded with the two models. It is obvious thattrends of the feature maps are totally opposite. Itseems that those encoding methods and correspond-ing feature maps lose some important features ofthe sentences and can't represent the emotionaltendency of the utterance well. Based on this, wepropose a new utterance representation vector re-ferred to as the sentence-level emotion orientationvector (SEOV) to further represent the potentialemotion correlation of sentences. The SEOV repre-sents the emotional intensity of sentence encodingvector with its \"size\", and models the emotion ten-dency between vectors with its \"direction\", whichis as shown in Figure 2.",
        "Based on the proposed SEOV , we further pro-pose a sentence-level emotion orientation vectorbased conversation emotion recognition model(SEOVER) to encode and decode the utterancesof dialogue, and get the emotions of each speaker.",
        "In the model, we employ an improved transformercalled as transformer-emo to extract the SEOV andthen jointly use the SEOV and dialogue sentimentanalysis model (DSAM) to obtain the contextualsemantic information. With continuous fine-tune,we finally get the speaker's emotion classificationresult.",
        "In general, our contributions are summarized asfollows:",
        "Figure 1: (a) Comparison of the value of each dimension of the sentence vector get by CNN and BERT. (b) Hotzone map of features. The shade of the color indicates the size of the value, the larger the value, the darker thecolor.",
        "1.We propose a new emotional feature extrac-tion paradigm called sentence-level emotional ori-entation vector (SEOV) to represent the potentialtendency and correlation of emotions between sen-tences.",
        "2.Based on proposed SEOV , we design a corre-sponding SEOV based emotion recognition model(SEOVER) including entail encoding and decodingprocess to recognize speaker's potential emotion.",
        "3.We compare three conversation emotion recog-nition models (RNN, GCN and LSTM) as thesecond-level fine-tuned emotion recognition modelto prove the feature extraction accuracy of the pro-posed SEOV . Then, comparative experiments onboth the IEMOCAP (Busso et al. 2008) and MELD(Poria et al. 2019) data sets are conducted. Theresult shows better performance of our proposalcompared with state-of-the-art CER algorithms (Di-alogueRNN, DialogueGCN, DialogXL, TRMSM,bc-LSTM and BERT).",
        "The rest of the paper is organized as follows:",
        "Section 2 discusses related work; Section 3 pro-vides detailed description of our model; Sections4 and 5 present the experimental results; finally,Section 6 concludes the paper.",
        "2 Related WorkIn this section, we investigate the related works ofCER and list the state-of-the-art algorithms.",
        "2.1 Emotion analysisEarly sentiment analysis works only use a singledata format, such as text emotion analysis (Strap-Figure 2: The schematic diagram of SEOV , where thex-axis represents the dimension of the vector, y-axisrepresents the value of each dimension of the vector,and z-axis represents different sentence-level emotionorientation vectors. There is a correlation between dif-ferent emotion vectors, indicating the \"direction\" of theSEOV .",
        "parava et al. 2004), and speech emotion analysis(Maghilnan S et al. 2018), while recent studiesconsider using multimodal data sets to build emo-tion analysis models (Tsai et al. 2019), but theyonly consider the emotion in single utterance andignore the emotion correlation of context in theconversation process.",
        "2.2 Emotion analysis in conversationHazarika et al. (2018b) propose the CMN model,which use a GRU structure to store multi-modaldata information, and consider the role of contex-tual information in conversation emotion recogni-tion. ICON (Hazarika et al. 2018b) is an extensionof CMN, which contains another GRU structure toconnect the output in the CMN model to distinguish2",
        "Figure 3: Our model's structurethe speaker relationship. Majumder et al. (2019)",
        "use three GRUs to obtain context information andupdate the speaker status. Ghosal et al. (2019)",
        "construct a conversation into a graph, then use agraph convolutional neural network to convert theemotion classification task of the conversation intoa node classification problem of the graph. Ghosalet al. (2020) use common sense knowledge to learnthe interaction of interlocutors. Limited by theGRU, GCN and other model structures, these mod-els can't effectively obtain long-distance contextualinformation.",
        "2.3 Pretrained language modelsInspired by the self-attention mechanism (Ben-gio et al. 2014), the transformer is proposed forcomputing representations and efficiently obtain-ing long-distance contextual information withoutusing sequence (Vaswani et al. 2017). Devlin etal. (2019) use the Transformer structure to train alarge-scale general-purpose text corpus to obtaina language model with syntactic and semantic in-formation, which can be used for downstream NLPtasks after fine-tuning. Bao et al. (2020) and Zhu etal. (2020) use pre-trained language models for con-versation tasks, but not for conversation emotionrecognition. By employing transformer based pre-training, Hazarika et al. (2020) transfer the context-level weight of the generated conversation model tothe conversation emotion recognition model. Jiang-nan Li et al. (2020) propose to use the dependencyrelationship of intra-speaker and inter-speaker tosimplify modeling, so as to obtain a longer-termcontext relationship. Yuzhao Mao et al. (2020)useTransformer to explore differentiated emotional be-haviors from the perspective of within and betweenmodels. Weizhou Shen et al. (2020) use the XL-DatasetParameter settingsepochs lr dr batch size classes dimMELD 50 0.0005 0.1 30 7 600IEMOCAP 60 0.0001 0.1 30 6 100Table 1: Parameter settings of MELD dataset andIEMOCAP dataset. lr represents the learning rate, drrepresents the dropout rate, classes represents the num-ber of types of emotion classification, and dim repre-sents the dimension of the text vector input.",
        "Net model for conversation emotion recognition(DialogXL) to obtain longer-term contextual in-formation, which is the state-of-the-art algorithmfor CER. The above-mentioned algorithms focuson distinguishing the speaker relationship and ob-taining contextual information, but these methodscannot well represent the grammatical and seman-tic information of the utterances.",
        "3 MethodologyIn the task of CER, existing methods directly en-code sentences to a sentence vector for reflectingthe features of utterance's emotion. From the per-spective of the dimensions of the vector, each di-mension of the sentence vector represents the direc-tion of the sentence vector in space, but we cannotintuitively understand the meaning of these \"direc-tions\" and its relation with utterance's emotions. Inorder to clearly map the tendency of emotions insentence vectors, we propose a new vectors collec-tion called as sentence-level emotion orientationvector (SEOV) to construct the emotion connectionbetween the sentences vectors. As shown in Fig.",
        "2, the SEOV maps the different emotion levels ineach vector and reflect the emotion tendency withthe \"direction\" between SEOVs. More profoundly,3",
        "we design a new SEOVs based conversation emo-tion recognition model referred to as SEOVER. Inthe proposed model, the sentence-level encoderat first map the input data to the sentence vectorby transformer-emo. Then the PCA algorithm isemployed to extract the emotions of the sentencevector to get the compressed emotion vector. Then,the SEOV is constructed by fusing the sentencevector and compressed emotion vector. Finally, theSEOV is fed into the DSAM to obtain contextualinformation thus get the result of emotion classifi-cation.",
        "3.1 Problem DefinitionGiven a conversation U:U1; U2; U3; :::U n,Nis thenumber of conversations, the speaker's utteranceis represented by the function Pt(Ui), where tisthe t-th speaker. Our task is to input each utteranceUiand to get its correct classification result in theemotional label set L:l1; l2; l3; :::l m, where Misthe number of types of emotional label.",
        "3.2 ModelAs shown in the Figure 3, our model is divided intothree parts: Sentence-level encoder, emotion-levelencoder and context modeling.",
        "Sentence-level Encoder: Since the pre-trainedlanguage model can't directly encode the dialogue,we at first split the conversation Uninto a seriesof single sentences : s1; s2; s3:::sn. Then, we inputthem into an improved pre-trained language modelcalled transformer-emo model, which can map therepresentation of utterance to the sentence vectorsQ:",
        "Q=Transformer \u0000emo(s1; s2; s3:::s n)(1)",
        "where Qis a series of sentence vectors:q1; q2; q3:::qn, the length of each sentence vector isset as d(dis set to 786 in our experiment). The clas-sical pre-trained language model BERT is proposedwith transformer structure to train the general textcorpus (Devlin et al., 2019). Compared with theCNN model (Kim,2014), transformer can obtainmore syntactic and semantic information. In orderto better adapting to our model, we improve thetransformer model by adjusting the output form (re-ferred to as transformer-emo) to obtain the sentencevectors of the utterance.",
        "Emotion-level Encoder: As for sentence vectors,the length of the sentence itself, semantic informa-tion, symbols, etc. are the \"size\" of the vector, andits classification attributes are the \"direction\" of thevector. The sentence representation obtained by en-coding the sentence in the existing method is not a\"vector\" in the true sense, because it only containsthe \"size\" without the \"direction\". So we propose anew expression paradigm, SEOV , which representsthe emotional intensity of sentence vector with its\"size\", and models the emotion tendency betweenemotion vectors with its \"direction\".",
        "We map the vector qfrom the k-dim space to thek*-dim space to obtain the emotion vector q\u0003, toachieve emotions representation:",
        "where w1; w2; : : : ; w k\u0003is the weight parameter. Intheory, when the spatial dimension after the map-ping is equal to the number of categories, we canregard it as the classification result. The elementsin the emotion vector q\u0003represent the probabilitiesof classified emotions in each sentence.",
        "Then we merge the original vector qand theobtained emotion vector q\u0003to obtain the sentence-level emotion orientation vector e:",
        "Theeis called as SEOV , in which, the emotion'sdirection is represented in the collection of SEOVsas shown in fig.2. With the final DSAM, the â€œdirec-tion\" of the emotion tendencies can be efficientlyextracted and achieves better emotion recognitionperformance.",
        "Context Modeling: Finally, we reassembleSEOVs into dialogue lists, and input them intothe DSAM. The existing DSAMs can distinguishthe speaker and obtain the speaker state, so it canfully obtain the context information and context.",
        "We choose DialogueRNN, DialogueGCN, and bc-LSTM models as the benchmark version of thedialogue emotion analysis models to compare thetest results.",
        "4 Experimental Setting4.1 DatasetsIEMOCAP (Busso et al. 2008): IEMOCAPdataset contains the conversation data of ten ac-tors in the emotional interaction process, includingvideo, voice, facial expression capture, and con-versation text. The emotions are classified intosix types of emotions: happy, sad, neutral, angry,excited, frustrated.",
        "MELD (Poria et al. 2019): MELD dataset selects1432 conversations from the TV series \"Friends\",with a total of 13,708 sentences, including video,text, voice and other data content. Emotions areclassified into seven emotions: neutral, surprise,fear, sadness, joy, disgust, and angry.",
        "4.2 State-of-the-Art BaselinesDialogueRNN (Majumder et al. 2019): Dia-logueRNN uses different GRU units to obtain con-textual information and speaker relationships. It isthe first conversation sentiment analysis model todistinguish between speakers.",
        "DialogueGCN (Ghosal et al. 2019): Dia-logueGCN constructs a conversation into a graph,transforms the speech emotion classification prob-lem into a node classification problem of the graph,and uses the graph convolutional neural network toclassify the results.",
        "DialogXL (Weizhou Shen et al. 2020): DialogXLuse XLNet model for conversation emotion recog-nition to obtain longer-term contextual information.",
        "Bc-LSTM (Poria et al. 2017): Bc-LSTM usesa two-way LSTM structure to obtain contextualsemantic information, but does not distinguish be-tween speaker states.",
        "TRMSM (Jiangnan Li et al. 2020): TRMSM usesthe transformer structure to simplify the conver-sation relationship into Intra-Speaker and Inter-Speaker, which can solve the problem of long-distance context dependence.",
        "BERT (Devlin et al. 2019): BERT is a pre-trainedlanguage model that can be fine-tuned to achievegood results in downstream tasks. In this article,we use BERT to classify the sentiment of a singlesentence text and compare it with our model.",
        "4.3 ImplementationFor Transformer-emo, an improved uncased BERT-based model is adopted. For the MELD dataset,the epochs is set to 50, the learning rate is set to0.0005, and the dropout rate is set to 0.1; for theIEMOCAP dataset, epochs is set to 60, the learningrate is set to 0.0001, and the dropout rate is setto 0.1. In order to make a reasonable comparisonwith the existing DialogueRNN and DialogueGCN,the dimension of the SEOV is reduced to the same5",
        "Confusion Matrixneutral surprise fear sad joy disgust angerneutral 3781 164 30 176 428 33 135surprise 28 134 1 20 52 4 41fear 14 6 1 6 8 5 10sad 44 20 3 68 28 7 38joy 71 27 2 14 272 2 14disgust 17 6 4 12 9 9 11anger 55 36 5 29 30 18 165Table 4: Confusion matrix obtained on the test set using the MELD dataset and DialogueRNN as the benchmarkmodelModel Accuracy F1 scoreDialogueRNN 56.10 55.90DialogueRNN-BERT 58.59 59.40SEOVER-RNN 63.32 63.86Table 5: Results of ablation experiments, whereaccuracy and F1 score are both weighted results.",
        "DialogueRNN-BERT represents the result obtained byusing the BERT model to get the sentence vector with-out adding the emotion vector.",
        "to the benchmarks, which is 600-dimensional ofthe MELD dataset, and 100-dimensional of theIEMOCAP dataset. The detailed parameter settingis shown in Table 1.",
        "5 Results and AnalysisWe compare our model with all baselines on theMELD and IEMOCAP datasets and get the experi-mental results in Tables 2 and 3. As expected, ourproposal outperforms other baseline models.",
        "5.1 Comparison with conversation emotionanalysis modelsCompared with the state-of-the-art conversationemotion recognition models, our proposal achievesbetter performance. This is cause by two reasons.",
        "Firstly, our model can obtain more syntactic and se-mantic features of utterances' presentation by usingtransformer-emo which is a transformer based pre-training model. Secondly, the proposed SEOV canefficiently map the emotion orientations betweensentence vectors.",
        "5.2 Ablation StudyCompared with the conventional BERT modelwhich only encodes the context information, it iseasy to find that the emotion tendency should alsobe encoded to reflect the emotion tendency of the ut-terance. The classification accuracy with the SEOVcontaining emotion tendencies is higher than theones with only sentence vectors.",
        "In order to study the influence of the fusion ofSEOV on the experimental results, we remove theemotion vectors in the SEOVER to compare theemotion recognition performance with benchmarkmodel of DialogueRNN on the MELD dataset. Asshown in Table 5, unsurprisingly, the accuracy andF1 score of the model without emotion vectorsare much lower. The comparison illustrates theimportance of the emotion tendency encoding inSEOV for the conversation emotion recognition.",
        "5.3 Error AnalysisWe analyze the confusion matrix on the MELDdata set. As shown in Table 4, assuming \"sad\" and\"joy\" are opposite emotions, the count of the mis-judge between \"sad\" an \"joy\" is relatively small.",
        "On the other hand, the \"sad\" and \"angry\" are a pairof adjacent emotions. The misjudge count of thispair adjacent is relatively high. It is not difficult tofind that the fusion of emotion vectors is a double-edged sword, which improves the classificationaccuracy of flipped emotions, but slightly reducedthe classification accuracy of adjacent emotions.",
        "Therefore, in future research, we will focus on im-proving the performance of the adjacent emotionsclassification.",
        "On the other hand, similar to the method pro-posed by Ghosal et al. (2019), our model only usestext data as input. However, the emotion of theutterance is highly expressed by other informationsuch as talking speed and intonation. When emo-tions are flipped, if the text cannot provide enoughuseful information, changes in facial expressionsand voice can provide very useful information for6",
        "recognition. The MELD and IEMOCAP datasetsalso provide multi-modal data such as speech andvision, visual expression capture and voice signalswhich gives potential applications of joint multi-modal based recognition. We will try to employmulti-modal information to the emotion recogni-tion with our proposed model.",
        "6 ConclusionIn this work, we propose a new paradigm ofsentence-level emotion orientation vector(SEOV)",
        "to assist the emotion recognition, which solves thediscourse representation information loss problemof conventional methods in the CER. Then, we de-signed a conversation emotion recognition modelbased on the SEOV called as SEOVER. It uses theTransformer-emo model to encode sentence vec-tors and emotion vectors containing emotion ten-dency information and fuses them as SEOV . Then,the SEOV is leveraged as input to active the finaldialogue emotion analysis model to classify thespeaker's emotions. We conducted comparativeexperiments of the proposal with several bench-marks on both the MELD and IEMOCAP dataset."
    ]
}
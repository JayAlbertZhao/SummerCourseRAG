{
    "Abstract": "This paper introduces a novel deep metric learning-basedsemi-supervised regression (DML-S2R) method for param-eter estimation problems. The proposed DML-S2R methodaims to mitigate the problems of insufficient amount of la-beled samples without collecting any additional samples wi thtarget values. To this end, the proposed DML-S2R method ismade up of two main steps: i) pairwise similarity modelingwith scarce labeled data; and ii) triplet-based metric lear ningwith abundant unlabeled data. The first step aims to modelpairwise sample similarities by using a small number of la-beled samples. This is achieved by estimating the target val uedifferences of labeled samples with a Siamese neural networ k(SNN). The second step aims to learn a triplet-based metricspace (in which similar samples are close to each other anddissimilar samples are far apart from each other) when thenumber of labeled samples is insufficient. This is achievedby employing the SNN of the first step for triplet-based deepmetric learning that exploits not only labeled samples but a lsounlabeled samples. For the end-to-end training of DML-S2R,we investigate an alternate learning strategy for the two st eps.Due to this strategy, the encoded information in each stepbecomes a guidance for learning the other step. The exper-imental results confirm the success of DML-S2R comparedto the state-of-the-art semi-supervised regression metho ds.The code of the proposed method is publicly available athttps://git.tu-berlin.de/rsim/DML-S2R .Index Terms —Semi-supervised regression, parameterestimation, metric learning, deep learning.1",
    "Keywords": null,
    "Body": [
        "INTRODUCTIONThe accurate estimation of parameters from specific data(e.g., estimation of carbon monoxide concentration in the e n-vironment, estimation of forest parameters, estimation of glu-cose concentration in diabetic patients, etc.) is an import antresearch field in machine learning and pattern recognition [ 1].",
        "The use of regression methods, which aim at learning func-tional relations between a set of variables and correspondi ngtarget values, is an effective way for parameter estimationproblems. Accordingly, several regression methods (e.g.,random forest regression [2], support vector regression [1 ],deep learning based regression [3, 4], etc.) have been intro -duced in the literature. The success of these methods depend son the quantity and quality of training samples for which therespective target values are available (i.e., labeled samp les).",
        "The quantity of training samples depends on the numberof available labeled samples, while the quality of trainingsamples depends on their capability to represent the real sa m-ple distribution. A small amount of labeled samples withinsufficient quality can lead to inadequate modeling of theregression task. However, collecting labeled samples is of -ten expensive and complex as producing reference measuresmay require significant time [1]. To address this issue, semi -supervised regression (SSR) methods, which aim at jointlyusing the information of both labeled and unlabeled samplesin the learning phase of the regression algorithm, have beenintroduced [5–7].",
        "In recent years, deep metric learning (DML) has been uti-lized in the framework of SSR. DML operates on sampletuples (e.g., triplets) to learn a metric space, in which sim -ilar samples are mapped close to each other and dissimilarsamples are mapped apart from each other. In [8], a metric-learning based SSR method that utilizes a Siamese neural net -work (SNN) with contrastive loss function is introduced tolearn a pairwise metric space. In this method, dissimilar an dsimilar pairs, which are constructed from both labeled and u n-labeled samples, are utilized for DML. To define sample sim-ilarity, the absolute differences of target values are expl oitedwith a thresholding strategy for labeled samples. The simil ar-ity of unlabeled samples to labeled samples is estimated bas edon Euclidean distances between data points, for which thres h-olding is applied. Once the SNN is trained with dissimilar an dsimilar sample pairs, the target value estimation of sample sis performed on the learnt metric space by k-nearest neigh-bors (k-NN) algorithm. This method only considers the pair-wise similarities, which may not be sufficient in accuratelylearning a metric space. In [9], a SSR method that exploits along short-term memory (LSTM) based SNN is proposed forcontinuous emotion recognition. This method applies a two-stage training. In the first stage, the SNN is trained with log -ratio loss function (which operates on sample triplets) by u s-ing only labeled samples. Then, the SNN is used to generatepseudo-labels for unlabeled samples. In the second stage, t heSNN is trained with mean squared error loss function by us-",
        "ing pseudo-labels. This method utilizes only labeled sampl esfor learning metric space in the first stage. This may lead toinaccurate characterization of sample similarities in the caseof availability of small-sized labeled training sets. To ad -dress the limitations of above-mentioned methods, we intro -duce a novel DeepMetric Learning-based Semi-SupervisedRegression (DML-S2R) method for parameter estimation.",
        "2. PROPOSED DML-S2R METHODLetS={(xli,yli)}Ni=1be a set of Nlabeled samples, wherexliis theith labeled sample and yliis its corresponding tar-get value. Let U={xuj}Mj=1be the set of Munlabeledsamples, where xujis thejth sample, for which the corre-sponding target value is unknown. The training set Tcon-sists of the labeled sample set Sand the unlabeled sample setU(T=S∪U ). We assume that Nis significantly lower thanM(N<<M ).",
        "The proposed DML-S2R method aims to learn a metricspace, in which similar samples are located close to eachother, by effectively exploiting abundant unlabeled data t o-gether with scarce labeled data. This is achieved by two mainsteps: i) pairwise similarity modeling with scarce labeleddata; and ii) triplet-based metric learning with abundant u n-labeled data. For the end-to-end training of DML-S2R, weinvestigate an alternate learning strategy for the two step s.",
        "Fig. 1 shows a general overview of the proposed method,which is explained in detail in the following subsections.",
        "2.1. Pairwise Similarity Modeling with Scarce LabeledDataThe first step aims to model the pairwise sample similarity byusing a small amount of labeled samples. For this purpose,one could exploit contrastive loss function that requires t heselection of similar and dissimilar pairs based on the targe tvalues of training samples. However, in the framework of re-gression problems, it is challenging to define the boundarie sbetween the target values of similar/dissimilar samples [1 0].",
        "To overcome this problem, we redefine the regression prob-lem. Instead of learning a function that estimates the targe tvalue of each sample, DML-S2R learns a function fdwhichestimates the target value differences of two labeled sampl esas:",
        "fd(xli,xlj) =yli−ylj. (1)",
        "While learning to estimate target value differences, DML-S2R is also enforced to model the sample similarity for theaccurate prediction of the differences. Due to this definiti on,instead of using Nsamples in the first step, proposed DML-S2R exploits N(N−1)pairs of samples (i.e., all the possiblepairs from labeled sample set). This mitigates the limitati onsof labeled data scarcity, and thus over-fitting. For fd, we uti-lize a SNN, which contains two identical sub-networks withshared weights. Two sub-networks of the SNN takes an inputLPSMxli SNNfd(xli,xlj)",
        "xljPxliSNNxli SNNSNNSNNNxliLPLNLRRLFirst StepSecond StepFig. 1 : Illustration of the proposed DML-S2R method.",
        "pair and provides the sample features, which are concatenat edto form the feature associated to the input pair. Then, thisfeature is fed into a fully connected layer (FC) that directl yestimates the target value differences of sample pairs. Let zijbe the target value difference of xliandxljwhilei/ne}ationslash=j. Tolearn the model parameters of fd, we propose the pairwisesimilarity modelling loss function LPSMas follows:",
        "LPSM=1N(N−1)N/summationdisplayi=1N/summationdisplayj=11[i/\\egatio\\slash=j](zij−fd(xli,xlj))2,(2)",
        "where1is the indicator function. Once the model parametersof the SNN is learnt with (2), its feature space, which encode sthe pairwise sample similarity, forms the basis for the seco ndstep.",
        "2.2. Triplet-Based Metric Learning with Abundant Unla-beled DataThe second step aims to learn a metric space (where similarsamples are located close to each other) when the number oflabeled samples is limited. This is achieved by triplet-bas edDML that takes into account not only labeled samples butalso unlabeled samples. Triplet-based DML requires sampletriplets for the characterization of a metric space. Accord -ingly, we convert the SNN of the first step to the triplet-base dSNN by replicating one of its identical sub-networks threetimes without changing weights, and thus make it appropriat efor triplet-based DML. A standard triplet consists of a sam-ple anchor and a positive sample, which is similar to the an-",
        "Algorithm 1: Positive-negative set selection of ananchor for the proposed DML-S2R methodInput :xla,U,fd,kOutput:Pxla,Nxla1Pxla=∅,Nxla=∅2while|Pxla| ≤kdo3argminxui∈U\\Pxlafd(xla,xui)→Pxla(Positive set selection)",
        "4end5while|Nxla| ≤kdo6argmaxxui∈U\\Nxlafd(xla,xui)→Nxla(Negative set selection)",
        "7end8returnPxla,Nxlachor, and a negative sample, which is dissimilar to the an-chor. For the construction of sample triplets, anchor sampl esare selected from the labeled set Sand positive and negativesamples are selected from the unlabeled set U. To effectivelyexploit abundant unlabeled data, we create a set of positiveand negative samples per anchor that leads a faster conver-gence compared having only one positive and one negativesample per anchor [11]. For an anchor xla∈ S, the set of kpositive samples Pxlaand the set of knegative samples Nxlaare selected with fdfrom the first step based on their esti-mated target value differences with the anchor. In detail, a llthe possible pairs between the anchor xlaand each unlabeledsamplexui∈ U are created. Then, the target value differenceof each pair is estimated by using fdfrom the first step. Thekunlabeled samples having the smallest difference with theanchor are selected for Pxla, while the kunlabeled sampleshaving the highest difference with the anchor are selected f orNxla. The positive-negative set selection procedure is shownin Algorithm 1. After the selection of the positive and neg-ative sets for all anchor samples, we employ the ranked listloss function [11] as follows:",
        "w(xla,xuj) =exp(τ(d(xla,xuj)−(α−m))),LP(xla,S) =/summationdisplayxuj∈Sw(xla,xuj)/summationtextxuj∈Sw(xla,xuj)Lm(xla,xuj),LRLL=12NN/summationdisplayi=1LP(xla,Pxla)+LP(xla,Nxla),(3)",
        "whereτis the temperature parameter, αis the negative sam-ple boundary, mis the margin parameter, dmeasures the Eu-clidean distance between two samples in the feature space an dLmis the margin loss function. LRLLpulls a set of positivesamples closer than the set of negatives by the margin monthe feature space of the SNN.",
        "It is worth noting that the effectiveness of each step inDML-S2R depends on each other. Inaccurate learning of fdin the first step leads to incorrect selection of positive-ne gativesets in the second step. If the metric space is not accuratelylearned in the second step, the weights of the SNN can not beeffectively learnt in the first step due to a small number of la -beled samples. This prevents to utilize standard joint lear ningstrategies for DML-S2R. To accurately learn both steps, weinvestigate an alternate learning strategy, in which the SN N istrained for both steps within the consecutive training epoc hs.",
        "In detail, the whole learning procedure starts with trainin gthe SNN one epoch for the first step while minimizing LPSM.",
        "Then, the SNN is trained one epoch with all the anchor sam-ples and the associated positive-negative sets while minim iz-ingLRRL. Training continues while alternating between thetwo steps until convergence of both loss functions. Once thetraining of DML-S2R is completed, the target value estima-tion of a new sample is achieved based on fdas follows:",
        "y∗=1NN/summationdisplayi=1fd(x∗,xli)−fd(xli,x∗)",
        "3. EXPERIMENTAL RESULTSExperiments were conducted on Boston Housing [12], Super-conductivity [13] and Air Quality [14] datasets associated tothe regression problems of housing value estimation, criti caltemperature estimation of a superconductor and benzene es-timation for pollution monitoring, respectively. The Bost onHousing dataset includes 506 samples, each of which is asso-ciated with 13 variables. We randomly selected 200 samplesto construct the unlabeled sample set. The Superconduc-tivity dataset includes 21263 samples, while each sample isassociated with 81 variables. 1000 samples were randomlychosen to construct the unlabeled sample set. The Air Qualit ydataset consists of 9358 samples, each of which is associate dwith 14 variables. We randomly selected 1000 samples forthe construction of the unlabeled set. In the experiments, t henumber of labeled samples is varied as |S|= 10,20,50for alldatasets. The rest of samples, which were not selected to thelabeled and unlabeled sets, was used as the test set for eachdataset. All the samples from three datasets were normalize dby using min-max normalization. In the experiments, twohidden layers with 100 neurons in each were used for theSNN of DML-S2R. We trained our method for 30 epochsby using the Adam optimizer with the initial learning rate of0.001. The results are provided in terms of mean absoluteerror (MAE). To perform the ablation study of the proposedDML-S2R method, we compared it with only using the firststep of DML-S2R on the Superconductivity dataset. To as-sess the effectiveness of the proposed DML-S2R method,we compared it with the cotraining-style SSR algorithm(COREG) [15] and metric-based SSR (denoted as MSSR)",
        "method [8]. Table 1 shows the regression performances onthe Superconductivity dataset obtained when: i) only the fir ststep of the proposed DML-S2R method is applied; and ii)",
        "Table 1 : Mean absolute error (MAE) scores when the dif-ferent steps of the proposed DML-S2R method were utilized(Superconductivity dataset).",
        "Steps of DML-S2R Number of Labeled Samples1st2nd10 20 50 100✓ ✗ 23.5 22.8 18.2 17.2✓ ✓ 22.3 18.2 16.2 15.8Table 2 : Mean absolute error (MAE) scores obtained byCOREG, MSSR and the proposed DML-S2R method (Super-conductivity dataset).",
        "MethodNumber of Labeled Samples10 20 50COREG [15] 23.1 20.5 17.2MSSR [8] 22.4 21.2 18.1DML-S2R (Ours) 22.3 18.2 16.2both steps of DML-S2R are applied. By analyzing the table,one can see that utilizing both steps of DML-S2R results insignificantly higher accuracies than using only the first ste pof DML-S2R with different numbers of labeled samples. Asan example, DML-S2R achieves a MAE of 18.2 with 20labeled samples, whereas using only the first step of DML-S2R results in a MAE of 22.8 with the same number labeledsamples. This is due the fact that in the second step of DML-S2R, unlabeled samples are effectively exploited to learn adeep metric space that leads to more accurate target valueestimation. This also shows the effectiveness of alternatelearning strategy for learning both steps. Tables 2-4 showsthe regression performances of COREG, MSSR and proposedDML-S2R with different number of labeled samples for allthe datasets. One can see from the tables that our method pro-vides the highest accuracies for all datasets. As an example ,DML-S2R achieves a MAE of 3.3 with 50 labeled samples,whereas COREG provides a MAE of 9.5 and MSSR achievesa MAE of 12.7 with the same number of labeled samples forthe Superconductivity dataset. This is due to the fact that o urmethod effectively models the pairwise similarity of sampl esby using scarce labeled data, while accurately learning a de epmetric space by exploiting labeled and unlabeled samplestogether.",
        "4. CONCLUSIONIn this paper, we have presented a novel deep metric learning -based semi-supervised regression (DML-S2R) method for pa-rameter estimation problems. DML-S2R includes two con-secutive steps. The first step aims at modelling the pairwisesimilarities of samples when the labeled data is scarce. Thi sis achieved by learning to estimate the target value differ-ences of the labeled sample pairs with a SNN. Due to thisTable 3 : Mean absolute error (MAE) scores obtained byCOREG, MSSR and the proposed DML-S2R method (BostonHousing dataset).",
        "MethodNumber of Labeled Samples10 20 50COREG [15] 8.2 7.9 5.1MSSR [8] 6.5 5.6 4.9DML-S2R (Ours) 6.1 5.4 4.5Table 4 : Mean absolute error (MAE) scores obtained byCOREG, MSSR and the proposed DML-S2R method (AirQuality dataset).",
        "MethodNumber of Labeled Samples10 20 50COREG [15] 12.4 11.9 9.5MSSR [8] 17.9 17.3 12.7DML-S2R (Ours) 10.9 6.0 3.3step, the proposed DML-S2R method overcomes the chal-lenges of defining pairwise similar/dissimilar samples bas edon the target values of labeled samples in the framework of re -gression problems. The second step aims at learning a metricspace that utilizes not only labeled samples but also unlabe ledsamples to further enrich modelling sample similarities wi thabundant unlabeled data. This is achieved by employing theSNN of the first step for triplet-based DML where positive-negative samples in each triplet are defined from unlabeledsamples based on the SNN. The effectiveness of each stepdepends on each other. Accordingly, for the whole learningprocedure of DML-S2R, we investigate an alternate learningstrategy, in which the SNN is trained for both steps throughconsecutive training epochs. Due to this strategy, the enco dedinformation by the SNN in each step becomes a guidance forlearning the other step. Experimental results show the effe c-tiveness of proposed DML-S2R compared to state-of-the-artSSR methods. It is worth noting that DML-S2R is indepen-dent from the type of the SNN architecture, and thus suitableto be integrated in any Siamese-based deep architecture. Asa future work, we plan to apply DML-S2R to the parameterestimation problems in image domain (e.g., age prediction,emotion recognition, etc.)."
    ]
}
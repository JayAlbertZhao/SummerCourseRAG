{
    "Abstract": "Causal Emotion Entailment (CEE) aims todiscover the potential causes behind an emo-tion in a conversational utterance. Previousworks formalize CEE as independent utter-ance pair classification problems, with emo-tion and speaker information neglected. Froma new perspective, this paper considers CEEin a joint framework. We classify multipleutterances synchronously to capture the cor-relations between utterances in a global viewand propose a Two-Stream Attention Model(TSAM) to effectively model the speaker'semotional influences in the conversational his-tory. Specifically, the TSAM comprises threemodules: Emotion Attention Network (EAN),Speaker Attention Network (SAN), and inter-action module. The EAN and SAN incor-porate emotion and speaker information inparallel, and the subsequent interaction mod-ule effectively interchanges relevant informa-tion between the EAN and SAN via a mu-tual BiAffine transformation. Extensive ex-perimental results demonstrate that our modelachieves new State-Of-The-Art (SOTA) perfor-mance and outperforms baselines remarkably.",
    "Keywords": null,
    "Body": [
        "IntroductionWith the recent proliferation of open conversationaldata on social media platforms, such as Twitterand Facebook, Emotion Analysis in Conversations(EAC) has become a popular research topic in thefield of Natural Language Processing (NLP). Mostof the existing works on EAC mainly focus on Emo-tion Recognition in Conversations (ERC), i.e., rec-ognizing emotion labels of utterances (e.g., happy,sad, etc.) (Poria et al., 2017, 2019b; Wang et al.,2020; Zhang et al., 2020). However, Poria et al.",
        "(2021) point out that these studies lack further rea-soning about emotions, such as understanding the\u0003This work was done when Duzhen Zhang was interningat Pattern Recognition Center, WeChat AI, Tencent Inc, China.",
        "yCorresponding author.",
        "1, SA2, SBYou know I am getting married!Wow! That's great news. Who is the lucky person?excitedhappy1, SA2, SBI  like winter.Me too.happyneutral3, SAIt's snowing heavily. What about taking a  walk?happy4, SBThat's a good idea. Let's go!happy5, SAhappyWhat a heavy snow! Look! The water is  frozen.a)",
        "b)Figure 1: Example conversations sampled from thebenchmark dataset (Poria et al., 2021)",
        "stimuli and the cause of the emotion. Since Rec-ognizing Emotion Cause in Conversations (REC-CON) holds the potential to improve the inter-pretability and performance of affect-based mod-els, Poria et al. (2021) put forward a new promisingtask, named RECCON, which includes two differ-ent sub-tasks: Causal Span Extraction (CSE) atword/phrase level and Causal Emotion Entailment(CEE) at utterance level. Due to the simplicity andsufficiency describing emotion causes at the utter-ance level, we focus on the CEE task in this paper,whose goal is to predict which particular utterancesin the conversational history contain the cause ofnon-neutral emotion in the target utterance.",
        "Compared to the Emotion Cause Extraction(ECE) in news articles (Gui et al., 2016a; Xia andDing, 2019), CEE is particularly challenging due tothe informal expression style and the interminglingdynamic among interlocutors. Poria et al. (2021)",
        "consider CEE as a set of independent utterance pairclassification problems and neglect the emotion andspeaker information in the conversational history.",
        "Thus, they can neither capture the correlations be-tween contextual utterances in a global view nor",
        "6763model the speaker's emotional influences, namelythe intra-speaker and inter-speaker emotional influ-ences.1Intra-speaker emotional influences meanthat the cause of the emotion is primarily due tothe speaker's stable mood induced from previousdialogue turns. As shown in Figure 1 (a), utterance1 establishes the concept that Speaker A ( SA) likeswinter, which triggers a happy mood for futureutterances 3 and 5. Inter-speaker emotional influ-ences mean that the emotion of the target speakeris induced from an event mentioned or emotionrevealed by other speakers. As Figure 1 (b) shows,SB's happy emotion may be triggered by the event“getting married” mentioned by SA, or by the factthatSAis excited about getting married.",
        "To remedy this defect, we tackle CEE in a jointframework. We classify multiple utterances syn-chronously to capture the correlations between con-textual utterances and propose a TSAM to effec-tively model the speaker's emotional influences inthe conversational history. Specifically, the TSAMcontains three modules: EAN, SAN, and inter-action module. The EAN provides utterance-to-emotion interactions to incorporate emotion infor-mation by performing attention over emotion em-beddings. The SAN represents different speakerrelations between utterances in a graph, which pro-vides utterance-to-utterance interactions to incor-porate speaker information by performing attentionover the speaker relation graph. These two modulesincorporate emotion and speaker information in par-allel. Moreover, inspired by (Li et al., 2021a; Tanget al., 2020), the interaction module interchangesrelevant information between the EAN and SANthrough a mutual BiAffine transformation. Finally,the entire TSAM can be stacked in multiple layersto refine iteratively and interchange emotion andspeaker information.",
        "•For the first time, we tackle CEE in a jointframework to capture the correlations betweencontextual utterances in a global view.",
        "•We propose a TSAM to model the speaker'semotional influences in the conversational his-tory, which contains EAN, SAN, and interac-tion module to incorporate and interchangeemotion and speaker information.",
        "•Experimental results on the benchmarkdataset (Poria et al., 2021) demonstrate the1The speaker's emotional influences are predominant typesof emotion causes in the dataset as shown in Table 1.effectiveness of our proposed model, surpass-ing the SOTA model significantly.",
        "2 Related WorkECE Early works mainly exploit rule-basedmethods (Lee et al., 2010a,b; Chen et al., 2010)",
        "to identify the potential causes for certain emotionexpressions in the text. Gui et al. (2016a) first re-lease a public annotated dataset for ECE, and basedon which some feature based (Gui et al., 2016b)",
        "and neural based methods (Gui et al., 2017; Li et al.,2018; Ding et al., 2019; Xia et al., 2019; Yan et al.,2021; Li et al., 2021b) appear. To extract emotionand its corresponding cause jointly, Xia and Ding(2019) first put forward the Emotion-Cause PairExtraction (ECPE) task and tackle it by a two-stepmethod. Subsequently, many improved methodsare proposed to tackle ECPE in an end2end man-ner (Ding et al., 2020a,b; Yuan et al., 2020; Fanet al., 2020; Wei et al., 2020; Cheng et al., 2020;",
        "Chen et al., 2020a,b). However, these works men-tioned above use news articles as the target corpusfor ECE, which largely reduces reasoning complex-ity. By contrast, CEE is more challenging due tothe intermingling dynamic among interlocutors andthe informal expression style.",
        "ERC Recently, due to the proliferation of pub-licly available conversational datasets (Zhou et al.,2018; Chen et al., 2019; Poria et al., 2019a; Chat-terjee et al., 2019; Xie et al., 2022), there is a grow-ing number of studies on ERC (Hazarika et al.,2018a,b; Majumder et al., 2019; Zhong et al., 2019;",
        "Jiao et al., 2019; Ghosal et al., 2020b; Ishiwatariet al., 2020; Ghosal et al., 2020a; Shen et al., 2021;",
        "Zhu et al., 2021; Hu et al., 2021; Guibon et al.,2021; Zhao et al., 2022; Peng et al., 2022). Al-though substantial progress has been made in ERC,these studies lack further reasoning about emotions,such as understanding the stimuli or the cause of anemotion expressed by a speaker (Poria et al., 2021).",
        "RECCON For further reasoning about emo-tions, Poria et al. (2021) propose a new task namedRECCON, which contains two different sub-tasks:",
        "CSE at word/phrase level and CEE at utterancelevel. Poria et al. (2021) formalize CEE as a set ofindependent utterance pair classification problems,neglecting the emotion and speaker information inthe conversational history. Specifically, they pair atarget utterance with each utterance in its conversa-tional history and determine whether the utterance",
        "[SEP]",
        "[CLS]",
        "[SEP]",
        "[CLS]",
        "[SEP]",
        "[CLS]",
        "[SEP]",
        "[CLS]",
        "[SEP]",
        "EANSANInteractionFCN\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000fEmotionEmbeddingContextual Representation TSAM Cause Prediction(a) EAN (b) SAN\u000f\u000f\u000f\u000f\u000f\u000f \u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000f\u000fSpeaker1 Speaker2Intra relation Inter relation\u000f\u000f\u000fFigure 2: The top is the proposed model's entire architecture, and the bottom is the detailed architecture of modelcomponents: (a) EAN, (b) SAN. First, we obtain the contextual representation for each utterance with RoBERTa.",
        "Then, the TSAM is utilized to model the speaker's emotional influences in the conversational history. Finally, thecause prediction module is used to output the predictions.",
        "contains the cause of emotion in the target utter-ance. Thus, they cannot capture the correlationsbetween contextual utterances in a global view andfail to model the speaker's emotional influences inthe conversational history. From a new perspective,we tackle CEE in a joint framework. We encodeand classify multiple utterances synchronously tocapture the correlations in a global view and pro-pose a TSAM to model the speaker's emotionalinfluences effectively.",
        "3 Task DefinitionWe first define the task of CEE formally. For atarget utterance ut, i.e., thetthutterance in theconversation, the goal of CEE is to predict whichparticular utterances in the conversational historyL(ut) = (u1;u2;:::;u t)are responsible for thenon-neutral emotion in the target utterance. uiisset as a positive example if it contains the causeof non-neutral emotion in the target utterance anda negative example otherwise, where i= 1;:::;t .",
        "The independent utterance pair classification frame-work (Poria et al., 2021) performs tindependentclassifications, each of which takes (ut;ui)as in-put. Therefore, it fails to capture the correlationsbetween contextual utterances in a global view. Onthe contrary, the proposed joint classification frame-work only performs one joint classification withL(ut)as input, which makes it possible to capturethe correlations between contextual utterances.",
        "4 MethodThe proposed model consists of three components:",
        "the contextual utterance representation, the TSAM,and the cause prediction modules. The whole ar-chitecture of our model is illustrated in Figure 2.",
        "4.1 Contextual Utterance RepresentationThe pre-trained RoBERTa is utilized as the utter-ance encoder, and we extract the contextual utter-ance representations by feeding the whole of theconversational history L(ut)into the RoBERTa(Liu et al., 2019). Specifically, each utterance inL(ut)is expanded to start with the token “[CLS]”",
        "and end with the token “[SEP]”. The input represen-tation for each token is the sum of its correspondingtoken and position embeddings. The contextual rep-resentationhui2Rdhfor utterance uiis the output",
        "6765of the corresponding “[CLS]” token, where dhde-notes the dimension of the utterance representation.",
        "The contextual representation for all utterances isdenoted asHu2Rt\u0002dh. The RoBERTa we uti-lized is fine-tuned with the training process.",
        "4.2 TSAMThe TSAM models the speaker's emotional influ-ences with three modules: EAN, SAN, and Inter-action. We first illustrate the calculation process ofeach module in one-layer TSAM and then general-ize it to multiple successive layers.",
        "4.2.1 EANThe EAN provides utterance-to-emotion interac-tions to explicitly incorporate emotion informationby performing attention over emotion embeddings.",
        "Emotion Representation Given the set of candi-date emotion labels E=fe1;:::;ejEjg, each emo-tion labelekis represented using an embeddingvector (Cui and Zhang, 2019):",
        "xek=Embed (ek)2Rdh (1)",
        "wherek= 1;:::;jEj,dhdenotes the dimensionof the emotion embedding. Embed represents anemotion embedding lookup table, which is initial-ized by contextual embeddings from RoBERTa andtuned during model training. The embedding forthe set of the whole emotion labels is denoted asXe2RjEj\u0002dh.",
        "EAN Inference With the emotion labels repre-sented as embeddings, we extract the emotion in-formationHe2Rt\u0002dhby performing dot-productattention over contextual utterance representationsand emotion embeddings, which is calculated as:",
        "He=attention (Q;K;V) =V (2)",
        "=softmax (QKTpdh) (3)",
        "whereQ=Hu;K=V=Xe,2Rt\u0002jEjisan attention matrix consisting of potential emotiondistributions for all utterances. Compared to thestandard attention mechanism above, it may be ben-eficial to use multi-head attention (Vaswani et al.,2017) to capture multiple emotion distributions inparallel and obtain richer emotion information:",
        "He=concat (head 1;:::;head m) (4)",
        "head j=attention (QWQj;KWKj;VWVj)(5)whereWQj;WKj;WVj2Rdh\u0002dhmare learnableparameters and mis the number of parallel heads.",
        "Since the emotion labels of the utterances inthe conversational history are known, we can alsosimply use the embedding of emotion label corre-sponding to the utterance as the extracted emotioninformation:",
        "where ~Xe2Rt\u0002dhis the embedding of emotionlabels corresponding to all utterances in the his-tory. We refer to the method as Direct ApplicationEmotional Embedding (DAEE). Compared withDAEE, the potential advantages of the EAN areas follows: (1) The EAN can provide utterance-to-emotion interactions and capture multiple potentialemotion distributions through multi-head attentionto obtain more comprehensive and richer emotioninformation; (2) The soft emotion distributions canmodel the mutual impact among different emotionsfor further enhancement of emotion embeddings,while each emotion embedding is relatively inde-pendent of each other in DAEE; (3) The EAN canavoid emotion annotation errors to a certain extent.",
        "We apply EAN in our model to incorporate emo-tion information by default and compare the EANand DAEE in the part of experiments.",
        "4.2.2 SANThe SAN provides utterance-to-utterance interac-tions to incorporate speaker information by per-forming attention over the speaker relation graph.",
        "Graphical Structure We define a conversationalhistory with tutterances as a graph G= (V;E;R),with nodes (utterances) vi2V and labeled edges(relations) (vi;r;v j)2E, wherer2R is a rela-tion type. We also add a self-loop edge to everynode, as the cause may be present within the targetutterance itself. The representation of node viisinitialized with the contextual utterance represen-tationhui2Rdh, i.e., theithembedding in Hu.",
        "There are two relation types of edges: (1) Intrarelation type: how the utterance influences otherutterances (including itself) expressed by the samespeaker; (2) Inter relation type: how the utteranceinfluences ones expressed by other speakers.",
        "SAN Inference The representation of a node hiis updated by aggregating representations of itsneighborhoodNr(i)under the relation type r. Thegraph attention mechanism (Veli ˇckovi ´c et al., 2018)",
        "6766is used to attend to the neighborhood's representa-tions. The output of a node hsi2Rdhis calculatedas the sum of the hidden features hir2Rdhunderrelationr. The propagation is defined as follows:",
        "ijr=softmaxi(LRL(aTr[Wrhui;Wrhuj]))(7)",
        "hir=Xj2Nr(i)",
        "ijrWrhuj (8)",
        "hsi=Xr2Rhir (9)",
        "whereijrdenotes the edge weight from utteranceuito its neighborhood ujunder relation type r,Wr2Rdh\u0002dhandar2Rdhdenote a learnableweight matrix and a vector for each relation typerrespectively. LRL denotes LeakyReLU activationfunction. The updated representation of all nodesis denoted asHs2Rt\u0002dh.",
        "4.2.3 Interaction ModuleTo effectively interchange relevant information be-tween the EAN and SAN, we apply a mutual Bi-Affine transformation as a bridge. The calculationprocess is formulated as follows:",
        "A1=softmax (HeW1(Hs)T) (10)",
        "A2=softmax (HsW2(He)T) (11)",
        "whereW1;W22Rdh\u0002dhare trainable parametersandA1;A22Rt\u0002tare temporary alignment ma-trices projecting from HstoHeandHetoHs,respectively. Here, He02Rt\u0002dhcan be viewed asa projection from HstoHe, andHs02Rt\u0002dhfollows the same principle.",
        "4.2.4 The Whole ProcessWe generalize the TSAM to multiple successivelayers to iteratively refine and interchange emotionand speaker information. The detailed proceduresare as follows:",
        "Hel=EAN (El;Xe) (14)",
        "Hsl=SAN(Sl) (15)",
        "He0l;Hs0l=Interaction (Hel;Hsl) (16)",
        "whereE0=S0=Hu. The TSAM can be stackedinLlayers andl2[0;L\u00001].Statistics RECCON-DDDataDistributionsTrainPositive 7269Negative 20646DevPositive 347Negative 838TestPositive 1894Negative 5330CauseTypeDistributionsNo Context 43%Inter 32%Intra 9%Hybrid 11%Unmentioned 5%Table 1: Statistics of the RECCON-DD dataset. NoContext : The cause is present within the target ut-terance itself; Inter : Inter-speaker emotional influ-ences; Intra : Intra-speaker emotion influences (Self-Contagion); Hybrid : Inter and Intra can jointly causethe emotion of an utterance; Unmentioned : Some in-stances have no explicit emotion causes in the conver-sational history.",
        "4.3 Cause PredictionWe obtain the final utterance representation for uiby concatenating the output (EL;SL)of the L-layer TSAM. Finally, the concatenated vector isclassified using a Fully-Connected Network (FCN):",
        "li=ReLU (W1[eLi;sLi] +b1) (18)",
        "^yi=sigmoid (W2li+b2) (19)",
        "where ^yiis the probability for utterance uicon-taining the cause of emotion in the target utter-ance,eLi;sLi2Rdhdenote theithembedding inELandSL, respectively, W12Rdh\u00022dh;W22R1\u0002dh;b12Rdhandb2are learnable parametersof FCN.",
        "5 Experimental Settings5.1 Dataset and Evaluation MetricsWe evaluate the proposed model on a benchmarkdataset for RECCON, named RECCON-DD (Poriaet al., 2021), which is constructed based on Daily-Dialog dataset (Li et al., 2017).2Some statisticsabout RECCON-DD are reported in Table 1. Fol-lowing (Poria et al., 2021), the macro-averaged F1score is utilized as the evaluation metric in this pa-per. We also report the F1 score for both positiveand negative samples, denoted as Pos. F1 and Neg.",
        "F1 respectively.",
        "2DailyDialog is a natural human communication datasetwhich is usually used in ERC task. It contains utterance-levelemotion labels and covers various topics related to daily lives.",
        "6767# ModelW/O CH W/ CHPos. F1 Neg. F1 macro F1 Pos.F1 Neg.F1 macro F10INDEP base 56.64 85.13 70.88 64.28 88.74 76.511INDEP large 50.48 87.35 68.91 66.23 87.89 77.062JOINT base - - - 66.61 89.11 77.863JOINT large - - - 68.30 89.16 78.734 Ours base - - - 68.59 89.75 79.175 Ours large - - - 70.00y90.48y80.24yTable 2: Performance of our model and baselines on the test set of RECCON-DD. Bold font denotes the bestperformance. “Ours” denotes the proposed model without removing any module (“Ours” = “JOINT” + TSAM). “y”",
        "denotes that Ours large is statistically significant (Koehn, 2004) better than INDEP large W/ CH (p-value<0:05).",
        "5.2 BaselinesFor a comprehensive performance evaluation, wecompare our model with the following baselines:",
        "(1) INDEP base (Poria et al., 2021) It tackles CEEin an independent classification framework (IN-DEP) and uses the RoBERTa-Base model (Liuet al., 2019) as the utterance encoder. The input isformated as \"[CLS] ut[SEP]ui[SEP]\" and the clas-sification is performed from the final representationof the token \"[CLS]\".",
        "(2) INDEP large (Poria et al., 2021) Compared to(1), it uses the RoBERTa-Large model as utteranceencoder;",
        "(3) JOINT base It's one of the variants of ourmodel, where the TSAM is removed. It tacklesRECCON in a joint classification framework(JOINT) and uses the RoBERTa-Base model as theutterance encoder. Moreover, its input format is\"[CLS]u1[SEP][CLS] u2[SEP],...,[CLS] ut[SEP]\"",
        "and the classifications are performed syn-chronously from the corresponding contextualutterance representations of the [CLS] tokens;",
        "(4) JOINT large Compared to (3), it usesRoBERTa-Large model as the utterance encoder.",
        "ForINDEP baselines, there are two differentsettings: With Conversational History (W/ CH)",
        "and Without Conversational History (W/O CH).",
        "W/ CH means considering the conversational his-tory. When performing utterance pair classification,the conversational history L(ut)is concatenated af-ter the input to incorporate contextual information,while W/O CH means ignoring the history.",
        "5.3 Implementation DetailsOur model's base and large versions use pre-trainedRoBERTa-Base and RoBERTa-Large models as theutterance encoders, respectively.3The binary cross-entropy loss along with L2-regularization is usedduring training, where the coefficient of L2 termis0:01in the RoBERTa structure and 1e-5in otherstructures. We set the dropout rate to 0:1. Thelearning rate and the batch size are set as 1e-5and2, respectively. Our model is trained with the Adamoptimizer (Kingma and Ba, 2015). We set the di-mensions of the contextual utterance representationdhas 768/1024 in the Base/Large version of theproposed model. We use 4-head attention in EAN,and the number of TSAM layers Lis set to 3. Wetrain the model for 40epochs in total and use theearly stopping strategy based on the performanceon the development set. Then, the model with thehighest macro-averaged F1 score is used to evalu-ate the test set. Other hyper-parameters are selectedaccording to the performance of the developmentset. All of the experiments are conducted on anNVIDIA V100 GPU with 32GB of memory.",
        "6 Results and Discussions6.1 Main ResultsExperimental results are reported in Table 2. Wedirectly cite the results for the baselines reportedin (Poria et al., 2021). For the performance ofeach model we implemented, we report the averagescore of 5 runs. From Table 2, we can find thatthe proposed model (#5) outperforms all of thebaselines and surpasses the best model (#1, W/CH) in (Poria et al., 2021) with more than 3 pointsof macro F1 score.",
        "Further comparisons show that models with thelarge pre-trained utterance encoder are more likely3Our RoBERTa models are adapted from this im-plementation: https://github.com/huggingface/transformers",
        "6768to achieve better performance (about 1 point ofmacro F1 score) than the corresponding modelswith the base one, except for the models underW/O CH setting in the Table 2. By comparingtwo different settings W/O CH and W/ CH inTable 2, we can find that the conversational his-tory plays a significant role for INDEP base andINDEP large models. This is mainly because thatthe conversational history is able to provide thecontextual information for prediction. Due to thesimultaneous classification of multiple utterancesin the conversational history under the joint frame-work, JOINT baseandJOINT large models can natu-rally incorporate the contextual information. TheJOINT base and JOINT large models significantlyoutperform the INDEP baseW/ CH and INDEP largeW/ CH models by about 1.5 points of macro F1scores respectively (comparing #0 with #2, and #1with #3 in Table 2). There may be two main fac-tors: 1) Simply concatenating the conversationalhistory after the utterance pair to be classified inINDEP W/ CH models may destroy the structureof the conversation; 2) Compared to INDEP W/CH models, classification of multiple utterancessynchronously in JOINT models will have moresufficient supervision signals and can more effec-tively model the correlations between contextualutterances in a global view, i.e., utterances with sim-ilar semantics are supposed to have similar chancesbeing the emotion cause. The comparison between#2 and #4 (or #3 and #5) in Table 2 shows the effec-tiveness of the proposed TSAM. The model withTSAM (#5) achieves an improvement up to 1.51points of macro F1 score than the model withoutTSAM (#3).",
        "6.2 Ablation StudyIn this subsection, we conduct ablation studies toanalyze the effects of different components basedon Ours large mentioned in Table 2.",
        "Effect of Emotion Information We comparethree different ways for incorporating the emotioninformation: no emotion information incorporated,incorporating emotion information with direct ap-plication emotional embedding, and incorporatingemotion information with EAN. The results areshown in Table 3. We can find that the performanceof the proposed model degrades if the emotion in-formation is not incorporated (comparing row 1with 3 in Table 3). This result shows that the emo-tion information in the conversational history playsEmotion Information Pos. F1 Neg. F1 macro F1No 68.40 89.80 79.10DAEE 68.90 90.03 79.47EAN 70.00 90.48 80.24Table 3: Comparison of different ways of incorporat-ing emotion information. No: no emotion informationincorporated; DAEE : incorporating the emotion infor-mation with direct application emotional embedding.",
        "a significant role in the task of CEE. By comparingrows 2 with 3 in Table 3, the result shows that EANachieves better performance than DAEE since EANcan extract richer emotion information and modelthe mutual impact among different emotions.",
        "Effect of Speaker Information To evaluate theeffects of speaker information, we remove thespeaker relations in SAN, resulting in a single edgerelation throughout the graph. As Table 4 shows,the performance of our model decreases dramat-ically if not considering the speaker information.",
        "This result presents that modeling the speaker in-formation in the conversational history is very im-portant for the final performance.",
        "Speaker Information Pos. F1 Neg. F1 macro F1Not Consider 67.99 89.42 78.71Consider 70.00 90.48 80.24Table 4: Results on experiments whether consideringspeaker information or not in SAN.",
        "Effect of Interaction Module We remove theinteraction module in each layer so that the EANand SAN can't interact. As Table 5 shows, theperformance of our model decreases dramaticallywhen the interaction module is removed. This re-sult shows that the effective interchange of relevantinformation between EAN and SAN is conduciveto the final performance.",
        "Pos. F1 Neg. F1 macro F1W/O Interaction 68.18 88.93 78.56W/ Interaction 70.00 90.48 80.24Table 5: Results on experiments whether removing in-teraction module or not in TSAM.",
        "Ability on Modeling Emotional Influences Toevaluate the proposed model's ability to model thespeaker's emotional influences, we collect the pos-itive examples from the test set where the causes",
        "6769Models Intra InterW/O TSAM 62.06 72.67W/ TSAM 63.82 74.81Table 6: Accuracy on the collected samples. In-tra: Intra-speaker emotional influences; Inter : Inter-speaker emotional influences.",
        "are induced from the inter-speaker or intra-speakeremotional influences. And we test the predictionaccuracy on the collected samples for the proposedOurs large with and without TSAM. As shown inTable 6, W/ TSAM outperforms W/O TSAM byaround 2points on both cause types, which furtherverifies that the TSAM can effectively model theemotional influences between speakers.",
        "6.3 Impact of the TSAM Layer NumberSince TSAM for modeling speakers' emotionalinfluences is the critical component of our model,we chose the number of TSAM layers L(rangingfrom 1 to 5) on the development set of RECCON-DD. As shown in Figure 3, our model with threeTSAM layers achieves the best performance. Onthe one hand, emotion and speaker informationmay not be refined and interchanged well when thenumber of layers is small. On the other hand, if thenumber of layers is excessive, the performance willdecrease, possibly due to information redundancy.",
        "Figure 3: Results of Ours large with various TSAM lay-ers on the development set of RECCON-DD.",
        "6.4 Error AnalysisBy analyzing our predicted emotion causes, wefind that the following aspects mainly cause thepredicted errors. Firstly, our model weakly givesthe correct predictions for target utterances withthree or more causes.4Compared to the utteranceswith 1 or 2 causes, the proposed model dropped sixmacro F1 scores on utterances with multiple causes.",
        "Secondly, our model cannot predict well when theunderlying emotional cause is latent. At this point,recognizing emotion causes may require complexreasoning steps, and commonsense knowledge isan integral part of this process. We take the casebelow as an example:",
        "•SA(happy): Hello, thanks for calling 123Tech Help. I'm Todd. How can I help you?",
        "•SB(fear): Hello? Can you help me? Mycomputer! Oh, man...",
        "In this case, SAis happy to help SB. In this exam-ple, the cause of happy emotion is due to the event“greeting” or intention to provide help. On the otherhand,SBis fearful because his or her computer isbroken. Both speakers' causes of elicited emotionscan only be inferred using commonsense knowl-edge, which our model does not explicitly consider.",
        "7 Conclusion and Future WorkFor the first time, we tackle CEE in a jointframework. We classify multiple utterances syn-chronously to capture the correlations between con-textual utterances in a global view and propose aTSAM to effectively model the speaker's emotionalinfluences. Experimental results on the benchmarkdataset show that our model significantly outper-forms the SOTA model, and further analysis ver-ifies the effectiveness of each component. Thispaper points out a new reliable route for follow-up works: incorporating the emotion and speakerinformation explicitly and modeling the speaker'semotional influences effectively can bring enor-mous benefits for the tasks similar to CEE.",
        "In the future, we would explore three aspects:",
        "(1) Learn emotion recognition and emotion causerecognition in conversations jointly; (2) Apply ourmodel to other similar tasks which need to incorpo-rate the speaker and emotion information; (3) In-corporate commonsense knowledge into the modelexplicitly to address situations when the underlyingemotion cause is latent."
    ]
}